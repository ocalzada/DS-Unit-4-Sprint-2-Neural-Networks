{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVfaLrjLvxvQ"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Neural Networks\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 1*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "## Define the Following:\n",
    "You can add image, diagrams, whatever you need to ensure that you understand the concepts below.\n",
    "\n",
    "### Input Layer: \n",
    "The first layer of a neural network. Takes in raw data (from the outside world or the output of other neurons) and feeds it into the rest of the network/Perceptron. No computations are performed in the input nodes.\n",
    "\n",
    "### Hidden Layer:\n",
    "The layer inbetween the input and output nodes and does not see anything outside of it. Computationally intensive.\n",
    "\n",
    "### Output Layer:\n",
    "The last layer of the neural network, this is where the results from a classification/regression emerge. This is the layer where the outputs/predictions are returned to.\n",
    "\n",
    "### Neuron:\n",
    "A Neuron takes the inputs and multiplies them by their weights, adds bias, sums them up, then applies the activation function to the sum to determine whether the passed information should go to the next stage. The neuron with the greatest/most signal in a layer, gets to move forward.\n",
    "\n",
    "### Weight:\n",
    "Assigns an importance value to inputs; oftentimes it's a matrix that transforms inputs to a neuron into another shape.\n",
    "\n",
    "### Activation Function:\n",
    "An activation function transforms a matrix into the desired shape for the output. Examples include sigmoid, tanh, step and relu activation functions.\n",
    "\n",
    "### Node Map:\n",
    "A visual diagram depicting the architecture of a neural network. We can think of Node Maps as flow charts showing paths between inputs and outputs.\n",
    "\n",
    "### Perceptron:\n",
    "A single layer of a neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NXuy9WcWzxa4"
   },
   "source": [
    "## Inputs -> Outputs\n",
    "\n",
    "### Explain the flow of information through a neural network from inputs to outputs. Be sure to include: inputs, weights, bias, and activation functions. How does it all flow from beginning to end?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PlSwIJMC0A8F"
   },
   "source": [
    "#### Your Answer Here\n",
    "Inputs > weights + bias > activation function > outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6sWR43PTwhSk"
   },
   "source": [
    "## Write your own perceptron code that can correctly classify (99.0% accuracy) a NAND gate. \n",
    "\n",
    "| x1 | x2 | y |\n",
    "|----|----|---|\n",
    "| 0  | 0  | 1 |\n",
    "| 1  | 0  | 1 |\n",
    "| 0  | 1  | 1 |\n",
    "| 1  | 1  | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = { 'x1': [0,1,0,1],\n",
    "         'x2': [0,0,1,1],\n",
    "         'y':  [1,1,1,0]\n",
    "       }\n",
    "\n",
    "df = pd.DataFrame.from_dict(data).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sgh7VFGwnXGH"
   },
   "outputs": [],
   "source": [
    "##### Your Code Here #####\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    ''' A function that takes 1 parameter, x, and returns the sigmoid calculation of it'''\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    ''' A function that takes 1 parameter, x, and returns the sigmoid derivative of it'''\n",
    "    sx = sigmoid(x)\n",
    "    return sx * (1-sx)\n",
    "\n",
    "def perceptron(inputs, outputs, num_passes):\n",
    "    ''' A function that runs a simple neural network: A Perceptron.\n",
    "        Takes in inputs, outputs to search for and the number of passes to learn from.'''\n",
    "    \n",
    "    ''' Assigning random weights to our inputs'''\n",
    "    weights = 2 * np.random.random((len(inputs.T), 1)) - 1\n",
    "    \n",
    "    for iteration in range(num_passes):\n",
    "        ''' Calculating the dot product of the inputs times the weights '''\n",
    "        weighted_sum = np.dot(inputs, weights)\n",
    "    \n",
    "        ''' Output the activated value for the end of 1 training epoch '''\n",
    "        activated_output = sigmoid(weighted_sum)\n",
    "    \n",
    "        ''' Taking the difference between Output and the True values to calculate error '''\n",
    "        error = outputs - activated_output\n",
    "    \n",
    "        ''' Gradient descent/backprop - magic!'''\n",
    "        adjusted = error * sigmoid_derivative(activated_output)\n",
    "    \n",
    "        ''' Updating the weights after each iteration'''\n",
    "        weights += np.dot(inputs.T, adjusted)\n",
    "    \n",
    "        print(iteration)\n",
    "        # print('Weights after training: \\n', weights, '\\n')\n",
    "        print('Outputs After the Training: \\n', activated_output, '\\n')\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([\n",
    "    [0,0,1],\n",
    "    [1,0,1],\n",
    "    [0,1,1],\n",
    "    [1,1,1]\n",
    "])\n",
    "''' Ok, so we ran into a problem where the first output is always 0.5 and the last output is increasing although its final \n",
    "value should be 0. \n",
    "The first row of inputs are all 0s, thus the output value was never going to change from that & since the last row are all 1s,\n",
    "it was never going to achieve a 0 as the final output value.\n",
    "To resolve this issue, I added a third column of 1s in the inputs. Therefore introducing 'bias' to offset the multiplication issue of 0.\n",
    "'''\n",
    "outputs = np.array([\n",
    "    [1],\n",
    "    [1],\n",
    "    [1],\n",
    "    [0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Outputs After the Training: \n",
      " [[0.64858511]\n",
      " [0.56484083]\n",
      " [0.48568963]\n",
      " [0.3990918 ]] \n",
      "\n",
      "1\n",
      "Outputs After the Training: \n",
      " [[0.69380854]\n",
      " [0.61553583]\n",
      " [0.54321261]\n",
      " [0.45659449]] \n",
      "\n",
      "2\n",
      "Outputs After the Training: \n",
      " [[0.7253866 ]\n",
      " [0.64637777]\n",
      " [0.58039812]\n",
      " [0.48905734]] \n",
      "\n",
      "3\n",
      "Outputs After the Training: \n",
      " [[0.74890522]\n",
      " [0.66577544]\n",
      " [0.60519374]\n",
      " [0.50587744]] \n",
      "\n",
      "4\n",
      "Outputs After the Training: \n",
      " [[0.76744379]\n",
      " [0.67841994]\n",
      " [0.62240811]\n",
      " [0.51308918]] \n",
      "\n",
      "5\n",
      "Outputs After the Training: \n",
      " [[0.78274336]\n",
      " [0.68695794]\n",
      " [0.63487824]\n",
      " [0.51434914]] \n",
      "\n",
      "6\n",
      "Outputs After the Training: \n",
      " [[0.79582993]\n",
      " [0.69294123]\n",
      " [0.64431257]\n",
      " [0.51189912]] \n",
      "\n",
      "7\n",
      "Outputs After the Training: \n",
      " [[0.8073321 ]\n",
      " [0.69731336]\n",
      " [0.65176771]\n",
      " [0.50714655]] \n",
      "\n",
      "8\n",
      "Outputs After the Training: \n",
      " [[0.81764813]\n",
      " [0.70066409]\n",
      " [0.65791208]\n",
      " [0.50099296]] \n",
      "\n",
      "9\n",
      "Outputs After the Training: \n",
      " [[0.82703805]\n",
      " [0.70336898]\n",
      " [0.66317569]\n",
      " [0.4940233 ]] \n",
      "\n",
      "10\n",
      "Outputs After the Training: \n",
      " [[0.83567664]\n",
      " [0.70566976]\n",
      " [0.66783844]\n",
      " [0.48661926]] \n",
      "\n",
      "11\n",
      "Outputs After the Training: \n",
      " [[0.84368497]\n",
      " [0.70772269]\n",
      " [0.67208394]\n",
      " [0.47902958]] \n",
      "\n",
      "12\n",
      "Outputs After the Training: \n",
      " [[0.85114967]\n",
      " [0.70962866]\n",
      " [0.67603322]\n",
      " [0.47141514]] \n",
      "\n",
      "13\n",
      "Outputs After the Training: \n",
      " [[0.85813496]\n",
      " [0.71145234]\n",
      " [0.67976637]\n",
      " [0.46387836]] \n",
      "\n",
      "14\n",
      "Outputs After the Training: \n",
      " [[0.86469022]\n",
      " [0.71323467]\n",
      " [0.68333653]\n",
      " [0.4564828 ]] \n",
      "\n",
      "15\n",
      "Outputs After the Training: \n",
      " [[0.8708548 ]\n",
      " [0.71500098]\n",
      " [0.68677903]\n",
      " [0.4492662 ]] \n",
      "\n",
      "16\n",
      "Outputs After the Training: \n",
      " [[0.87666116]\n",
      " [0.71676638]\n",
      " [0.69011748]\n",
      " [0.4422492 ]] \n",
      "\n",
      "17\n",
      "Outputs After the Training: \n",
      " [[0.88213688]\n",
      " [0.71853935]\n",
      " [0.69336772]\n",
      " [0.43544132]] \n",
      "\n",
      "18\n",
      "Outputs After the Training: \n",
      " [[0.88730595]\n",
      " [0.72032406]\n",
      " [0.69654049]\n",
      " [0.4288448 ]] \n",
      "\n",
      "19\n",
      "Outputs After the Training: \n",
      " [[0.89218964]\n",
      " [0.72212198]\n",
      " [0.69964319]\n",
      " [0.42245734]] \n",
      "\n",
      "20\n",
      "Outputs After the Training: \n",
      " [[0.89680711]\n",
      " [0.72393288]\n",
      " [0.702681  ]\n",
      " [0.4162738 ]] \n",
      "\n",
      "21\n",
      "Outputs After the Training: \n",
      " [[0.90117577]\n",
      " [0.72575553]\n",
      " [0.70565767]\n",
      " [0.41028739]] \n",
      "\n",
      "22\n",
      "Outputs After the Training: \n",
      " [[0.90531158]\n",
      " [0.72758812]\n",
      " [0.70857602]\n",
      " [0.40449044]] \n",
      "\n",
      "23\n",
      "Outputs After the Training: \n",
      " [[0.90922925]\n",
      " [0.72942858]\n",
      " [0.71143826]\n",
      " [0.39887489]] \n",
      "\n",
      "24\n",
      "Outputs After the Training: \n",
      " [[0.9129424 ]\n",
      " [0.73127472]\n",
      " [0.71424623]\n",
      " [0.39343263]] \n",
      "\n",
      "25\n",
      "Outputs After the Training: \n",
      " [[0.91646363]\n",
      " [0.73312438]\n",
      " [0.71700149]\n",
      " [0.38815569]] \n",
      "\n",
      "26\n",
      "Outputs After the Training: \n",
      " [[0.91980468]\n",
      " [0.73497548]\n",
      " [0.71970546]\n",
      " [0.38303633]] \n",
      "\n",
      "27\n",
      "Outputs After the Training: \n",
      " [[0.92297645]\n",
      " [0.73682606]\n",
      " [0.72235945]\n",
      " [0.37806717]] \n",
      "\n",
      "28\n",
      "Outputs After the Training: \n",
      " [[0.92598911]\n",
      " [0.73867431]\n",
      " [0.72496471]\n",
      " [0.37324118]] \n",
      "\n",
      "29\n",
      "Outputs After the Training: \n",
      " [[0.92885213]\n",
      " [0.74051856]\n",
      " [0.72752243]\n",
      " [0.36855171]] \n",
      "\n",
      "30\n",
      "Outputs After the Training: \n",
      " [[0.93157434]\n",
      " [0.7423573 ]\n",
      " [0.73003378]\n",
      " [0.36399247]] \n",
      "\n",
      "31\n",
      "Outputs After the Training: \n",
      " [[0.934164  ]\n",
      " [0.74418915]\n",
      " [0.73249989]\n",
      " [0.35955755]] \n",
      "\n",
      "32\n",
      "Outputs After the Training: \n",
      " [[0.9366288 ]\n",
      " [0.74601289]\n",
      " [0.73492188]\n",
      " [0.35524138]] \n",
      "\n",
      "33\n",
      "Outputs After the Training: \n",
      " [[0.93897593]\n",
      " [0.74782739]\n",
      " [0.73730083]\n",
      " [0.35103871]] \n",
      "\n",
      "34\n",
      "Outputs After the Training: \n",
      " [[0.94121211]\n",
      " [0.74963169]\n",
      " [0.73963782]\n",
      " [0.34694461]] \n",
      "\n",
      "35\n",
      "Outputs After the Training: \n",
      " [[0.94334363]\n",
      " [0.75142489]\n",
      " [0.7419339 ]\n",
      " [0.34295445]] \n",
      "\n",
      "36\n",
      "Outputs After the Training: \n",
      " [[0.94537637]\n",
      " [0.75320622]\n",
      " [0.74419009]\n",
      " [0.33906386]] \n",
      "\n",
      "37\n",
      "Outputs After the Training: \n",
      " [[0.94731582]\n",
      " [0.75497499]\n",
      " [0.74640739]\n",
      " [0.33526872]] \n",
      "\n",
      "38\n",
      "Outputs After the Training: \n",
      " [[0.94916714]\n",
      " [0.75673061]\n",
      " [0.74858678]\n",
      " [0.33156515]] \n",
      "\n",
      "39\n",
      "Outputs After the Training: \n",
      " [[0.95093513]\n",
      " [0.75847255]\n",
      " [0.75072923]\n",
      " [0.3279495 ]] \n",
      "\n",
      "40\n",
      "Outputs After the Training: \n",
      " [[0.95262434]\n",
      " [0.76020035]\n",
      " [0.75283565]\n",
      " [0.32441831]] \n",
      "\n",
      "41\n",
      "Outputs After the Training: \n",
      " [[0.95423898]\n",
      " [0.76191361]\n",
      " [0.75490695]\n",
      " [0.32096833]] \n",
      "\n",
      "42\n",
      "Outputs After the Training: \n",
      " [[0.95578303]\n",
      " [0.76361201]\n",
      " [0.75694402]\n",
      " [0.31759647]] \n",
      "\n",
      "43\n",
      "Outputs After the Training: \n",
      " [[0.95726023]\n",
      " [0.76529524]\n",
      " [0.7589477 ]\n",
      " [0.31429981]] \n",
      "\n",
      "44\n",
      "Outputs After the Training: \n",
      " [[0.95867407]\n",
      " [0.76696309]\n",
      " [0.76091884]\n",
      " [0.3110756 ]] \n",
      "\n",
      "45\n",
      "Outputs After the Training: \n",
      " [[0.96002784]\n",
      " [0.76861534]\n",
      " [0.76285823]\n",
      " [0.3079212 ]] \n",
      "\n",
      "46\n",
      "Outputs After the Training: \n",
      " [[0.96132463]\n",
      " [0.77025185]\n",
      " [0.76476666]\n",
      " [0.30483414]] \n",
      "\n",
      "47\n",
      "Outputs After the Training: \n",
      " [[0.96256735]\n",
      " [0.77187249]\n",
      " [0.76664488]\n",
      " [0.30181206]] \n",
      "\n",
      "48\n",
      "Outputs After the Training: \n",
      " [[0.96375873]\n",
      " [0.77347716]\n",
      " [0.76849363]\n",
      " [0.29885272]] \n",
      "\n",
      "49\n",
      "Outputs After the Training: \n",
      " [[0.96490133]\n",
      " [0.77506582]\n",
      " [0.77031362]\n",
      " [0.29595399]] \n",
      "\n",
      "50\n",
      "Outputs After the Training: \n",
      " [[0.9659976 ]\n",
      " [0.77663841]\n",
      " [0.77210555]\n",
      " [0.29311384]] \n",
      "\n",
      "51\n",
      "Outputs After the Training: \n",
      " [[0.9670498 ]\n",
      " [0.77819492]\n",
      " [0.77387008]\n",
      " [0.29033035]] \n",
      "\n",
      "52\n",
      "Outputs After the Training: \n",
      " [[0.96806008]\n",
      " [0.77973537]\n",
      " [0.77560786]\n",
      " [0.28760167]] \n",
      "\n",
      "53\n",
      "Outputs After the Training: \n",
      " [[0.96903048]\n",
      " [0.78125978]\n",
      " [0.77731951]\n",
      " [0.28492605]] \n",
      "\n",
      "54\n",
      "Outputs After the Training: \n",
      " [[0.96996291]\n",
      " [0.78276819]\n",
      " [0.77900565]\n",
      " [0.28230182]] \n",
      "\n",
      "55\n",
      "Outputs After the Training: \n",
      " [[0.97085918]\n",
      " [0.78426065]\n",
      " [0.78066687]\n",
      " [0.27972738]] \n",
      "\n",
      "56\n",
      "Outputs After the Training: \n",
      " [[0.97172099]\n",
      " [0.78573725]\n",
      " [0.78230373]\n",
      " [0.27720121]] \n",
      "\n",
      "57\n",
      "Outputs After the Training: \n",
      " [[0.97254995]\n",
      " [0.78719806]\n",
      " [0.78391678]\n",
      " [0.27472184]] \n",
      "\n",
      "58\n",
      "Outputs After the Training: \n",
      " [[0.97334759]\n",
      " [0.78864318]\n",
      " [0.78550658]\n",
      " [0.27228787]] \n",
      "\n",
      "59\n",
      "Outputs After the Training: \n",
      " [[0.97411534]\n",
      " [0.79007272]\n",
      " [0.78707362]\n",
      " [0.26989798]] \n",
      "\n",
      "60\n",
      "Outputs After the Training: \n",
      " [[0.97485458]\n",
      " [0.79148678]\n",
      " [0.78861842]\n",
      " [0.26755087]] \n",
      "\n",
      "61\n",
      "Outputs After the Training: \n",
      " [[0.97556659]\n",
      " [0.79288549]\n",
      " [0.79014147]\n",
      " [0.26524533]] \n",
      "\n",
      "62\n",
      "Outputs After the Training: \n",
      " [[0.97625259]\n",
      " [0.79426898]\n",
      " [0.79164324]\n",
      " [0.26298018]] \n",
      "\n",
      "63\n",
      "Outputs After the Training: \n",
      " [[0.97691373]\n",
      " [0.79563737]\n",
      " [0.79312418]\n",
      " [0.26075428]] \n",
      "\n",
      "64\n",
      "Outputs After the Training: \n",
      " [[0.97755113]\n",
      " [0.79699082]\n",
      " [0.79458475]\n",
      " [0.25856655]] \n",
      "\n",
      "65\n",
      "Outputs After the Training: \n",
      " [[0.97816581]\n",
      " [0.79832946]\n",
      " [0.79602537]\n",
      " [0.25641595]] \n",
      "\n",
      "66\n",
      "Outputs After the Training: \n",
      " [[0.97875877]\n",
      " [0.79965344]\n",
      " [0.79744647]\n",
      " [0.25430148]] \n",
      "\n",
      "67\n",
      "Outputs After the Training: \n",
      " [[0.97933094]\n",
      " [0.80096291]\n",
      " [0.79884845]\n",
      " [0.25222218]] \n",
      "\n",
      "68\n",
      "Outputs After the Training: \n",
      " [[0.9798832 ]\n",
      " [0.80225802]\n",
      " [0.8002317 ]\n",
      " [0.25017711]] \n",
      "\n",
      "69\n",
      "Outputs After the Training: \n",
      " [[0.98041641]\n",
      " [0.80353893]\n",
      " [0.80159662]\n",
      " [0.24816539]] \n",
      "\n",
      "70\n",
      "Outputs After the Training: \n",
      " [[0.98093137]\n",
      " [0.80480581]\n",
      " [0.80294357]\n",
      " [0.24618617]] \n",
      "\n",
      "71\n",
      "Outputs After the Training: \n",
      " [[0.98142883]\n",
      " [0.80605879]\n",
      " [0.80427291]\n",
      " [0.2442386 ]] \n",
      "\n",
      "72\n",
      "Outputs After the Training: \n",
      " [[0.98190953]\n",
      " [0.80729806]\n",
      " [0.80558501]\n",
      " [0.2423219 ]] \n",
      "\n",
      "73\n",
      "Outputs After the Training: \n",
      " [[0.98237414]\n",
      " [0.80852377]\n",
      " [0.80688019]\n",
      " [0.2404353 ]] \n",
      "\n",
      "74\n",
      "Outputs After the Training: \n",
      " [[0.98282334]\n",
      " [0.80973608]\n",
      " [0.80815879]\n",
      " [0.23857805]] \n",
      "\n",
      "75\n",
      "Outputs After the Training: \n",
      " [[0.98325774]\n",
      " [0.81093515]\n",
      " [0.80942114]\n",
      " [0.23674943]] \n",
      "\n",
      "76\n",
      "Outputs After the Training: \n",
      " [[0.98367794]\n",
      " [0.81212115]\n",
      " [0.81066756]\n",
      " [0.23494877]] \n",
      "\n",
      "77\n",
      "Outputs After the Training: \n",
      " [[0.98408451]\n",
      " [0.81329424]\n",
      " [0.81189833]\n",
      " [0.23317538]] \n",
      "\n",
      "78\n",
      "Outputs After the Training: \n",
      " [[0.98447798]\n",
      " [0.81445457]\n",
      " [0.81311377]\n",
      " [0.23142864]] \n",
      "\n",
      "79\n",
      "Outputs After the Training: \n",
      " [[0.98485887]\n",
      " [0.81560232]\n",
      " [0.81431417]\n",
      " [0.2297079 ]] \n",
      "\n",
      "80\n",
      "Outputs After the Training: \n",
      " [[0.98522768]\n",
      " [0.81673765]\n",
      " [0.8154998 ]\n",
      " [0.22801257]] \n",
      "\n",
      "81\n",
      "Outputs After the Training: \n",
      " [[0.98558486]\n",
      " [0.8178607 ]\n",
      " [0.81667094]\n",
      " [0.22634206]] \n",
      "\n",
      "82\n",
      "Outputs After the Training: \n",
      " [[0.98593087]\n",
      " [0.81897165]\n",
      " [0.81782787]\n",
      " [0.22469582]] \n",
      "\n",
      "83\n",
      "Outputs After the Training: \n",
      " [[0.98626614]\n",
      " [0.82007064]\n",
      " [0.81897083]\n",
      " [0.22307329]] \n",
      "\n",
      "84\n",
      "Outputs After the Training: \n",
      " [[0.98659107]\n",
      " [0.82115784]\n",
      " [0.82010008]\n",
      " [0.22147394]] \n",
      "\n",
      "85\n",
      "Outputs After the Training: \n",
      " [[0.98690606]\n",
      " [0.82223341]\n",
      " [0.82121588]\n",
      " [0.21989727]] \n",
      "\n",
      "86\n",
      "Outputs After the Training: \n",
      " [[0.98721146]\n",
      " [0.82329748]\n",
      " [0.82231846]\n",
      " [0.21834277]] \n",
      "\n",
      "87\n",
      "Outputs After the Training: \n",
      " [[0.98750765]\n",
      " [0.82435023]\n",
      " [0.82340806]\n",
      " [0.21680997]] \n",
      "\n",
      "88\n",
      "Outputs After the Training: \n",
      " [[0.98779496]\n",
      " [0.82539179]\n",
      " [0.82448491]\n",
      " [0.2152984 ]] \n",
      "\n",
      "89\n",
      "Outputs After the Training: \n",
      " [[0.98807371]\n",
      " [0.82642233]\n",
      " [0.82554923]\n",
      " [0.2138076 ]] \n",
      "\n",
      "90\n",
      "Outputs After the Training: \n",
      " [[0.98834422]\n",
      " [0.82744198]\n",
      " [0.82660124]\n",
      " [0.21233713]] \n",
      "\n",
      "91\n",
      "Outputs After the Training: \n",
      " [[0.98860679]\n",
      " [0.82845089]\n",
      " [0.82764115]\n",
      " [0.21088657]] \n",
      "\n",
      "92\n",
      "Outputs After the Training: \n",
      " [[0.98886169]\n",
      " [0.82944921]\n",
      " [0.82866918]\n",
      " [0.20945551]] \n",
      "\n",
      "93\n",
      "Outputs After the Training: \n",
      " [[0.98910921]\n",
      " [0.83043708]\n",
      " [0.82968552]\n",
      " [0.20804355]] \n",
      "\n",
      "94\n",
      "Outputs After the Training: \n",
      " [[0.9893496 ]\n",
      " [0.83141465]\n",
      " [0.83069038]\n",
      " [0.20665028]] \n",
      "\n",
      "95\n",
      "Outputs After the Training: \n",
      " [[0.98958312]\n",
      " [0.83238204]\n",
      " [0.83168394]\n",
      " [0.20527534]] \n",
      "\n",
      "96\n",
      "Outputs After the Training: \n",
      " [[0.98981001]\n",
      " [0.83333941]\n",
      " [0.8326664 ]\n",
      " [0.20391835]] \n",
      "\n",
      "97\n",
      "Outputs After the Training: \n",
      " [[0.9900305 ]\n",
      " [0.83428687]\n",
      " [0.83363794]\n",
      " [0.20257896]] \n",
      "\n",
      "98\n",
      "Outputs After the Training: \n",
      " [[0.9902448 ]\n",
      " [0.83522458]\n",
      " [0.83459874]\n",
      " [0.20125682]] \n",
      "\n",
      "99\n",
      "Outputs After the Training: \n",
      " [[0.99045313]\n",
      " [0.83615266]\n",
      " [0.83554899]\n",
      " [0.19995159]] \n",
      "\n",
      "100\n",
      "Outputs After the Training: \n",
      " [[0.9906557 ]\n",
      " [0.83707124]\n",
      " [0.83648884]\n",
      " [0.19866294]] \n",
      "\n",
      "101\n",
      "Outputs After the Training: \n",
      " [[0.99085269]\n",
      " [0.83798046]\n",
      " [0.83741847]\n",
      " [0.19739054]] \n",
      "\n",
      "102\n",
      "Outputs After the Training: \n",
      " [[0.9910443 ]\n",
      " [0.83888043]\n",
      " [0.83833805]\n",
      " [0.1961341 ]] \n",
      "\n",
      "103\n",
      "Outputs After the Training: \n",
      " [[0.99123071]\n",
      " [0.83977129]\n",
      " [0.83924774]\n",
      " [0.19489329]] \n",
      "\n",
      "104\n",
      "Outputs After the Training: \n",
      " [[0.99141209]\n",
      " [0.84065315]\n",
      " [0.84014769]\n",
      " [0.19366784]] \n",
      "\n",
      "105\n",
      "Outputs After the Training: \n",
      " [[0.9915886 ]\n",
      " [0.84152614]\n",
      " [0.84103806]\n",
      " [0.19245744]] \n",
      "\n",
      "106\n",
      "Outputs After the Training: \n",
      " [[0.99176041]\n",
      " [0.84239038]\n",
      " [0.841919  ]\n",
      " [0.19126182]] \n",
      "\n",
      "107\n",
      "Outputs After the Training: \n",
      " [[0.99192766]\n",
      " [0.84324598]\n",
      " [0.84279065]\n",
      " [0.1900807 ]] \n",
      "\n",
      "108\n",
      "Outputs After the Training: \n",
      " [[0.99209051]\n",
      " [0.84409307]\n",
      " [0.84365317]\n",
      " [0.18891383]] \n",
      "\n",
      "109\n",
      "Outputs After the Training: \n",
      " [[0.9922491 ]\n",
      " [0.84493176]\n",
      " [0.84450669]\n",
      " [0.18776093]] \n",
      "\n",
      "110\n",
      "Outputs After the Training: \n",
      " [[0.99240357]\n",
      " [0.84576216]\n",
      " [0.84535136]\n",
      " [0.18662175]] \n",
      "\n",
      "111\n",
      "Outputs After the Training: \n",
      " [[0.99255404]\n",
      " [0.84658438]\n",
      " [0.8461873 ]\n",
      " [0.18549606]] \n",
      "\n",
      "112\n",
      "Outputs After the Training: \n",
      " [[0.99270064]\n",
      " [0.84739853]\n",
      " [0.84701465]\n",
      " [0.1843836 ]] \n",
      "\n",
      "113\n",
      "Outputs After the Training: \n",
      " [[0.9928435 ]\n",
      " [0.84820473]\n",
      " [0.84783354]\n",
      " [0.18328414]] \n",
      "\n",
      "114\n",
      "Outputs After the Training: \n",
      " [[0.99298273]\n",
      " [0.84900307]\n",
      " [0.84864411]\n",
      " [0.18219745]] \n",
      "\n",
      "115\n",
      "Outputs After the Training: \n",
      " [[0.99311844]\n",
      " [0.84979366]\n",
      " [0.84944646]\n",
      " [0.18112331]] \n",
      "\n",
      "116\n",
      "Outputs After the Training: \n",
      " [[0.99325075]\n",
      " [0.8505766 ]\n",
      " [0.85024073]\n",
      " [0.18006151]] \n",
      "\n",
      "117\n",
      "Outputs After the Training: \n",
      " [[0.99337975]\n",
      " [0.851352  ]\n",
      " [0.85102704]\n",
      " [0.17901182]] \n",
      "\n",
      "118\n",
      "Outputs After the Training: \n",
      " [[0.99350556]\n",
      " [0.85211996]\n",
      " [0.8518055 ]\n",
      " [0.17797403]] \n",
      "\n",
      "119\n",
      "Outputs After the Training: \n",
      " [[0.99362826]\n",
      " [0.85288058]\n",
      " [0.85257623]\n",
      " [0.17694796]] \n",
      "\n",
      "120\n",
      "Outputs After the Training: \n",
      " [[0.99374795]\n",
      " [0.85363394]\n",
      " [0.85333934]\n",
      " [0.17593339]] \n",
      "\n",
      "121\n",
      "Outputs After the Training: \n",
      " [[0.99386471]\n",
      " [0.85438015]\n",
      " [0.85409494]\n",
      " [0.17493013]] \n",
      "\n",
      "122\n",
      "Outputs After the Training: \n",
      " [[0.99397865]\n",
      " [0.8551193 ]\n",
      " [0.85484314]\n",
      " [0.173938  ]] \n",
      "\n",
      "123\n",
      "Outputs After the Training: \n",
      " [[0.99408984]\n",
      " [0.85585149]\n",
      " [0.85558405]\n",
      " [0.1729568 ]] \n",
      "\n",
      "124\n",
      "Outputs After the Training: \n",
      " [[0.99419836]\n",
      " [0.8565768 ]\n",
      " [0.85631777]\n",
      " [0.17198637]] \n",
      "\n",
      "125\n",
      "Outputs After the Training: \n",
      " [[0.99430429]\n",
      " [0.85729533]\n",
      " [0.8570444 ]\n",
      " [0.17102651]] \n",
      "\n",
      "126\n",
      "Outputs After the Training: \n",
      " [[0.99440771]\n",
      " [0.85800716]\n",
      " [0.85776405]\n",
      " [0.17007706]] \n",
      "\n",
      "127\n",
      "Outputs After the Training: \n",
      " [[0.99450869]\n",
      " [0.85871238]\n",
      " [0.85847681]\n",
      " [0.16913784]] \n",
      "\n",
      "128\n",
      "Outputs After the Training: \n",
      " [[0.9946073 ]\n",
      " [0.85941107]\n",
      " [0.85918278]\n",
      " [0.1682087 ]] \n",
      "\n",
      "129\n",
      "Outputs After the Training: \n",
      " [[0.99470361]\n",
      " [0.86010333]\n",
      " [0.85988205]\n",
      " [0.16728947]] \n",
      "\n",
      "130\n",
      "Outputs After the Training: \n",
      " [[0.99479768]\n",
      " [0.86078923]\n",
      " [0.86057472]\n",
      " [0.16638   ]] \n",
      "\n",
      "131\n",
      "Outputs After the Training: \n",
      " [[0.99488959]\n",
      " [0.86146885]\n",
      " [0.86126088]\n",
      " [0.16548012]] \n",
      "\n",
      "132\n",
      "Outputs After the Training: \n",
      " [[0.99497938]\n",
      " [0.86214228]\n",
      " [0.86194061]\n",
      " [0.16458968]] \n",
      "\n",
      "133\n",
      "Outputs After the Training: \n",
      " [[0.99506712]\n",
      " [0.86280959]\n",
      " [0.86261401]\n",
      " [0.16370855]] \n",
      "\n",
      "134\n",
      "Outputs After the Training: \n",
      " [[0.99515286]\n",
      " [0.86347087]\n",
      " [0.86328117]\n",
      " [0.16283657]] \n",
      "\n",
      "135\n",
      "Outputs After the Training: \n",
      " [[0.99523667]\n",
      " [0.86412618]\n",
      " [0.86394216]\n",
      " [0.16197359]] \n",
      "\n",
      "136\n",
      "Outputs After the Training: \n",
      " [[0.9953186 ]\n",
      " [0.86477561]\n",
      " [0.86459707]\n",
      " [0.16111949]] \n",
      "\n",
      "137\n",
      "Outputs After the Training: \n",
      " [[0.99539869]\n",
      " [0.86541922]\n",
      " [0.86524598]\n",
      " [0.16027413]] \n",
      "\n",
      "138\n",
      "Outputs After the Training: \n",
      " [[0.995477  ]\n",
      " [0.8660571 ]\n",
      " [0.86588898]\n",
      " [0.15943736]] \n",
      "\n",
      "139\n",
      "Outputs After the Training: \n",
      " [[0.99555358]\n",
      " [0.86668931]\n",
      " [0.86652614]\n",
      " [0.15860907]] \n",
      "\n",
      "140\n",
      "Outputs After the Training: \n",
      " [[0.99562847]\n",
      " [0.86731593]\n",
      " [0.86715754]\n",
      " [0.15778912]] \n",
      "\n",
      "141\n",
      "Outputs After the Training: \n",
      " [[0.99570171]\n",
      " [0.86793703]\n",
      " [0.86778325]\n",
      " [0.15697739]] \n",
      "\n",
      "142\n",
      "Outputs After the Training: \n",
      " [[0.99577337]\n",
      " [0.86855266]\n",
      " [0.86840335]\n",
      " [0.15617376]] \n",
      "\n",
      "143\n",
      "Outputs After the Training: \n",
      " [[0.99584347]\n",
      " [0.86916291]\n",
      " [0.86901792]\n",
      " [0.15537809]] \n",
      "\n",
      "144\n",
      "Outputs After the Training: \n",
      " [[0.99591205]\n",
      " [0.86976784]\n",
      " [0.86962702]\n",
      " [0.15459029]] \n",
      "\n",
      "145\n",
      "Outputs After the Training: \n",
      " [[0.99597916]\n",
      " [0.87036751]\n",
      " [0.87023073]\n",
      " [0.15381023]] \n",
      "\n",
      "146\n",
      "Outputs After the Training: \n",
      " [[0.99604484]\n",
      " [0.87096199]\n",
      " [0.87082911]\n",
      " [0.15303779]] \n",
      "\n",
      "147\n",
      "Outputs After the Training: \n",
      " [[0.99610913]\n",
      " [0.87155134]\n",
      " [0.87142224]\n",
      " [0.15227288]] \n",
      "\n",
      "148\n",
      "Outputs After the Training: \n",
      " [[0.99617205]\n",
      " [0.87213563]\n",
      " [0.87201019]\n",
      " [0.15151537]] \n",
      "\n",
      "149\n",
      "Outputs After the Training: \n",
      " [[0.99623365]\n",
      " [0.87271491]\n",
      " [0.87259301]\n",
      " [0.15076516]] \n",
      "\n",
      "150\n",
      "Outputs After the Training: \n",
      " [[0.99629395]\n",
      " [0.87328926]\n",
      " [0.87317077]\n",
      " [0.15002215]] \n",
      "\n",
      "151\n",
      "Outputs After the Training: \n",
      " [[0.99635301]\n",
      " [0.87385872]\n",
      " [0.87374355]\n",
      " [0.14928623]] \n",
      "\n",
      "152\n",
      "Outputs After the Training: \n",
      " [[0.99641083]\n",
      " [0.87442335]\n",
      " [0.87431139]\n",
      " [0.14855731]] \n",
      "\n",
      "153\n",
      "Outputs After the Training: \n",
      " [[0.99646746]\n",
      " [0.87498323]\n",
      " [0.87487437]\n",
      " [0.14783528]] \n",
      "\n",
      "154\n",
      "Outputs After the Training: \n",
      " [[0.99652293]\n",
      " [0.87553839]\n",
      " [0.87543254]\n",
      " [0.14712005]] \n",
      "\n",
      "155\n",
      "Outputs After the Training: \n",
      " [[0.99657726]\n",
      " [0.8760889 ]\n",
      " [0.87598596]\n",
      " [0.14641152]] \n",
      "\n",
      "156\n",
      "Outputs After the Training: \n",
      " [[0.99663049]\n",
      " [0.87663482]\n",
      " [0.8765347 ]\n",
      " [0.1457096 ]] \n",
      "\n",
      "157\n",
      "Outputs After the Training: \n",
      " [[0.99668264]\n",
      " [0.87717619]\n",
      " [0.87707881]\n",
      " [0.1450142 ]] \n",
      "\n",
      "158\n",
      "Outputs After the Training: \n",
      " [[0.99673374]\n",
      " [0.87771308]\n",
      " [0.87761834]\n",
      " [0.14432522]] \n",
      "\n",
      "159\n",
      "Outputs After the Training: \n",
      " [[0.99678381]\n",
      " [0.87824554]\n",
      " [0.87815336]\n",
      " [0.14364259]] \n",
      "\n",
      "160\n",
      "Outputs After the Training: \n",
      " [[0.99683289]\n",
      " [0.87877361]\n",
      " [0.87868392]\n",
      " [0.14296621]] \n",
      "\n",
      "161\n",
      "Outputs After the Training: \n",
      " [[0.99688098]\n",
      " [0.87929735]\n",
      " [0.87921007]\n",
      " [0.14229599]] \n",
      "\n",
      "162\n",
      "Outputs After the Training: \n",
      " [[0.99692813]\n",
      " [0.87981681]\n",
      " [0.87973186]\n",
      " [0.14163186]] \n",
      "\n",
      "163\n",
      "Outputs After the Training: \n",
      " [[0.99697435]\n",
      " [0.88033204]\n",
      " [0.88024936]\n",
      " [0.14097373]] \n",
      "\n",
      "164\n",
      "Outputs After the Training: \n",
      " [[0.99701966]\n",
      " [0.8808431 ]\n",
      " [0.8807626 ]\n",
      " [0.14032153]] \n",
      "\n",
      "165\n",
      "Outputs After the Training: \n",
      " [[0.99706408]\n",
      " [0.88135001]\n",
      " [0.88127165]\n",
      " [0.13967517]] \n",
      "\n",
      "166\n",
      "Outputs After the Training: \n",
      " [[0.99710765]\n",
      " [0.88185285]\n",
      " [0.88177655]\n",
      " [0.13903457]] \n",
      "\n",
      "167\n",
      "Outputs After the Training: \n",
      " [[0.99715037]\n",
      " [0.88235165]\n",
      " [0.88227735]\n",
      " [0.13839966]] \n",
      "\n",
      "168\n",
      "Outputs After the Training: \n",
      " [[0.99719227]\n",
      " [0.88284646]\n",
      " [0.8827741 ]\n",
      " [0.13777037]] \n",
      "\n",
      "169\n",
      "Outputs After the Training: \n",
      " [[0.99723336]\n",
      " [0.88333732]\n",
      " [0.88326684]\n",
      " [0.13714662]] \n",
      "\n",
      "170\n",
      "Outputs After the Training: \n",
      " [[0.99727367]\n",
      " [0.88382428]\n",
      " [0.88375563]\n",
      " [0.13652833]] \n",
      "\n",
      "171\n",
      "Outputs After the Training: \n",
      " [[0.99731322]\n",
      " [0.88430739]\n",
      " [0.88424051]\n",
      " [0.13591545]] \n",
      "\n",
      "172\n",
      "Outputs After the Training: \n",
      " [[0.99735201]\n",
      " [0.88478669]\n",
      " [0.88472153]\n",
      " [0.13530789]] \n",
      "\n",
      "173\n",
      "Outputs After the Training: \n",
      " [[0.99739008]\n",
      " [0.88526222]\n",
      " [0.88519873]\n",
      " [0.13470558]] \n",
      "\n",
      "174\n",
      "Outputs After the Training: \n",
      " [[0.99742743]\n",
      " [0.88573402]\n",
      " [0.88567215]\n",
      " [0.13410848]] \n",
      "\n",
      "175\n",
      "Outputs After the Training: \n",
      " [[0.99746409]\n",
      " [0.88620214]\n",
      " [0.88614184]\n",
      " [0.13351649]] \n",
      "\n",
      "176\n",
      "Outputs After the Training: \n",
      " [[0.99750006]\n",
      " [0.88666662]\n",
      " [0.88660785]\n",
      " [0.13292957]] \n",
      "\n",
      "177\n",
      "Outputs After the Training: \n",
      " [[0.99753537]\n",
      " [0.88712749]\n",
      " [0.8870702 ]\n",
      " [0.13234764]] \n",
      "\n",
      "178\n",
      "Outputs After the Training: \n",
      " [[0.99757003]\n",
      " [0.88758481]\n",
      " [0.88752896]\n",
      " [0.13177065]] \n",
      "\n",
      "179\n",
      "Outputs After the Training: \n",
      " [[0.99760405]\n",
      " [0.8880386 ]\n",
      " [0.88798415]\n",
      " [0.13119852]] \n",
      "\n",
      "180\n",
      "Outputs After the Training: \n",
      " [[0.99763745]\n",
      " [0.88848891]\n",
      " [0.88843581]\n",
      " [0.13063121]] \n",
      "\n",
      "181\n",
      "Outputs After the Training: \n",
      " [[0.99767025]\n",
      " [0.88893577]\n",
      " [0.888884  ]\n",
      " [0.13006865]] \n",
      "\n",
      "182\n",
      "Outputs After the Training: \n",
      " [[0.99770245]\n",
      " [0.88937923]\n",
      " [0.88932874]\n",
      " [0.12951077]] \n",
      "\n",
      "183\n",
      "Outputs After the Training: \n",
      " [[0.99773406]\n",
      " [0.88981932]\n",
      " [0.88977007]\n",
      " [0.12895754]] \n",
      "\n",
      "184\n",
      "Outputs After the Training: \n",
      " [[0.99776512]\n",
      " [0.89025607]\n",
      " [0.89020804]\n",
      " [0.12840888]] \n",
      "\n",
      "185\n",
      "Outputs After the Training: \n",
      " [[0.99779561]\n",
      " [0.89068953]\n",
      " [0.89064267]\n",
      " [0.12786474]] \n",
      "\n",
      "186\n",
      "Outputs After the Training: \n",
      " [[0.99782556]\n",
      " [0.89111973]\n",
      " [0.89107402]\n",
      " [0.12732506]] \n",
      "\n",
      "187\n",
      "Outputs After the Training: \n",
      " [[0.99785498]\n",
      " [0.89154671]\n",
      " [0.8915021 ]\n",
      " [0.1267898 ]] \n",
      "\n",
      "188\n",
      "Outputs After the Training: \n",
      " [[0.99788389]\n",
      " [0.8919705 ]\n",
      " [0.89192697]\n",
      " [0.12625889]] \n",
      "\n",
      "189\n",
      "Outputs After the Training: \n",
      " [[0.99791228]\n",
      " [0.89239113]\n",
      " [0.89234865]\n",
      " [0.12573229]] \n",
      "\n",
      "190\n",
      "Outputs After the Training: \n",
      " [[0.99794017]\n",
      " [0.89280863]\n",
      " [0.89276718]\n",
      " [0.12520995]] \n",
      "\n",
      "191\n",
      "Outputs After the Training: \n",
      " [[0.99796758]\n",
      " [0.89322305]\n",
      " [0.89318259]\n",
      " [0.12469181]] \n",
      "\n",
      "192\n",
      "Outputs After the Training: \n",
      " [[0.99799452]\n",
      " [0.89363442]\n",
      " [0.89359492]\n",
      " [0.12417782]] \n",
      "\n",
      "193\n",
      "Outputs After the Training: \n",
      " [[0.99802098]\n",
      " [0.89404276]\n",
      " [0.89400421]\n",
      " [0.12366793]] \n",
      "\n",
      "194\n",
      "Outputs After the Training: \n",
      " [[0.99804699]\n",
      " [0.89444811]\n",
      " [0.89441047]\n",
      " [0.1231621 ]] \n",
      "\n",
      "195\n",
      "Outputs After the Training: \n",
      " [[0.99807256]\n",
      " [0.89485051]\n",
      " [0.89481375]\n",
      " [0.12266028]] \n",
      "\n",
      "196\n",
      "Outputs After the Training: \n",
      " [[0.99809769]\n",
      " [0.89524997]\n",
      " [0.89521408]\n",
      " [0.12216242]] \n",
      "\n",
      "197\n",
      "Outputs After the Training: \n",
      " [[0.99812238]\n",
      " [0.89564654]\n",
      " [0.89561149]\n",
      " [0.12166848]] \n",
      "\n",
      "198\n",
      "Outputs After the Training: \n",
      " [[0.99814667]\n",
      " [0.89604024]\n",
      " [0.89600601]\n",
      " [0.1211784 ]] \n",
      "\n",
      "199\n",
      "Outputs After the Training: \n",
      " [[0.99817054]\n",
      " [0.89643111]\n",
      " [0.89639767]\n",
      " [0.12069215]] \n",
      "\n",
      "200\n",
      "Outputs After the Training: \n",
      " [[0.998194  ]\n",
      " [0.89681916]\n",
      " [0.89678651]\n",
      " [0.12020968]] \n",
      "\n",
      "201\n",
      "Outputs After the Training: \n",
      " [[0.99821708]\n",
      " [0.89720444]\n",
      " [0.89717254]\n",
      " [0.11973094]] \n",
      "\n",
      "202\n",
      "Outputs After the Training: \n",
      " [[0.99823977]\n",
      " [0.89758698]\n",
      " [0.89755581]\n",
      " [0.1192559 ]] \n",
      "\n",
      "203\n",
      "Outputs After the Training: \n",
      " [[0.99826208]\n",
      " [0.89796679]\n",
      " [0.89793633]\n",
      " [0.11878451]] \n",
      "\n",
      "204\n",
      "Outputs After the Training: \n",
      " [[0.99828403]\n",
      " [0.8983439 ]\n",
      " [0.89831415]\n",
      " [0.11831674]] \n",
      "\n",
      "205\n",
      "Outputs After the Training: \n",
      " [[0.99830561]\n",
      " [0.89871836]\n",
      " [0.89868928]\n",
      " [0.11785253]] \n",
      "\n",
      "206\n",
      "Outputs After the Training: \n",
      " [[0.99832683]\n",
      " [0.89909017]\n",
      " [0.89906175]\n",
      " [0.11739185]] \n",
      "\n",
      "207\n",
      "Outputs After the Training: \n",
      " [[0.99834771]\n",
      " [0.89945938]\n",
      " [0.8994316 ]\n",
      " [0.11693466]] \n",
      "\n",
      "208\n",
      "Outputs After the Training: \n",
      " [[0.99836825]\n",
      " [0.89982599]\n",
      " [0.89979885]\n",
      " [0.11648092]] \n",
      "\n",
      "209\n",
      "Outputs After the Training: \n",
      " [[0.99838846]\n",
      " [0.90019006]\n",
      " [0.90016352]\n",
      " [0.1160306 ]] \n",
      "\n",
      "210\n",
      "Outputs After the Training: \n",
      " [[0.99840833]\n",
      " [0.90055159]\n",
      " [0.90052564]\n",
      " [0.11558365]] \n",
      "\n",
      "211\n",
      "Outputs After the Training: \n",
      " [[0.99842789]\n",
      " [0.90091061]\n",
      " [0.90088524]\n",
      " [0.11514003]] \n",
      "\n",
      "212\n",
      "Outputs After the Training: \n",
      " [[0.99844713]\n",
      " [0.90126715]\n",
      " [0.90124235]\n",
      " [0.11469972]] \n",
      "\n",
      "213\n",
      "Outputs After the Training: \n",
      " [[0.99846607]\n",
      " [0.90162123]\n",
      " [0.90159698]\n",
      " [0.11426267]] \n",
      "\n",
      "214\n",
      "Outputs After the Training: \n",
      " [[0.9984847 ]\n",
      " [0.90197289]\n",
      " [0.90194917]\n",
      " [0.11382885]] \n",
      "\n",
      "215\n",
      "Outputs After the Training: \n",
      " [[0.99850304]\n",
      " [0.90232213]\n",
      " [0.90229894]\n",
      " [0.11339822]] \n",
      "\n",
      "216\n",
      "Outputs After the Training: \n",
      " [[0.99852109]\n",
      " [0.90266899]\n",
      " [0.90264631]\n",
      " [0.11297075]] \n",
      "\n",
      "217\n",
      "Outputs After the Training: \n",
      " [[0.99853885]\n",
      " [0.90301349]\n",
      " [0.90299131]\n",
      " [0.11254641]] \n",
      "\n",
      "218\n",
      "Outputs After the Training: \n",
      " [[0.99855633]\n",
      " [0.90335565]\n",
      " [0.90333395]\n",
      " [0.11212515]] \n",
      "\n",
      "219\n",
      "Outputs After the Training: \n",
      " [[0.99857354]\n",
      " [0.9036955 ]\n",
      " [0.90367427]\n",
      " [0.11170696]] \n",
      "\n",
      "220\n",
      "Outputs After the Training: \n",
      " [[0.99859048]\n",
      " [0.90403306]\n",
      " [0.90401229]\n",
      " [0.11129178]] \n",
      "\n",
      "221\n",
      "Outputs After the Training: \n",
      " [[0.99860716]\n",
      " [0.90436835]\n",
      " [0.90434803]\n",
      " [0.1108796 ]] \n",
      "\n",
      "222\n",
      "Outputs After the Training: \n",
      " [[0.99862358]\n",
      " [0.90470139]\n",
      " [0.90468151]\n",
      " [0.11047038]] \n",
      "\n",
      "223\n",
      "Outputs After the Training: \n",
      " [[0.99863975]\n",
      " [0.90503221]\n",
      " [0.90501275]\n",
      " [0.11006409]] \n",
      "\n",
      "224\n",
      "Outputs After the Training: \n",
      " [[0.99865567]\n",
      " [0.90536082]\n",
      " [0.90534178]\n",
      " [0.1096607 ]] \n",
      "\n",
      "225\n",
      "Outputs After the Training: \n",
      " [[0.99867134]\n",
      " [0.90568726]\n",
      " [0.90566862]\n",
      " [0.10926017]] \n",
      "\n",
      "226\n",
      "Outputs After the Training: \n",
      " [[0.99868678]\n",
      " [0.90601153]\n",
      " [0.90599329]\n",
      " [0.10886248]] \n",
      "\n",
      "227\n",
      "Outputs After the Training: \n",
      " [[0.99870198]\n",
      " [0.90633366]\n",
      " [0.9063158 ]\n",
      " [0.1084676 ]] \n",
      "\n",
      "228\n",
      "Outputs After the Training: \n",
      " [[0.99871695]\n",
      " [0.90665367]\n",
      " [0.90663619]\n",
      " [0.1080755 ]] \n",
      "\n",
      "229\n",
      "Outputs After the Training: \n",
      " [[0.9987317 ]\n",
      " [0.90697158]\n",
      " [0.90695447]\n",
      " [0.10768615]] \n",
      "\n",
      "230\n",
      "Outputs After the Training: \n",
      " [[0.99874622]\n",
      " [0.90728742]\n",
      " [0.90727067]\n",
      " [0.10729952]] \n",
      "\n",
      "231\n",
      "Outputs After the Training: \n",
      " [[0.99876053]\n",
      " [0.90760119]\n",
      " [0.90758479]\n",
      " [0.10691558]] \n",
      "\n",
      "232\n",
      "Outputs After the Training: \n",
      " [[0.99877462]\n",
      " [0.90791292]\n",
      " [0.90789687]\n",
      " [0.10653431]] \n",
      "\n",
      "233\n",
      "Outputs After the Training: \n",
      " [[0.9987885 ]\n",
      " [0.90822264]\n",
      " [0.90820691]\n",
      " [0.10615568]] \n",
      "\n",
      "234\n",
      "Outputs After the Training: \n",
      " [[0.99880218]\n",
      " [0.90853035]\n",
      " [0.90851495]\n",
      " [0.10577966]] \n",
      "\n",
      "235\n",
      "Outputs After the Training: \n",
      " [[0.99881565]\n",
      " [0.90883608]\n",
      " [0.908821  ]\n",
      " [0.10540622]] \n",
      "\n",
      "236\n",
      "Outputs After the Training: \n",
      " [[0.99882893]\n",
      " [0.90913984]\n",
      " [0.90912507]\n",
      " [0.10503534]] \n",
      "\n",
      "237\n",
      "Outputs After the Training: \n",
      " [[0.99884201]\n",
      " [0.90944166]\n",
      " [0.90942719]\n",
      " [0.10466699]] \n",
      "\n",
      "238\n",
      "Outputs After the Training: \n",
      " [[0.9988549 ]\n",
      " [0.90974155]\n",
      " [0.90972738]\n",
      " [0.10430115]] \n",
      "\n",
      "239\n",
      "Outputs After the Training: \n",
      " [[0.9988676 ]\n",
      " [0.91003954]\n",
      " [0.91002565]\n",
      " [0.1039378 ]] \n",
      "\n",
      "240\n",
      "Outputs After the Training: \n",
      " [[0.99888012]\n",
      " [0.91033562]\n",
      " [0.91032202]\n",
      " [0.1035769 ]] \n",
      "\n",
      "241\n",
      "Outputs After the Training: \n",
      " [[0.99889245]\n",
      " [0.91062984]\n",
      " [0.91061651]\n",
      " [0.10321843]] \n",
      "\n",
      "242\n",
      "Outputs After the Training: \n",
      " [[0.99890461]\n",
      " [0.9109222 ]\n",
      " [0.91090914]\n",
      " [0.10286237]] \n",
      "\n",
      "243\n",
      "Outputs After the Training: \n",
      " [[0.9989166 ]\n",
      " [0.91121272]\n",
      " [0.91119992]\n",
      " [0.10250869]] \n",
      "\n",
      "244\n",
      "Outputs After the Training: \n",
      " [[0.99892841]\n",
      " [0.91150141]\n",
      " [0.91148887]\n",
      " [0.10215738]] \n",
      "\n",
      "245\n",
      "Outputs After the Training: \n",
      " [[0.99894005]\n",
      " [0.9117883 ]\n",
      " [0.91177601]\n",
      " [0.1018084 ]] \n",
      "\n",
      "246\n",
      "Outputs After the Training: \n",
      " [[0.99895153]\n",
      " [0.9120734 ]\n",
      " [0.91206135]\n",
      " [0.10146174]] \n",
      "\n",
      "247\n",
      "Outputs After the Training: \n",
      " [[0.99896284]\n",
      " [0.91235672]\n",
      " [0.91234492]\n",
      " [0.10111737]] \n",
      "\n",
      "248\n",
      "Outputs After the Training: \n",
      " [[0.998974  ]\n",
      " [0.91263829]\n",
      " [0.91262672]\n",
      " [0.10077527]] \n",
      "\n",
      "249\n",
      "Outputs After the Training: \n",
      " [[0.998985  ]\n",
      " [0.91291811]\n",
      " [0.91290677]\n",
      " [0.10043542]] \n",
      "\n",
      "250\n",
      "Outputs After the Training: \n",
      " [[0.99899584]\n",
      " [0.91319621]\n",
      " [0.91318509]\n",
      " [0.10009779]] \n",
      "\n",
      "251\n",
      "Outputs After the Training: \n",
      " [[0.99900653]\n",
      " [0.9134726 ]\n",
      " [0.9134617 ]\n",
      " [0.09976237]] \n",
      "\n",
      "252\n",
      "Outputs After the Training: \n",
      " [[0.99901707]\n",
      " [0.91374729]\n",
      " [0.9137366 ]\n",
      " [0.09942913]] \n",
      "\n",
      "253\n",
      "Outputs After the Training: \n",
      " [[0.99902747]\n",
      " [0.9140203 ]\n",
      " [0.91400982]\n",
      " [0.09909805]] \n",
      "\n",
      "254\n",
      "Outputs After the Training: \n",
      " [[0.99903772]\n",
      " [0.91429165]\n",
      " [0.91428137]\n",
      " [0.09876912]] \n",
      "\n",
      "255\n",
      "Outputs After the Training: \n",
      " [[0.99904783]\n",
      " [0.91456134]\n",
      " [0.91455127]\n",
      " [0.0984423 ]] \n",
      "\n",
      "256\n",
      "Outputs After the Training: \n",
      " [[0.9990578 ]\n",
      " [0.91482941]\n",
      " [0.91481953]\n",
      " [0.09811759]] \n",
      "\n",
      "257\n",
      "Outputs After the Training: \n",
      " [[0.99906764]\n",
      " [0.91509585]\n",
      " [0.91508616]\n",
      " [0.09779496]] \n",
      "\n",
      "258\n",
      "Outputs After the Training: \n",
      " [[0.99907734]\n",
      " [0.91536068]\n",
      " [0.91535118]\n",
      " [0.09747438]] \n",
      "\n",
      "259\n",
      "Outputs After the Training: \n",
      " [[0.9990869 ]\n",
      " [0.91562392]\n",
      " [0.9156146 ]\n",
      " [0.09715585]] \n",
      "\n",
      "260\n",
      "Outputs After the Training: \n",
      " [[0.99909634]\n",
      " [0.91588558]\n",
      " [0.91587644]\n",
      " [0.09683935]] \n",
      "\n",
      "261\n",
      "Outputs After the Training: \n",
      " [[0.99910565]\n",
      " [0.91614568]\n",
      " [0.91613671]\n",
      " [0.09652484]] \n",
      "\n",
      "262\n",
      "Outputs After the Training: \n",
      " [[0.99911484]\n",
      " [0.91640422]\n",
      " [0.91639542]\n",
      " [0.09621233]] \n",
      "\n",
      "263\n",
      "Outputs After the Training: \n",
      " [[0.9991239 ]\n",
      " [0.91666123]\n",
      " [0.9166526 ]\n",
      " [0.09590177]] \n",
      "\n",
      "264\n",
      "Outputs After the Training: \n",
      " [[0.99913283]\n",
      " [0.91691671]\n",
      " [0.91690825]\n",
      " [0.09559317]] \n",
      "\n",
      "265\n",
      "Outputs After the Training: \n",
      " [[0.99914165]\n",
      " [0.91717069]\n",
      " [0.91716238]\n",
      " [0.0952865 ]] \n",
      "\n",
      "266\n",
      "Outputs After the Training: \n",
      " [[0.99915036]\n",
      " [0.91742316]\n",
      " [0.91741501]\n",
      " [0.09498173]] \n",
      "\n",
      "267\n",
      "Outputs After the Training: \n",
      " [[0.99915894]\n",
      " [0.91767415]\n",
      " [0.91766615]\n",
      " [0.09467887]] \n",
      "\n",
      "268\n",
      "Outputs After the Training: \n",
      " [[0.99916741]\n",
      " [0.91792366]\n",
      " [0.91791581]\n",
      " [0.09437788]] \n",
      "\n",
      "269\n",
      "Outputs After the Training: \n",
      " [[0.99917577]\n",
      " [0.91817172]\n",
      " [0.91816402]\n",
      " [0.09407875]] \n",
      "\n",
      "270\n",
      "Outputs After the Training: \n",
      " [[0.99918402]\n",
      " [0.91841833]\n",
      " [0.91841077]\n",
      " [0.09378146]] \n",
      "\n",
      "271\n",
      "Outputs After the Training: \n",
      " [[0.99919217]\n",
      " [0.9186635 ]\n",
      " [0.91865608]\n",
      " [0.093486  ]] \n",
      "\n",
      "272\n",
      "Outputs After the Training: \n",
      " [[0.9992002 ]\n",
      " [0.91890725]\n",
      " [0.91889997]\n",
      " [0.09319235]] \n",
      "\n",
      "273\n",
      "Outputs After the Training: \n",
      " [[0.99920813]\n",
      " [0.91914959]\n",
      " [0.91914244]\n",
      " [0.0929005 ]] \n",
      "\n",
      "274\n",
      "Outputs After the Training: \n",
      " [[0.99921596]\n",
      " [0.91939054]\n",
      " [0.91938352]\n",
      " [0.09261042]] \n",
      "\n",
      "275\n",
      "Outputs After the Training: \n",
      " [[0.99922369]\n",
      " [0.91963009]\n",
      " [0.9196232 ]\n",
      " [0.0923221 ]] \n",
      "\n",
      "276\n",
      "Outputs After the Training: \n",
      " [[0.99923131]\n",
      " [0.91986827]\n",
      " [0.9198615 ]\n",
      " [0.09203553]] \n",
      "\n",
      "277\n",
      "Outputs After the Training: \n",
      " [[0.99923884]\n",
      " [0.92010508]\n",
      " [0.92009844]\n",
      " [0.09175069]] \n",
      "\n",
      "278\n",
      "Outputs After the Training: \n",
      " [[0.99924627]\n",
      " [0.92034055]\n",
      " [0.92033403]\n",
      " [0.09146756]] \n",
      "\n",
      "279\n",
      "Outputs After the Training: \n",
      " [[0.99925361]\n",
      " [0.92057467]\n",
      " [0.92056827]\n",
      " [0.09118613]] \n",
      "\n",
      "280\n",
      "Outputs After the Training: \n",
      " [[0.99926085]\n",
      " [0.92080746]\n",
      " [0.92080117]\n",
      " [0.09090639]] \n",
      "\n",
      "281\n",
      "Outputs After the Training: \n",
      " [[0.999268  ]\n",
      " [0.92103893]\n",
      " [0.92103276]\n",
      " [0.09062832]] \n",
      "\n",
      "282\n",
      "Outputs After the Training: \n",
      " [[0.99927506]\n",
      " [0.9212691 ]\n",
      " [0.92126304]\n",
      " [0.0903519 ]] \n",
      "\n",
      "283\n",
      "Outputs After the Training: \n",
      " [[0.99928203]\n",
      " [0.92149797]\n",
      " [0.92149201]\n",
      " [0.09007712]] \n",
      "\n",
      "284\n",
      "Outputs After the Training: \n",
      " [[0.99928891]\n",
      " [0.92172555]\n",
      " [0.9217197 ]\n",
      " [0.08980397]] \n",
      "\n",
      "285\n",
      "Outputs After the Training: \n",
      " [[0.99929571]\n",
      " [0.92195185]\n",
      " [0.92194611]\n",
      " [0.08953243]] \n",
      "\n",
      "286\n",
      "Outputs After the Training: \n",
      " [[0.99930242]\n",
      " [0.92217689]\n",
      " [0.92217125]\n",
      " [0.08926248]] \n",
      "\n",
      "287\n",
      "Outputs After the Training: \n",
      " [[0.99930904]\n",
      " [0.92240068]\n",
      " [0.92239514]\n",
      " [0.08899412]] \n",
      "\n",
      "288\n",
      "Outputs After the Training: \n",
      " [[0.99931559]\n",
      " [0.92262322]\n",
      " [0.92261777]\n",
      " [0.08872734]] \n",
      "\n",
      "289\n",
      "Outputs After the Training: \n",
      " [[0.99932205]\n",
      " [0.92284453]\n",
      " [0.92283918]\n",
      " [0.0884621 ]] \n",
      "\n",
      "290\n",
      "Outputs After the Training: \n",
      " [[0.99932843]\n",
      " [0.92306461]\n",
      " [0.92305935]\n",
      " [0.08819841]] \n",
      "\n",
      "291\n",
      "Outputs After the Training: \n",
      " [[0.99933474]\n",
      " [0.92328347]\n",
      " [0.92327831]\n",
      " [0.08793625]] \n",
      "\n",
      "292\n",
      "Outputs After the Training: \n",
      " [[0.99934096]\n",
      " [0.92350114]\n",
      " [0.92349606]\n",
      " [0.08767561]] \n",
      "\n",
      "293\n",
      "Outputs After the Training: \n",
      " [[0.99934711]\n",
      " [0.9237176 ]\n",
      " [0.92371262]\n",
      " [0.08741647]] \n",
      "\n",
      "294\n",
      "Outputs After the Training: \n",
      " [[0.99935319]\n",
      " [0.92393288]\n",
      " [0.92392798]\n",
      " [0.08715882]] \n",
      "\n",
      "295\n",
      "Outputs After the Training: \n",
      " [[0.99935919]\n",
      " [0.92414699]\n",
      " [0.92414217]\n",
      " [0.08690265]] \n",
      "\n",
      "296\n",
      "Outputs After the Training: \n",
      " [[0.99936511]\n",
      " [0.92435993]\n",
      " [0.92435519]\n",
      " [0.08664795]] \n",
      "\n",
      "297\n",
      "Outputs After the Training: \n",
      " [[0.99937097]\n",
      " [0.92457171]\n",
      " [0.92456706]\n",
      " [0.0863947 ]] \n",
      "\n",
      "298\n",
      "Outputs After the Training: \n",
      " [[0.99937675]\n",
      " [0.92478234]\n",
      " [0.92477777]\n",
      " [0.08614289]] \n",
      "\n",
      "299\n",
      "Outputs After the Training: \n",
      " [[0.99938247]\n",
      " [0.92499183]\n",
      " [0.92498734]\n",
      " [0.08589251]] \n",
      "\n",
      "300\n",
      "Outputs After the Training: \n",
      " [[0.99938811]\n",
      " [0.9252002 ]\n",
      " [0.92519578]\n",
      " [0.08564354]] \n",
      "\n",
      "301\n",
      "Outputs After the Training: \n",
      " [[0.99939369]\n",
      " [0.92540744]\n",
      " [0.9254031 ]\n",
      " [0.08539599]] \n",
      "\n",
      "302\n",
      "Outputs After the Training: \n",
      " [[0.9993992 ]\n",
      " [0.92561357]\n",
      " [0.9256093 ]\n",
      " [0.08514982]] \n",
      "\n",
      "303\n",
      "Outputs After the Training: \n",
      " [[0.99940465]\n",
      " [0.9258186 ]\n",
      " [0.9258144 ]\n",
      " [0.08490504]] \n",
      "\n",
      "304\n",
      "Outputs After the Training: \n",
      " [[0.99941003]\n",
      " [0.92602253]\n",
      " [0.92601841]\n",
      " [0.08466163]] \n",
      "\n",
      "305\n",
      "Outputs After the Training: \n",
      " [[0.99941535]\n",
      " [0.92622537]\n",
      " [0.92622132]\n",
      " [0.08441958]] \n",
      "\n",
      "306\n",
      "Outputs After the Training: \n",
      " [[0.9994206 ]\n",
      " [0.92642714]\n",
      " [0.92642316]\n",
      " [0.08417887]] \n",
      "\n",
      "307\n",
      "Outputs After the Training: \n",
      " [[0.99942579]\n",
      " [0.92662784]\n",
      " [0.92662392]\n",
      " [0.0839395 ]] \n",
      "\n",
      "308\n",
      "Outputs After the Training: \n",
      " [[0.99943092]\n",
      " [0.92682748]\n",
      " [0.92682363]\n",
      " [0.08370146]] \n",
      "\n",
      "309\n",
      "Outputs After the Training: \n",
      " [[0.99943599]\n",
      " [0.92702606]\n",
      " [0.92702228]\n",
      " [0.08346474]] \n",
      "\n",
      "310\n",
      "Outputs After the Training: \n",
      " [[0.999441  ]\n",
      " [0.9272236 ]\n",
      " [0.92721988]\n",
      " [0.08322931]] \n",
      "\n",
      "311\n",
      "Outputs After the Training: \n",
      " [[0.99944596]\n",
      " [0.92742011]\n",
      " [0.92741644]\n",
      " [0.08299519]] \n",
      "\n",
      "312\n",
      "Outputs After the Training: \n",
      " [[0.99945085]\n",
      " [0.92761558]\n",
      " [0.92761198]\n",
      " [0.08276234]] \n",
      "\n",
      "313\n",
      "Outputs After the Training: \n",
      " [[0.99945569]\n",
      " [0.92781004]\n",
      " [0.92780649]\n",
      " [0.08253077]] \n",
      "\n",
      "314\n",
      "Outputs After the Training: \n",
      " [[0.99946047]\n",
      " [0.92800348]\n",
      " [0.92799999]\n",
      " [0.08230046]] \n",
      "\n",
      "315\n",
      "Outputs After the Training: \n",
      " [[0.9994652 ]\n",
      " [0.92819591]\n",
      " [0.92819248]\n",
      " [0.0820714 ]] \n",
      "\n",
      "316\n",
      "Outputs After the Training: \n",
      " [[0.99946987]\n",
      " [0.92838735]\n",
      " [0.92838398]\n",
      " [0.08184359]] \n",
      "\n",
      "317\n",
      "Outputs After the Training: \n",
      " [[0.99947449]\n",
      " [0.9285778 ]\n",
      " [0.92857448]\n",
      " [0.08161701]] \n",
      "\n",
      "318\n",
      "Outputs After the Training: \n",
      " [[0.99947906]\n",
      " [0.92876726]\n",
      " [0.928764  ]\n",
      " [0.08139165]] \n",
      "\n",
      "319\n",
      "Outputs After the Training: \n",
      " [[0.99948357]\n",
      " [0.92895576]\n",
      " [0.92895255]\n",
      " [0.0811675 ]] \n",
      "\n",
      "320\n",
      "Outputs After the Training: \n",
      " [[0.99948803]\n",
      " [0.92914328]\n",
      " [0.92914012]\n",
      " [0.08094456]] \n",
      "\n",
      "321\n",
      "Outputs After the Training: \n",
      " [[0.99949245]\n",
      " [0.92932984]\n",
      " [0.92932674]\n",
      " [0.08072281]] \n",
      "\n",
      "322\n",
      "Outputs After the Training: \n",
      " [[0.99949681]\n",
      " [0.92951545]\n",
      " [0.92951239]\n",
      " [0.08050225]] \n",
      "\n",
      "323\n",
      "Outputs After the Training: \n",
      " [[0.99950112]\n",
      " [0.92970011]\n",
      " [0.92969711]\n",
      " [0.08028285]] \n",
      "\n",
      "324\n",
      "Outputs After the Training: \n",
      " [[0.99950539]\n",
      " [0.92988384]\n",
      " [0.92988088]\n",
      " [0.08006463]] \n",
      "\n",
      "325\n",
      "Outputs After the Training: \n",
      " [[0.9995096 ]\n",
      " [0.93006663]\n",
      " [0.93006372]\n",
      " [0.07984756]] \n",
      "\n",
      "326\n",
      "Outputs After the Training: \n",
      " [[0.99951377]\n",
      " [0.9302485 ]\n",
      " [0.93024563]\n",
      " [0.07963164]] \n",
      "\n",
      "327\n",
      "Outputs After the Training: \n",
      " [[0.99951789]\n",
      " [0.93042945]\n",
      " [0.93042663]\n",
      " [0.07941686]] \n",
      "\n",
      "328\n",
      "Outputs After the Training: \n",
      " [[0.99952197]\n",
      " [0.93060949]\n",
      " [0.93060671]\n",
      " [0.07920321]] \n",
      "\n",
      "329\n",
      "Outputs After the Training: \n",
      " [[0.999526  ]\n",
      " [0.93078862]\n",
      " [0.93078589]\n",
      " [0.07899068]] \n",
      "\n",
      "330\n",
      "Outputs After the Training: \n",
      " [[0.99952999]\n",
      " [0.93096686]\n",
      " [0.93096417]\n",
      " [0.07877926]] \n",
      "\n",
      "331\n",
      "Outputs After the Training: \n",
      " [[0.99953393]\n",
      " [0.9311442 ]\n",
      " [0.93114156]\n",
      " [0.07856894]] \n",
      "\n",
      "332\n",
      "Outputs After the Training: \n",
      " [[0.99953783]\n",
      " [0.93132066]\n",
      " [0.93131806]\n",
      " [0.07835972]] \n",
      "\n",
      "333\n",
      "Outputs After the Training: \n",
      " [[0.99954169]\n",
      " [0.93149624]\n",
      " [0.93149368]\n",
      " [0.07815159]] \n",
      "\n",
      "334\n",
      "Outputs After the Training: \n",
      " [[0.9995455 ]\n",
      " [0.93167095]\n",
      " [0.93166843]\n",
      " [0.07794453]] \n",
      "\n",
      "335\n",
      "Outputs After the Training: \n",
      " [[0.99954927]\n",
      " [0.9318448 ]\n",
      " [0.93184231]\n",
      " [0.07773855]] \n",
      "\n",
      "336\n",
      "Outputs After the Training: \n",
      " [[0.999553  ]\n",
      " [0.93201778]\n",
      " [0.93201534]\n",
      " [0.07753363]] \n",
      "\n",
      "337\n",
      "Outputs After the Training: \n",
      " [[0.99955669]\n",
      " [0.93218991]\n",
      " [0.93218751]\n",
      " [0.07732976]] \n",
      "\n",
      "338\n",
      "Outputs After the Training: \n",
      " [[0.99956034]\n",
      " [0.9323612 ]\n",
      " [0.93235883]\n",
      " [0.07712693]] \n",
      "\n",
      "339\n",
      "Outputs After the Training: \n",
      " [[0.99956396]\n",
      " [0.93253164]\n",
      " [0.93252931]\n",
      " [0.07692515]] \n",
      "\n",
      "340\n",
      "Outputs After the Training: \n",
      " [[0.99956753]\n",
      " [0.93270125]\n",
      " [0.93269895]\n",
      " [0.07672439]] \n",
      "\n",
      "341\n",
      "Outputs After the Training: \n",
      " [[0.99957106]\n",
      " [0.93287003]\n",
      " [0.93286777]\n",
      " [0.07652466]] \n",
      "\n",
      "342\n",
      "Outputs After the Training: \n",
      " [[0.99957456]\n",
      " [0.93303799]\n",
      " [0.93303576]\n",
      " [0.07632594]] \n",
      "\n",
      "343\n",
      "Outputs After the Training: \n",
      " [[0.99957801]\n",
      " [0.93320513]\n",
      " [0.93320294]\n",
      " [0.07612823]] \n",
      "\n",
      "344\n",
      "Outputs After the Training: \n",
      " [[0.99958144]\n",
      " [0.93337146]\n",
      " [0.9333693 ]\n",
      " [0.07593152]] \n",
      "\n",
      "345\n",
      "Outputs After the Training: \n",
      " [[0.99958482]\n",
      " [0.93353699]\n",
      " [0.93353486]\n",
      " [0.0757358 ]] \n",
      "\n",
      "346\n",
      "Outputs After the Training: \n",
      " [[0.99958817]\n",
      " [0.93370171]\n",
      " [0.93369962]\n",
      " [0.07554107]] \n",
      "\n",
      "347\n",
      "Outputs After the Training: \n",
      " [[0.99959148]\n",
      " [0.93386565]\n",
      " [0.93386359]\n",
      " [0.07534731]] \n",
      "\n",
      "348\n",
      "Outputs After the Training: \n",
      " [[0.99959476]\n",
      " [0.93402879]\n",
      " [0.93402676]\n",
      " [0.07515452]] \n",
      "\n",
      "349\n",
      "Outputs After the Training: \n",
      " [[0.999598  ]\n",
      " [0.93419115]\n",
      " [0.93418915]\n",
      " [0.07496269]] \n",
      "\n",
      "350\n",
      "Outputs After the Training: \n",
      " [[0.99960121]\n",
      " [0.93435274]\n",
      " [0.93435077]\n",
      " [0.07477183]] \n",
      "\n",
      "351\n",
      "Outputs After the Training: \n",
      " [[0.99960439]\n",
      " [0.93451355]\n",
      " [0.93451161]\n",
      " [0.07458191]] \n",
      "\n",
      "352\n",
      "Outputs After the Training: \n",
      " [[0.99960753]\n",
      " [0.9346736 ]\n",
      " [0.93467169]\n",
      " [0.07439293]] \n",
      "\n",
      "353\n",
      "Outputs After the Training: \n",
      " [[0.99961064]\n",
      " [0.93483288]\n",
      " [0.934831  ]\n",
      " [0.07420488]] \n",
      "\n",
      "354\n",
      "Outputs After the Training: \n",
      " [[0.99961372]\n",
      " [0.93499141]\n",
      " [0.93498956]\n",
      " [0.07401777]] \n",
      "\n",
      "355\n",
      "Outputs After the Training: \n",
      " [[0.99961676]\n",
      " [0.93514919]\n",
      " [0.93514737]\n",
      " [0.07383157]] \n",
      "\n",
      "356\n",
      "Outputs After the Training: \n",
      " [[0.99961978]\n",
      " [0.93530623]\n",
      " [0.93530443]\n",
      " [0.07364629]] \n",
      "\n",
      "357\n",
      "Outputs After the Training: \n",
      " [[0.99962276]\n",
      " [0.93546253]\n",
      " [0.93546075]\n",
      " [0.07346192]] \n",
      "\n",
      "358\n",
      "Outputs After the Training: \n",
      " [[0.99962571]\n",
      " [0.93561809]\n",
      " [0.93561634]\n",
      " [0.07327845]] \n",
      "\n",
      "359\n",
      "Outputs After the Training: \n",
      " [[0.99962863]\n",
      " [0.93577292]\n",
      " [0.9357712 ]\n",
      " [0.07309588]] \n",
      "\n",
      "360\n",
      "Outputs After the Training: \n",
      " [[0.99963152]\n",
      " [0.93592702]\n",
      " [0.93592533]\n",
      " [0.07291419]] \n",
      "\n",
      "361\n",
      "Outputs After the Training: \n",
      " [[0.99963438]\n",
      " [0.93608041]\n",
      " [0.93607874]\n",
      " [0.07273338]] \n",
      "\n",
      "362\n",
      "Outputs After the Training: \n",
      " [[0.99963722]\n",
      " [0.93623308]\n",
      " [0.93623144]\n",
      " [0.07255345]] \n",
      "\n",
      "363\n",
      "Outputs After the Training: \n",
      " [[0.99964002]\n",
      " [0.93638505]\n",
      " [0.93638342]\n",
      " [0.07237439]] \n",
      "\n",
      "364\n",
      "Outputs After the Training: \n",
      " [[0.9996428 ]\n",
      " [0.9365363 ]\n",
      " [0.93653471]\n",
      " [0.0721962 ]] \n",
      "\n",
      "365\n",
      "Outputs After the Training: \n",
      " [[0.99964554]\n",
      " [0.93668686]\n",
      " [0.93668529]\n",
      " [0.07201886]] \n",
      "\n",
      "366\n",
      "Outputs After the Training: \n",
      " [[0.99964826]\n",
      " [0.93683672]\n",
      " [0.93683517]\n",
      " [0.07184237]] \n",
      "\n",
      "367\n",
      "Outputs After the Training: \n",
      " [[0.99965095]\n",
      " [0.9369859 ]\n",
      " [0.93698437]\n",
      " [0.07166673]] \n",
      "\n",
      "368\n",
      "Outputs After the Training: \n",
      " [[0.99965362]\n",
      " [0.93713438]\n",
      " [0.93713287]\n",
      " [0.07149192]] \n",
      "\n",
      "369\n",
      "Outputs After the Training: \n",
      " [[0.99965626]\n",
      " [0.93728219]\n",
      " [0.9372807 ]\n",
      " [0.07131795]] \n",
      "\n",
      "370\n",
      "Outputs After the Training: \n",
      " [[0.99965887]\n",
      " [0.93742931]\n",
      " [0.93742785]\n",
      " [0.07114481]] \n",
      "\n",
      "371\n",
      "Outputs After the Training: \n",
      " [[0.99966145]\n",
      " [0.93757577]\n",
      " [0.93757433]\n",
      " [0.07097248]] \n",
      "\n",
      "372\n",
      "Outputs After the Training: \n",
      " [[0.99966401]\n",
      " [0.93772156]\n",
      " [0.93772013]\n",
      " [0.07080097]] \n",
      "\n",
      "373\n",
      "Outputs After the Training: \n",
      " [[0.99966655]\n",
      " [0.93786668]\n",
      " [0.93786528]\n",
      " [0.07063028]] \n",
      "\n",
      "374\n",
      "Outputs After the Training: \n",
      " [[0.99966906]\n",
      " [0.93801115]\n",
      " [0.93800977]\n",
      " [0.07046038]] \n",
      "\n",
      "375\n",
      "Outputs After the Training: \n",
      " [[0.99967154]\n",
      " [0.93815496]\n",
      " [0.9381536 ]\n",
      " [0.07029129]] \n",
      "\n",
      "376\n",
      "Outputs After the Training: \n",
      " [[0.999674  ]\n",
      " [0.93829812]\n",
      " [0.93829678]\n",
      " [0.07012299]] \n",
      "\n",
      "377\n",
      "Outputs After the Training: \n",
      " [[0.99967643]\n",
      " [0.93844064]\n",
      " [0.93843931]\n",
      " [0.06995547]] \n",
      "\n",
      "378\n",
      "Outputs After the Training: \n",
      " [[0.99967885]\n",
      " [0.93858251]\n",
      " [0.93858121]\n",
      " [0.06978874]] \n",
      "\n",
      "379\n",
      "Outputs After the Training: \n",
      " [[0.99968123]\n",
      " [0.93872375]\n",
      " [0.93872246]\n",
      " [0.06962279]] \n",
      "\n",
      "380\n",
      "Outputs After the Training: \n",
      " [[0.9996836 ]\n",
      " [0.93886436]\n",
      " [0.93886309]\n",
      " [0.0694576 ]] \n",
      "\n",
      "381\n",
      "Outputs After the Training: \n",
      " [[0.99968594]\n",
      " [0.93900433]\n",
      " [0.93900308]\n",
      " [0.06929319]] \n",
      "\n",
      "382\n",
      "Outputs After the Training: \n",
      " [[0.99968826]\n",
      " [0.93914368]\n",
      " [0.93914245]\n",
      " [0.06912953]] \n",
      "\n",
      "383\n",
      "Outputs After the Training: \n",
      " [[0.99969055]\n",
      " [0.93928242]\n",
      " [0.9392812 ]\n",
      " [0.06896663]] \n",
      "\n",
      "384\n",
      "Outputs After the Training: \n",
      " [[0.99969283]\n",
      " [0.93942053]\n",
      " [0.93941933]\n",
      " [0.06880448]] \n",
      "\n",
      "385\n",
      "Outputs After the Training: \n",
      " [[0.99969508]\n",
      " [0.93955803]\n",
      " [0.93955685]\n",
      " [0.06864307]] \n",
      "\n",
      "386\n",
      "Outputs After the Training: \n",
      " [[0.99969731]\n",
      " [0.93969493]\n",
      " [0.93969376]\n",
      " [0.06848241]] \n",
      "\n",
      "387\n",
      "Outputs After the Training: \n",
      " [[0.99969952]\n",
      " [0.93983122]\n",
      " [0.93983007]\n",
      " [0.06832248]] \n",
      "\n",
      "388\n",
      "Outputs After the Training: \n",
      " [[0.9997017 ]\n",
      " [0.93996691]\n",
      " [0.93996577]\n",
      " [0.06816328]] \n",
      "\n",
      "389\n",
      "Outputs After the Training: \n",
      " [[0.99970387]\n",
      " [0.940102  ]\n",
      " [0.94010088]\n",
      " [0.06800481]] \n",
      "\n",
      "390\n",
      "Outputs After the Training: \n",
      " [[0.99970602]\n",
      " [0.9402365 ]\n",
      " [0.9402354 ]\n",
      " [0.06784705]] \n",
      "\n",
      "391\n",
      "Outputs After the Training: \n",
      " [[0.99970814]\n",
      " [0.94037041]\n",
      " [0.94036932]\n",
      " [0.06769001]] \n",
      "\n",
      "392\n",
      "Outputs After the Training: \n",
      " [[0.99971025]\n",
      " [0.94050374]\n",
      " [0.94050266]\n",
      " [0.06753369]] \n",
      "\n",
      "393\n",
      "Outputs After the Training: \n",
      " [[0.99971233]\n",
      " [0.94063648]\n",
      " [0.94063542]\n",
      " [0.06737807]] \n",
      "\n",
      "394\n",
      "Outputs After the Training: \n",
      " [[0.9997144 ]\n",
      " [0.94076865]\n",
      " [0.9407676 ]\n",
      " [0.06722315]] \n",
      "\n",
      "395\n",
      "Outputs After the Training: \n",
      " [[0.99971644]\n",
      " [0.94090024]\n",
      " [0.94089921]\n",
      " [0.06706893]] \n",
      "\n",
      "396\n",
      "Outputs After the Training: \n",
      " [[0.99971847]\n",
      " [0.94103127]\n",
      " [0.94103025]\n",
      " [0.06691539]] \n",
      "\n",
      "397\n",
      "Outputs After the Training: \n",
      " [[0.99972048]\n",
      " [0.94116172]\n",
      " [0.94116072]\n",
      " [0.06676255]] \n",
      "\n",
      "398\n",
      "Outputs After the Training: \n",
      " [[0.99972246]\n",
      " [0.94129162]\n",
      " [0.94129063]\n",
      " [0.06661039]] \n",
      "\n",
      "399\n",
      "Outputs After the Training: \n",
      " [[0.99972443]\n",
      " [0.94142095]\n",
      " [0.94141997]\n",
      " [0.06645891]] \n",
      "\n",
      "400\n",
      "Outputs After the Training: \n",
      " [[0.99972639]\n",
      " [0.94154973]\n",
      " [0.94154877]\n",
      " [0.0663081 ]] \n",
      "\n",
      "401\n",
      "Outputs After the Training: \n",
      " [[0.99972832]\n",
      " [0.94167796]\n",
      " [0.94167701]\n",
      " [0.06615796]] \n",
      "\n",
      "402\n",
      "Outputs After the Training: \n",
      " [[0.99973023]\n",
      " [0.94180563]\n",
      " [0.9418047 ]\n",
      " [0.06600849]] \n",
      "\n",
      "403\n",
      "Outputs After the Training: \n",
      " [[0.99973213]\n",
      " [0.94193276]\n",
      " [0.94193184]\n",
      " [0.06585968]] \n",
      "\n",
      "404\n",
      "Outputs After the Training: \n",
      " [[0.99973401]\n",
      " [0.94205935]\n",
      " [0.94205844]\n",
      " [0.06571152]] \n",
      "\n",
      "405\n",
      "Outputs After the Training: \n",
      " [[0.99973587]\n",
      " [0.94218541]\n",
      " [0.94218451]\n",
      " [0.06556402]] \n",
      "\n",
      "406\n",
      "Outputs After the Training: \n",
      " [[0.99973772]\n",
      " [0.94231092]\n",
      " [0.94231003]\n",
      " [0.06541716]] \n",
      "\n",
      "407\n",
      "Outputs After the Training: \n",
      " [[0.99973955]\n",
      " [0.94243591]\n",
      " [0.94243503]\n",
      " [0.06527095]] \n",
      "\n",
      "408\n",
      "Outputs After the Training: \n",
      " [[0.99974136]\n",
      " [0.94256036]\n",
      " [0.9425595 ]\n",
      " [0.06512538]] \n",
      "\n",
      "409\n",
      "Outputs After the Training: \n",
      " [[0.99974315]\n",
      " [0.94268429]\n",
      " [0.94268344]\n",
      " [0.06498044]] \n",
      "\n",
      "410\n",
      "Outputs After the Training: \n",
      " [[0.99974493]\n",
      " [0.9428077 ]\n",
      " [0.94280686]\n",
      " [0.06483614]] \n",
      "\n",
      "411\n",
      "Outputs After the Training: \n",
      " [[0.9997467 ]\n",
      " [0.94293059]\n",
      " [0.94292976]\n",
      " [0.06469246]] \n",
      "\n",
      "412\n",
      "Outputs After the Training: \n",
      " [[0.99974844]\n",
      " [0.94305297]\n",
      " [0.94305215]\n",
      " [0.0645494 ]] \n",
      "\n",
      "413\n",
      "Outputs After the Training: \n",
      " [[0.99975017]\n",
      " [0.94317483]\n",
      " [0.94317402]\n",
      " [0.06440697]] \n",
      "\n",
      "414\n",
      "Outputs After the Training: \n",
      " [[0.99975189]\n",
      " [0.94329618]\n",
      " [0.94329539]\n",
      " [0.06426515]] \n",
      "\n",
      "415\n",
      "Outputs After the Training: \n",
      " [[0.99975359]\n",
      " [0.94341703]\n",
      " [0.94341624]\n",
      " [0.06412394]] \n",
      "\n",
      "416\n",
      "Outputs After the Training: \n",
      " [[0.99975527]\n",
      " [0.94353738]\n",
      " [0.9435366 ]\n",
      " [0.06398333]] \n",
      "\n",
      "417\n",
      "Outputs After the Training: \n",
      " [[0.99975694]\n",
      " [0.94365722]\n",
      " [0.94365645]\n",
      " [0.06384333]] \n",
      "\n",
      "418\n",
      "Outputs After the Training: \n",
      " [[0.99975859]\n",
      " [0.94377657]\n",
      " [0.94377581]\n",
      " [0.06370394]] \n",
      "\n",
      "419\n",
      "Outputs After the Training: \n",
      " [[0.99976023]\n",
      " [0.94389542]\n",
      " [0.94389467]\n",
      " [0.06356513]] \n",
      "\n",
      "420\n",
      "Outputs After the Training: \n",
      " [[0.99976186]\n",
      " [0.94401379]\n",
      " [0.94401305]\n",
      " [0.06342692]] \n",
      "\n",
      "421\n",
      "Outputs After the Training: \n",
      " [[0.99976347]\n",
      " [0.94413166]\n",
      " [0.94413093]\n",
      " [0.0632893 ]] \n",
      "\n",
      "422\n",
      "Outputs After the Training: \n",
      " [[0.99976506]\n",
      " [0.94424905]\n",
      " [0.94424833]\n",
      " [0.06315226]] \n",
      "\n",
      "423\n",
      "Outputs After the Training: \n",
      " [[0.99976664]\n",
      " [0.94436596]\n",
      " [0.94436525]\n",
      " [0.0630158 ]] \n",
      "\n",
      "424\n",
      "Outputs After the Training: \n",
      " [[0.99976821]\n",
      " [0.94448239]\n",
      " [0.94448169]\n",
      " [0.06287993]] \n",
      "\n",
      "425\n",
      "Outputs After the Training: \n",
      " [[0.99976976]\n",
      " [0.94459835]\n",
      " [0.94459766]\n",
      " [0.06274462]] \n",
      "\n",
      "426\n",
      "Outputs After the Training: \n",
      " [[0.9997713 ]\n",
      " [0.94471383]\n",
      " [0.94471315]\n",
      " [0.06260988]] \n",
      "\n",
      "427\n",
      "Outputs After the Training: \n",
      " [[0.99977283]\n",
      " [0.94482884]\n",
      " [0.94482817]\n",
      " [0.06247571]] \n",
      "\n",
      "428\n",
      "Outputs After the Training: \n",
      " [[0.99977434]\n",
      " [0.94494339]\n",
      " [0.94494272]\n",
      " [0.06234211]] \n",
      "\n",
      "429\n",
      "Outputs After the Training: \n",
      " [[0.99977584]\n",
      " [0.94505747]\n",
      " [0.94505681]\n",
      " [0.06220906]] \n",
      "\n",
      "430\n",
      "Outputs After the Training: \n",
      " [[0.99977733]\n",
      " [0.94517108]\n",
      " [0.94517043]\n",
      " [0.06207657]] \n",
      "\n",
      "431\n",
      "Outputs After the Training: \n",
      " [[0.9997788 ]\n",
      " [0.94528424]\n",
      " [0.9452836 ]\n",
      " [0.06194464]] \n",
      "\n",
      "432\n",
      "Outputs After the Training: \n",
      " [[0.99978026]\n",
      " [0.94539694]\n",
      " [0.94539631]\n",
      " [0.06181325]] \n",
      "\n",
      "433\n",
      "Outputs After the Training: \n",
      " [[0.99978171]\n",
      " [0.94550919]\n",
      " [0.94550857]\n",
      " [0.06168241]] \n",
      "\n",
      "434\n",
      "Outputs After the Training: \n",
      " [[0.99978314]\n",
      " [0.94562099]\n",
      " [0.94562037]\n",
      " [0.06155211]] \n",
      "\n",
      "435\n",
      "Outputs After the Training: \n",
      " [[0.99978457]\n",
      " [0.94573234]\n",
      " [0.94573173]\n",
      " [0.06142235]] \n",
      "\n",
      "436\n",
      "Outputs After the Training: \n",
      " [[0.99978598]\n",
      " [0.94584324]\n",
      " [0.94584264]\n",
      " [0.06129312]] \n",
      "\n",
      "437\n",
      "Outputs After the Training: \n",
      " [[0.99978738]\n",
      " [0.9459537 ]\n",
      " [0.94595311]\n",
      " [0.06116443]] \n",
      "\n",
      "438\n",
      "Outputs After the Training: \n",
      " [[0.99978876]\n",
      " [0.94606372]\n",
      " [0.94606313]\n",
      " [0.06103627]] \n",
      "\n",
      "439\n",
      "Outputs After the Training: \n",
      " [[0.99979014]\n",
      " [0.9461733 ]\n",
      " [0.94617272]\n",
      " [0.06090863]] \n",
      "\n",
      "440\n",
      "Outputs After the Training: \n",
      " [[0.9997915 ]\n",
      " [0.94628245]\n",
      " [0.94628188]\n",
      " [0.06078152]] \n",
      "\n",
      "441\n",
      "Outputs After the Training: \n",
      " [[0.99979285]\n",
      " [0.94639116]\n",
      " [0.9463906 ]\n",
      " [0.06065492]] \n",
      "\n",
      "442\n",
      "Outputs After the Training: \n",
      " [[0.99979419]\n",
      " [0.94649945]\n",
      " [0.94649889]\n",
      " [0.06052885]] \n",
      "\n",
      "443\n",
      "Outputs After the Training: \n",
      " [[0.99979552]\n",
      " [0.94660731]\n",
      " [0.94660675]\n",
      " [0.06040328]] \n",
      "\n",
      "444\n",
      "Outputs After the Training: \n",
      " [[0.99979683]\n",
      " [0.94671474]\n",
      " [0.94671419]\n",
      " [0.06027823]] \n",
      "\n",
      "445\n",
      "Outputs After the Training: \n",
      " [[0.99979814]\n",
      " [0.94682175]\n",
      " [0.94682121]\n",
      " [0.06015368]] \n",
      "\n",
      "446\n",
      "Outputs After the Training: \n",
      " [[0.99979943]\n",
      " [0.94692833]\n",
      " [0.9469278 ]\n",
      " [0.06002964]] \n",
      "\n",
      "447\n",
      "Outputs After the Training: \n",
      " [[0.99980072]\n",
      " [0.9470345 ]\n",
      " [0.94703398]\n",
      " [0.0599061 ]] \n",
      "\n",
      "448\n",
      "Outputs After the Training: \n",
      " [[0.99980199]\n",
      " [0.94714026]\n",
      " [0.94713974]\n",
      " [0.05978306]] \n",
      "\n",
      "449\n",
      "Outputs After the Training: \n",
      " [[0.99980325]\n",
      " [0.9472456 ]\n",
      " [0.94724509]\n",
      " [0.05966051]] \n",
      "\n",
      "450\n",
      "Outputs After the Training: \n",
      " [[0.9998045 ]\n",
      " [0.94735053]\n",
      " [0.94735003]\n",
      " [0.05953846]] \n",
      "\n",
      "451\n",
      "Outputs After the Training: \n",
      " [[0.99980574]\n",
      " [0.94745506]\n",
      " [0.94745456]\n",
      " [0.0594169 ]] \n",
      "\n",
      "452\n",
      "Outputs After the Training: \n",
      " [[0.99980697]\n",
      " [0.94755918]\n",
      " [0.94755868]\n",
      " [0.05929582]] \n",
      "\n",
      "453\n",
      "Outputs After the Training: \n",
      " [[0.99980819]\n",
      " [0.94766289]\n",
      " [0.9476624 ]\n",
      " [0.05917522]] \n",
      "\n",
      "454\n",
      "Outputs After the Training: \n",
      " [[0.9998094 ]\n",
      " [0.9477662 ]\n",
      " [0.94776572]\n",
      " [0.05905511]] \n",
      "\n",
      "455\n",
      "Outputs After the Training: \n",
      " [[0.9998106 ]\n",
      " [0.94786912]\n",
      " [0.94786864]\n",
      " [0.05893547]] \n",
      "\n",
      "456\n",
      "Outputs After the Training: \n",
      " [[0.99981179]\n",
      " [0.94797163]\n",
      " [0.94797116]\n",
      " [0.05881631]] \n",
      "\n",
      "457\n",
      "Outputs After the Training: \n",
      " [[0.99981297]\n",
      " [0.94807376]\n",
      " [0.94807329]\n",
      " [0.05869763]] \n",
      "\n",
      "458\n",
      "Outputs After the Training: \n",
      " [[0.99981414]\n",
      " [0.94817549]\n",
      " [0.94817503]\n",
      " [0.05857941]] \n",
      "\n",
      "459\n",
      "Outputs After the Training: \n",
      " [[0.9998153 ]\n",
      " [0.94827683]\n",
      " [0.94827637]\n",
      " [0.05846165]] \n",
      "\n",
      "460\n",
      "Outputs After the Training: \n",
      " [[0.99981646]\n",
      " [0.94837778]\n",
      " [0.94837733]\n",
      " [0.05834437]] \n",
      "\n",
      "461\n",
      "Outputs After the Training: \n",
      " [[0.9998176 ]\n",
      " [0.94847835]\n",
      " [0.94847791]\n",
      " [0.05822754]] \n",
      "\n",
      "462\n",
      "Outputs After the Training: \n",
      " [[0.99981873]\n",
      " [0.94857853]\n",
      " [0.9485781 ]\n",
      " [0.05811117]] \n",
      "\n",
      "463\n",
      "Outputs After the Training: \n",
      " [[0.99981985]\n",
      " [0.94867834]\n",
      " [0.9486779 ]\n",
      " [0.05799526]] \n",
      "\n",
      "464\n",
      "Outputs After the Training: \n",
      " [[0.99982097]\n",
      " [0.94877776]\n",
      " [0.94877733]\n",
      " [0.0578798 ]] \n",
      "\n",
      "465\n",
      "Outputs After the Training: \n",
      " [[0.99982207]\n",
      " [0.94887681]\n",
      " [0.94887638]\n",
      " [0.0577648 ]] \n",
      "\n",
      "466\n",
      "Outputs After the Training: \n",
      " [[0.99982317]\n",
      " [0.94897548]\n",
      " [0.94897506]\n",
      " [0.05765024]] \n",
      "\n",
      "467\n",
      "Outputs After the Training: \n",
      " [[0.99982425]\n",
      " [0.94907378]\n",
      " [0.94907337]\n",
      " [0.05753612]] \n",
      "\n",
      "468\n",
      "Outputs After the Training: \n",
      " [[0.99982533]\n",
      " [0.94917171]\n",
      " [0.9491713 ]\n",
      " [0.05742245]] \n",
      "\n",
      "469\n",
      "Outputs After the Training: \n",
      " [[0.9998264 ]\n",
      " [0.94926927]\n",
      " [0.94926886]\n",
      " [0.05730922]] \n",
      "\n",
      "470\n",
      "Outputs After the Training: \n",
      " [[0.99982746]\n",
      " [0.94936646]\n",
      " [0.94936606]\n",
      " [0.05719643]] \n",
      "\n",
      "471\n",
      "Outputs After the Training: \n",
      " [[0.99982851]\n",
      " [0.94946329]\n",
      " [0.9494629 ]\n",
      " [0.05708407]] \n",
      "\n",
      "472\n",
      "Outputs After the Training: \n",
      " [[0.99982956]\n",
      " [0.94955976]\n",
      " [0.94955937]\n",
      " [0.05697215]] \n",
      "\n",
      "473\n",
      "Outputs After the Training: \n",
      " [[0.99983059]\n",
      " [0.94965586]\n",
      " [0.94965548]\n",
      " [0.05686065]] \n",
      "\n",
      "474\n",
      "Outputs After the Training: \n",
      " [[0.99983162]\n",
      " [0.94975161]\n",
      " [0.94975123]\n",
      " [0.05674959]] \n",
      "\n",
      "475\n",
      "Outputs After the Training: \n",
      " [[0.99983264]\n",
      " [0.949847  ]\n",
      " [0.94984663]\n",
      " [0.05663895]] \n",
      "\n",
      "476\n",
      "Outputs After the Training: \n",
      " [[0.99983365]\n",
      " [0.94994204]\n",
      " [0.94994167]\n",
      " [0.05652873]] \n",
      "\n",
      "477\n",
      "Outputs After the Training: \n",
      " [[0.99983465]\n",
      " [0.95003672]\n",
      " [0.95003635]\n",
      " [0.05641894]] \n",
      "\n",
      "478\n",
      "Outputs After the Training: \n",
      " [[0.99983565]\n",
      " [0.95013105]\n",
      " [0.95013069]\n",
      " [0.05630956]] \n",
      "\n",
      "479\n",
      "Outputs After the Training: \n",
      " [[0.99983663]\n",
      " [0.95022504]\n",
      " [0.95022468]\n",
      " [0.0562006 ]] \n",
      "\n",
      "480\n",
      "Outputs After the Training: \n",
      " [[0.99983761]\n",
      " [0.95031868]\n",
      " [0.95031832]\n",
      " [0.05609205]] \n",
      "\n",
      "481\n",
      "Outputs After the Training: \n",
      " [[0.99983858]\n",
      " [0.95041197]\n",
      " [0.95041162]\n",
      " [0.05598391]] \n",
      "\n",
      "482\n",
      "Outputs After the Training: \n",
      " [[0.99983955]\n",
      " [0.95050492]\n",
      " [0.95050457]\n",
      " [0.05587619]] \n",
      "\n",
      "483\n",
      "Outputs After the Training: \n",
      " [[0.9998405 ]\n",
      " [0.95059753]\n",
      " [0.95059718]\n",
      " [0.05576886]] \n",
      "\n",
      "484\n",
      "Outputs After the Training: \n",
      " [[0.99984145]\n",
      " [0.9506898 ]\n",
      " [0.95068946]\n",
      " [0.05566195]] \n",
      "\n",
      "485\n",
      "Outputs After the Training: \n",
      " [[0.99984239]\n",
      " [0.95078173]\n",
      " [0.95078139]\n",
      " [0.05555543]] \n",
      "\n",
      "486\n",
      "Outputs After the Training: \n",
      " [[0.99984333]\n",
      " [0.95087332]\n",
      " [0.95087299]\n",
      " [0.05544932]] \n",
      "\n",
      "487\n",
      "Outputs After the Training: \n",
      " [[0.99984425]\n",
      " [0.95096458]\n",
      " [0.95096426]\n",
      " [0.0553436 ]] \n",
      "\n",
      "488\n",
      "Outputs After the Training: \n",
      " [[0.99984517]\n",
      " [0.95105551]\n",
      " [0.95105519]\n",
      " [0.05523828]] \n",
      "\n",
      "489\n",
      "Outputs After the Training: \n",
      " [[0.99984608]\n",
      " [0.95114611]\n",
      " [0.95114579]\n",
      " [0.05513336]] \n",
      "\n",
      "490\n",
      "Outputs After the Training: \n",
      " [[0.99984699]\n",
      " [0.95123638]\n",
      " [0.95123607]\n",
      " [0.05502882]] \n",
      "\n",
      "491\n",
      "Outputs After the Training: \n",
      " [[0.99984789]\n",
      " [0.95132633]\n",
      " [0.95132601]\n",
      " [0.05492467]] \n",
      "\n",
      "492\n",
      "Outputs After the Training: \n",
      " [[0.99984878]\n",
      " [0.95141595]\n",
      " [0.95141564]\n",
      " [0.05482091]] \n",
      "\n",
      "493\n",
      "Outputs After the Training: \n",
      " [[0.99984966]\n",
      " [0.95150524]\n",
      " [0.95150494]\n",
      " [0.05471754]] \n",
      "\n",
      "494\n",
      "Outputs After the Training: \n",
      " [[0.99985054]\n",
      " [0.95159422]\n",
      " [0.95159391]\n",
      " [0.05461454]] \n",
      "\n",
      "495\n",
      "Outputs After the Training: \n",
      " [[0.99985141]\n",
      " [0.95168287]\n",
      " [0.95168257]\n",
      " [0.05451193]] \n",
      "\n",
      "496\n",
      "Outputs After the Training: \n",
      " [[0.99985227]\n",
      " [0.95177121]\n",
      " [0.95177091]\n",
      " [0.0544097 ]] \n",
      "\n",
      "497\n",
      "Outputs After the Training: \n",
      " [[0.99985313]\n",
      " [0.95185923]\n",
      " [0.95185893]\n",
      " [0.05430784]] \n",
      "\n",
      "498\n",
      "Outputs After the Training: \n",
      " [[0.99985398]\n",
      " [0.95194693]\n",
      " [0.95194664]\n",
      " [0.05420635]] \n",
      "\n",
      "499\n",
      "Outputs After the Training: \n",
      " [[0.99985482]\n",
      " [0.95203432]\n",
      " [0.95203404]\n",
      " [0.05410524]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "perceptron(inputs, outputs, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xf7sdqVs0s4x"
   },
   "source": [
    "## Implement your own Perceptron Class and use it to classify a binary dataset: \n",
    "- [The Pima Indians Diabetes dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv) \n",
    "\n",
    "You may need to search for other's implementations in order to get inspiration for your own. There are *lots* of perceptron implementations on the internet with varying levels of sophistication and complexity. Whatever your approach, make sure you understand **every** line of your implementation and what its purpose is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "diabetes = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv')\n",
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although neural networks can handle non-normalized data, scaling or normalizing your data will improve your neural network's learning speed. Try to apply the sklearn `MinMaxScaler` or `Normalizer` to your diabetes dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "\n",
    "feats = list(diabetes)[:-1]\n",
    "\n",
    "X = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-W0tiX1F1hh2"
   },
   "outputs": [],
   "source": [
    "##### Update this Class #####\n",
    "\n",
    "class Perceptron():\n",
    "    \n",
    "    def __init__(self, input_dim=2, output_dim=1, rate=0.01, n_iter=10):\n",
    "        '''\n",
    "        Class Parameters\n",
    "        -----------------------------------------\n",
    "        Input and Output dimensions (input_dim & output_dim): Int\n",
    "        Learning rate (rate): float\n",
    "        Number of Iterations(n_iter):Int\n",
    "        Weight (weight): random floats using Numpy\n",
    "        Bias (bias): a column of 1s to offset instances where all values in a row are 0\n",
    "        Loss (loss): an empty list to store the nudges as the network goes through gradient descent process.\n",
    "        '''\n",
    "        self.rate = rate\n",
    "        self.n_iter = n_iter\n",
    "        self.weight = np.random.randn(input_dim, output_dim)\n",
    "        self.bias = np.ones(output_dim)\n",
    "        self.loss = [ ]\n",
    "        pass\n",
    "    \n",
    "    def __sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def __sigmoid_derivative(self, x):\n",
    "        sx = self.__sigmoid(x)\n",
    "        return sx * (1-sx)\n",
    "    \n",
    "    def affine_transform_forward(self, x, weight, bias):\n",
    "        '''\n",
    "        Will transform the input matrix by multiplying it by the weight and adding bias.\n",
    "        Values will be stored in a cache to be passed to other functions within the class\n",
    "        (i.e. affine_transform_backward).\n",
    "        '''\n",
    "        scores = x.dot(weight) + bias\n",
    "        cache = (x, weight, bias)\n",
    "        return scores, cache\n",
    "    \n",
    "    def affine_transform_backward(self, dout, cache):\n",
    "        '''\n",
    "        Will take derivatives for expected outputs back into the hidden layer of the network and assign\n",
    "        'blame' for error. This will update the weights as the network runs iteratively to steer towards\n",
    "        minimal error. Note, in a classification of NAND gates, 1s should have outputs approaching 99% (.99)\n",
    "        and 0s should have outputs approaching 0% (0.00123)\n",
    "        '''\n",
    "        x, weight, bias = cache\n",
    "        dx = dout.dot(weight.T)\n",
    "        dweight = x.reshape(-1,1).dot(dout.reshape(-1,1))\n",
    "        dbias = np.sum(dout, axis=0)\n",
    "        return dx, dweight, dbias\n",
    "    \n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        '''\n",
    "        For every iteration designated as n_iter, the fit process will proceed as follows:\n",
    "        1. Forward Propagation, 2. Scoring, 3. Backward Propagation and 4. Gradient Descent\n",
    "        as weights and bias are iteratively 'nudged' with their derivatives and move towards\n",
    "        minimum error as per Gradient Descent.\n",
    "        '''\n",
    "\n",
    "        for i in range(self.n_iter):\n",
    "        \n",
    "            for j in range(x.shape[0]):\n",
    "            \n",
    "                ''' Forward Propagation '''\n",
    "                scores, cache = self.affine_transform_forward(x[j], self.weight, self.bias)\n",
    "            \n",
    "                ''' Scoring '''\n",
    "                out = self.__sigmoid(scores)\n",
    "                loss = y[j] - out.reshape(-1,)\n",
    "                self.loss.append(loss)\n",
    "            \n",
    "                ''' Backward Propagation '''\n",
    "                dout = loss * self.__sigmoid_derivative(out)\n",
    "                _, dweight, dbias = self.affine_transform_backward(dout.reshape(-1,), cache)\n",
    "            \n",
    "                ''' Gradient Descent, iteratively updating each pass '''\n",
    "                self.weight += dweight\n",
    "                self.bias += dbias\n",
    "            \n",
    "        pass\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "        '''Return class label after unit step'''\n",
    "        return self.__sigmoid(self.affine_transform_forward(x, self.weight, self.bias)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(diabetes.shape)\n",
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((768, 8), (768, 1))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = diabetes.iloc[:, 0:8].values\n",
    "y = diabetes.iloc[:, 8].values.reshape(-1,1)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((615, 8), (153, 8), (615, 1), (153, 1))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train, X_test), (y_train, y_test) = (X[:615, :], X[615:, :]), (y[:615, :], y[615:, :])\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = Perceptron(input_dim=8, output_dim=1, n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(diabetes.loss);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = diabetes.predict(X_test)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6470588235294118"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, np.round(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6QR4oAW1xdyu"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Research \"backpropagation\" to learn how weights get updated in neural networks (tomorrow's lecture). \n",
    "- Implement a multi-layer perceptron. (for non-linearly separable classes)\n",
    "- Try and implement your own backpropagation algorithm.\n",
    "- What are the pros and cons of the different activation functions? How should you decide between them for the different layers of a neural network?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_431_Intro_to_NN_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

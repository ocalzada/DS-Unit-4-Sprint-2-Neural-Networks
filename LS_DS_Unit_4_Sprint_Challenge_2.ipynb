{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2*\n",
    "\n",
    "# Sprint Challenge - Neural Network Foundations\n",
    "\n",
    "Table of Problems\n",
    "\n",
    "1. [Defining Neural Networks](#Q1)\n",
    "2. [Chocolate Gummy Bears](#Q2)\n",
    "    - Perceptron\n",
    "    - Multilayer Perceptron\n",
    "4. [Keras MMP](#Q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Q1\"></a>\n",
    "## 1. Define the following terms:\n",
    "\n",
    "- **Neuron:** A neuron takes inputs, multiplies them by their weights, sums them all up, then applies the activation function to this the weighted sum to determine whether the passed information should move onto the next stage. The neuron with greatest/most \"signal\" in a layer, gets to move forward.\n",
    "\n",
    "- **Input Layer:** The first layer of a neural network. Takes in raw data (from the outside world or the output of other neurons) and feeds it into the rest of the network. No computations are performed in the input nodes.\n",
    "\n",
    "- **Hidden Layer:** The layer(s) inbetween the input and output nodes, which does not see anything outside of it. Computationally intensive.\n",
    "\n",
    "- **Output Layer:** The last layer of a neural network. This is where the results from a classification/regression emerge. This is the layer where the outputs/predictions are returned to.\n",
    "\n",
    "- **Activation:** Is the process by which a neuron decides whether it should fire a signal (or not). Activation occurs by transforming a weighted sum + bias of inputs using activation functions (such as sigmoid, relu, etc) in order to introduce non-linearity into the output of a neuron.\n",
    "\n",
    "- **Backpropagation:** (of errors). The process of calculating \"gradients\", which are vector representations of the derivative for the expected activated output. AFTER backpropagation, gradient descent occurs, where these calculated gradients are used to reduce J(theta) == error, in other words, these gradients are taken back into the hidden layers of the network and assign 'blame' for error. This will update the weights as the network runs iteratively to steer towards minimal error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chocolate Gummy Bears <a id=\"Q2\"></a>\n",
    "\n",
    "Right now, you're probably thinking, \"yuck, who the hell would eat that?\". Great question. Your candy company wants to know too. And you thought I was kidding about the [Chocolate Gummy Bears](https://nuts.com/chocolatessweets/gummies/gummy-bears/milk-gummy-bears.html?utm_source=google&utm_medium=cpc&adpos=1o1&gclid=Cj0KCQjwrfvsBRD7ARIsAKuDvMOZrysDku3jGuWaDqf9TrV3x5JLXt1eqnVhN0KM6fMcbA1nod3h8AwaAvWwEALw_wcB). \n",
    "\n",
    "Let's assume that a candy company has gone out and collected information on the types of Halloween candy kids ate. Our candy company wants to predict the eating behavior of witches, warlocks, and ghosts -- aka costumed kids. They shared a sample dataset with us. Each row represents a piece of candy that a costumed child was presented with during \"trick\" or \"treat\". We know if the candy was `chocolate` (or not chocolate) or `gummy` (or not gummy). Your goal is to predict if the costumed kid `ate` the piece of candy. \n",
    "\n",
    "If both chocolate and gummy equal one, you've got a chocolate gummy bear on your hands!?!?!\n",
    "![Chocolate Gummy Bear](https://ed910ae2d60f0d25bcb8-80550f96b5feb12604f4f720bfefb46d.ssl.cf1.rackcdn.com/3fb630c04435b7b5-2leZuM7_-zoom.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "candy = pd.read_csv('chocolate_gummy_bears.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chocolate</th>\n",
       "      <th>gummy</th>\n",
       "      <th>ate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chocolate  gummy  ate\n",
       "0          0      1    1\n",
       "1          1      0    1\n",
       "2          0      1    1\n",
       "3          0      0    0\n",
       "4          1      1    0"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(candy.shape)\n",
    "candy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron\n",
    "\n",
    "To make predictions on the `candy` dataframe. Build and train a Perceptron using numpy. Your target column is `ate` and your features: `chocolate` and `gummy`. Do not do any feature engineering. :P\n",
    "\n",
    "Once you've trained your model, report your accuracy. You will not be able to achieve more than ~50% with the simple perceptron. Explain why you could not achieve a higher accuracy with the *simple perceptron* architecture, because it's possible to achieve ~95% accuracy on this dataset. Provide your answer in markdown (and *optional* data anlysis code) after your perceptron implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 2), (10000, 1))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start your candy perceptron here\n",
    "\n",
    "X = candy[['chocolate', 'gummy']].values\n",
    "y = candy['ate'].values.reshape(-1,1)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8000, 2), (2000, 2), (8000, 1), (2000, 1))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train, x_test), (y_train, y_test) = (X[:8000, :], X[8000:, :]), (y[:8000, :], y[8000:, :])\n",
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Your Code Here #####\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    ''' A function that takes 1 parameter, x, and returns the sigmoid calculation of it'''\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    ''' A function that takes 1 parameter, x, and returns the sigmoid derivative of it'''\n",
    "    sx = sigmoid(x)\n",
    "    return sx * (1-sx)\n",
    "\n",
    "def perceptron(inputs, outputs, num_passes):\n",
    "    ''' A function that runs a simple neural network: A Perceptron.\n",
    "        Takes in inputs, outputs to search for and the number of passes to learn from.'''\n",
    "    \n",
    "    ''' Assigning random weights to our inputs'''\n",
    "    weights = 2 * np.random.random((len(inputs.T), 1)) - 1\n",
    "    \n",
    "    for iteration in range(num_passes):\n",
    "        ''' Calculating the dot product of the inputs times the weights '''\n",
    "        weighted_sum = np.dot(inputs, weights)\n",
    "    \n",
    "        ''' Output the activated value for the end of 1 training epoch '''\n",
    "        activated_output = sigmoid(weighted_sum)\n",
    "    \n",
    "        ''' Taking the difference between Output and the True values to calculate error '''\n",
    "        error = outputs - activated_output\n",
    "    \n",
    "        ''' Gradient descent/backprop - magic!'''\n",
    "        adjusted = error * sigmoid_derivative(activated_output)\n",
    "    \n",
    "        ''' Updating the weights after each iteration'''\n",
    "        weights += np.dot(inputs.T, adjusted)\n",
    "    \n",
    "        print(iteration)\n",
    "        print('Weights after training: \\n', weights, '\\n')\n",
    "        print('Outputs After the Training: \\n', activated_output, '\\n')\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Weights after training: \n",
      " [[-242.50573146]\n",
      " [-232.79411957]] \n",
      "\n",
      "Outputs After the Training: \n",
      " [[0.70831385]\n",
      " [0.72779182]\n",
      " [0.70831385]\n",
      " ...\n",
      " [0.5       ]\n",
      " [0.72779182]\n",
      " [0.86653425]] \n",
      "\n",
      "1\n",
      "Weights after training: \n",
      " [[256.24426854]\n",
      " [267.20588043]] \n",
      "\n",
      "Outputs After the Training: \n",
      " [[7.92133626e-102]\n",
      " [4.79842814e-106]\n",
      " [7.92133626e-102]\n",
      " ...\n",
      " [5.00000000e-001]\n",
      " [4.79842814e-106]\n",
      " [3.80099628e-207]] \n",
      "\n",
      "2\n",
      "Weights after training: \n",
      " [[-136.97959795]\n",
      " [-125.62476219]] \n",
      "\n",
      "Outputs After the Training: \n",
      " [[1. ]\n",
      " [1. ]\n",
      " [1. ]\n",
      " ...\n",
      " [0.5]\n",
      " [1. ]\n",
      " [1. ]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "perceptron(x_train, y_train, 3)\n",
    "# it looks like the outputs get stuck at 0.5 for the first iteration; then they shift to essentially 0 and then\n",
    "# in the third iteration they shift to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron():\n",
    "    \n",
    "    def __init__(self, input_dim=2, output_dim=1, rate=0.01, n_iter=10):\n",
    "        '''\n",
    "        Class Parameters\n",
    "        -----------------------------------------\n",
    "        Input and Output dimensions (input_dim & output_dim): Int\n",
    "        Learning rate (rate): float\n",
    "        Number of Iterations(n_iter):Int\n",
    "        Weight (weight): random floats using Numpy\n",
    "        Bias (bias): a column of 1s to offset instances where all values in a row are 0\n",
    "        Loss (loss): an empty list to store the nudges as the network goes through gradient descent process.\n",
    "        '''\n",
    "        self.rate = rate\n",
    "        self.n_iter = n_iter\n",
    "        self.weight = np.random.randn(input_dim, output_dim)\n",
    "        self.bias = np.ones(output_dim)\n",
    "        self.loss = [ ]\n",
    "        pass\n",
    "    \n",
    "    def __sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def __sigmoid_derivative(self, x):\n",
    "        sx = self.__sigmoid(x)\n",
    "        return sx * (1-sx)\n",
    "    \n",
    "    def affine_transform_forward(self, x, weight, bias):\n",
    "        '''\n",
    "        Will transform the input matrix by multiplying it by the weight and adding bias.\n",
    "        Values will be stored in a cache to be passed to other functions within the class\n",
    "        (i.e. affine_transform_backward).\n",
    "        '''\n",
    "        scores = x.dot(weight) + bias\n",
    "        cache = (x, weight, bias)\n",
    "        return scores, cache\n",
    "    \n",
    "    def affine_transform_backward(self, dout, cache):\n",
    "        '''\n",
    "        Will take derivatives for expected outputs back into the hidden layer of the network and assign\n",
    "        'blame' for error. This will update the weights as the network runs iteratively to steer towards\n",
    "        minimal error. Note, in a classification of NAND gates, 1s should have outputs approaching 99% (.99)\n",
    "        and 0s should have outputs approaching 0% (0.00123)\n",
    "        '''\n",
    "        x, weight, bias = cache\n",
    "        dx = dout.dot(weight.T)\n",
    "        dweight = x.reshape(-1,1).dot(dout.reshape(-1,1))\n",
    "        dbias = np.sum(dout, axis=0)\n",
    "        return dx, dweight, dbias\n",
    "    \n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        '''\n",
    "        For every iteration designated as n_iter, the fit process will proceed as follows:\n",
    "        1. Forward Propagation, 2. Scoring, 3. Backward Propagation and 4. Gradient Descent\n",
    "        as weights and bias are iteratively 'nudged' with their derivatives and move towards\n",
    "        minimum error as per Gradient Descent.\n",
    "        '''\n",
    "\n",
    "        for i in range(self.n_iter):\n",
    "        \n",
    "            for j in range(x.shape[0]):\n",
    "            \n",
    "                ''' Forward Propagation '''\n",
    "                scores, cache = self.affine_transform_forward(x[j], self.weight, self.bias)\n",
    "            \n",
    "                ''' Scoring '''\n",
    "                out = self.__sigmoid(scores)\n",
    "                loss = y[j] - out.reshape(-1,)\n",
    "                self.loss.append(loss)\n",
    "            \n",
    "                ''' Backward Propagation '''\n",
    "                dout = loss * self.__sigmoid_derivative(out)\n",
    "                _, dweight, dbias = self.affine_transform_backward(dout.reshape(-1,), cache)\n",
    "            \n",
    "                ''' Gradient Descent, iteratively updating each pass '''\n",
    "                self.weight += dweight\n",
    "                self.bias += dbias\n",
    "            \n",
    "        pass\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "        '''Return class label after unit step'''\n",
    "        return self.__sigmoid(self.affine_transform_forward(x, self.weight, self.bias)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8000, 2), (8000, 1))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "candies = Perceptron(input_dim=2, output_dim=1, n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "candies.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 1)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PREDICTIONS\n",
    "y_pred = candies.predict(x_test)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.506"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, np.round(y_pred))\n",
    "# yep, in our simple perceptron model our accuracy score is approximately 50.6%\n",
    "# We're not be able to achieve more than ~50% with the simple perceptron,\n",
    "# because we need to introduce bias into our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron <a id=\"Q3\"></a>\n",
    "\n",
    "Using the sample candy dataset, implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights. Your Multilayer Perceptron should be implemented in Numpy. \n",
    "Your network must have one hidden layer.\n",
    "\n",
    "Once you've trained your model, report your accuracy. Explain why your MLP's performance is considerably better than your simple perceptron's on the candy dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork: \n",
    "    def __init__(self):\n",
    "        # Set-up Architecture \n",
    "        self.inputs = 2\n",
    "        self.hiddenNodes = 1\n",
    "        self.outputNodes = 1\n",
    "        \n",
    "        #Initial weights\n",
    "        self.weights1 = np.random.randn(self.inputs, self.hiddenNodes) #2x1\n",
    "        self.weights2 = np.random.rand(self.hiddenNodes, self.outputNodes) #1x1\n",
    "    \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1+np.exp(-s))\n",
    "    \n",
    "    def sigmoidPrime(self, s):\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        \"\"\"\n",
    "        Calculate the NN inference using feed forward.\n",
    "        \"\"\"\n",
    "        \n",
    "        #Weighted sum of inputs and hidden layer\n",
    "        self.hidden_sum = np.dot(X, self.weights1)\n",
    "        \n",
    "        #Acivations of weighted sum\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "        \n",
    "        # Weight sum between hidden and output\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
    "        \n",
    "        #Final activation of output\n",
    "        self.activated_output = self.sigmoid(self.output_sum)\n",
    "        \n",
    "        return self.activated_output\n",
    "    \n",
    "    def backward(self, X, y, o):\n",
    "        \"\"\"\n",
    "        Backward propagate through the network\n",
    "        \"\"\"\n",
    "        self.o_error = y - o #error in output\n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(o) # apply derivative of sigmoid to error\n",
    "        \n",
    "        self.z2_error = self.o_delta.dot(self.weights2.T) # z2 error: how much our hidden layer weights were off\n",
    "        self.z2_delta = self.z2_error*self.sigmoidPrime(self.activated_hidden)\n",
    "        \n",
    "        self.weights1 += X.T.dot(self.z2_delta) #Adjust first set (input => hidden) weights\n",
    "        self.weights2 += self.activated_hidden.T.dot(self.o_delta) #adjust second set (hidden => output) weights\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        o = self.feed_forward(X)\n",
    "        self.backward(X, y, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------EPOCH 1---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 0]\n",
      " [1 0]\n",
      " [1 1]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      "Predicted Output: \n",
      " [[0.59246087]\n",
      " [0.57044209]\n",
      " [0.59246087]\n",
      " ...\n",
      " [0.55253142]\n",
      " [0.57044209]\n",
      " [0.59800686]]\n",
      "Loss: \n",
      " 0.2537046796823867\n",
      "+---------EPOCH 2---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 0]\n",
      " [1 0]\n",
      " [1 1]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      "Predicted Output: \n",
      " [[6.49912937e-40]\n",
      " [1.84985571e-41]\n",
      " [6.49912937e-40]\n",
      " ...\n",
      " [4.29938060e-21]\n",
      " [1.84985571e-41]\n",
      " [1.84852211e-41]]\n",
      "Loss: \n",
      " 0.5\n",
      "+---------EPOCH 3---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 0]\n",
      " [1 0]\n",
      " [1 1]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      "Predicted Output: \n",
      " [[6.49912937e-40]\n",
      " [1.84985571e-41]\n",
      " [6.49912937e-40]\n",
      " ...\n",
      " [4.29938060e-21]\n",
      " [1.84985571e-41]\n",
      " [1.84852211e-41]]\n",
      "Loss: \n",
      " 0.5\n",
      "+---------EPOCH 4---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 0]\n",
      " [1 0]\n",
      " [1 1]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      "Predicted Output: \n",
      " [[6.49912937e-40]\n",
      " [1.84985571e-41]\n",
      " [6.49912937e-40]\n",
      " ...\n",
      " [4.29938060e-21]\n",
      " [1.84985571e-41]\n",
      " [1.84852211e-41]]\n",
      "Loss: \n",
      " 0.5\n",
      "+---------EPOCH 5---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 0]\n",
      " [1 0]\n",
      " [1 1]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      "Predicted Output: \n",
      " [[6.49912937e-40]\n",
      " [1.84985571e-41]\n",
      " [6.49912937e-40]\n",
      " ...\n",
      " [4.29938060e-21]\n",
      " [1.84985571e-41]\n",
      " [1.84852211e-41]]\n",
      "Loss: \n",
      " 0.5\n",
      "+---------EPOCH 50---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 0]\n",
      " [1 0]\n",
      " [1 1]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      "Predicted Output: \n",
      " [[6.49912937e-40]\n",
      " [1.84985571e-41]\n",
      " [6.49912937e-40]\n",
      " ...\n",
      " [4.29938060e-21]\n",
      " [1.84985571e-41]\n",
      " [1.84852211e-41]]\n",
      "Loss: \n",
      " 0.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "nn = NeuralNetwork()\n",
    "\n",
    "for i in range(50):\n",
    "    if (i+1 in [1,2,3,4,5]) or ((i+1) % 50 ==0):\n",
    "        print('+' + '---' * 3 + f'EPOCH {i+1}' + '---'*3 + '+')\n",
    "        print('Input: \\n', x_train)\n",
    "        print('Actual Output: \\n', y_train)\n",
    "        print('Predicted Output: \\n', str(nn.feed_forward(x_train)))\n",
    "        print(\"Loss: \\n\", str(np.mean(np.square(y_train - nn.feed_forward(x_train)))))\n",
    "    nn.train(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P.S. Don't try candy gummy bears. They're disgusting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Keras MMP <a id=\"Q3\"></a>\n",
    "\n",
    "Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy.\n",
    "Use the Heart Disease Dataset (binary classification)\n",
    "Use an appropriate loss function for a binary classification task\n",
    "Use an appropriate activation function on the final layer of your network.\n",
    "Train your model using verbose output for ease of grading.\n",
    "Use GridSearchCV or RandomSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
    "When hyperparameter tuning, show you work by adding code cells for each new experiment.\n",
    "Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
    "You must hyperparameter tune at least 3 parameters in order to get a 3 on this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>128</td>\n",
       "      <td>259</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>140</td>\n",
       "      <td>417</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>157</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>140</td>\n",
       "      <td>211</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>165</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>106</td>\n",
       "      <td>223</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>142</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>177</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "256   58    1   0       128   259    0        0      130      1      3.0   \n",
       "28    65    0   2       140   417    1        0      157      0      0.8   \n",
       "64    58    1   2       140   211    1        0      165      0      0.0   \n",
       "143   67    0   0       106   223    0        1      142      0      0.3   \n",
       "178   43    1   0       120   177    0        0      120      1      2.5   \n",
       "\n",
       "     slope  ca  thal  target  \n",
       "256      1   2     3       0  \n",
       "28       2   1     2       1  \n",
       "64       2   0     2       1  \n",
       "143      2   2     2       1  \n",
       "178      1   0     3       0  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')\n",
    "df = df.sample(frac=1)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into X_train and y_train\n",
    "X_train = df.loc[:,df.columns != 'target'].values\n",
    "y_train = df.loc[:, df.columns == 'target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.40075247,  0.68100522, -0.93851463, ..., -0.64911323,\n",
       "         1.24459328,  1.12302895],\n",
       "       [ 1.17277425, -1.46841752,  1.00257707, ...,  0.97635214,\n",
       "         0.26508221, -0.51292188],\n",
       "       [ 0.40075247,  0.68100522,  1.00257707, ...,  0.97635214,\n",
       "        -0.71442887, -0.51292188],\n",
       "       ...,\n",
       "       [ 1.39335191,  0.68100522, -0.93851463, ..., -0.64911323,\n",
       "        -0.71442887, -0.51292188],\n",
       "       [ 2.38595135, -1.46841752,  1.00257707, ..., -0.64911323,\n",
       "        -0.71442887, -0.51292188],\n",
       "       [-1.25357993,  0.68100522, -0.93851463, ...,  0.97635214,\n",
       "        -0.71442887, -0.51292188]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# taking a look at our training data\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((303, 13), (303, 1))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_36 (Dense)             (None, 13)                182       \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 13)                182       \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 13)                182       \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 1)                 14        \n",
      "=================================================================\n",
      "Total params: 560\n",
      "Trainable params: 560\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 272 samples, validate on 31 samples\n",
      "Epoch 1/5\n",
      "272/272 [==============================] - 1s 2ms/sample - loss: 0.7071 - accuracy: 0.5368 - val_loss: 0.6855 - val_accuracy: 0.5484\n",
      "Epoch 2/5\n",
      "272/272 [==============================] - 0s 109us/sample - loss: 0.6869 - accuracy: 0.5772 - val_loss: 0.6758 - val_accuracy: 0.6129\n",
      "Epoch 3/5\n",
      "272/272 [==============================] - 0s 109us/sample - loss: 0.6695 - accuracy: 0.6176 - val_loss: 0.6661 - val_accuracy: 0.6774\n",
      "Epoch 4/5\n",
      "272/272 [==============================] - 0s 125us/sample - loss: 0.6519 - accuracy: 0.6838 - val_loss: 0.6554 - val_accuracy: 0.7097\n",
      "Epoch 5/5\n",
      "272/272 [==============================] - 0s 117us/sample - loss: 0.6339 - accuracy: 0.7353 - val_loss: 0.6436 - val_accuracy: 0.7419\n"
     ]
    }
   ],
   "source": [
    "#Baseline Model\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from keras import backend\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Instantiate our model\n",
    "model = Sequential()\n",
    "# Add a Dense layer where all the nodes are connected between layers\n",
    "# ReLU is best initial activation functions\n",
    "model.add(Dense(13, input_dim=13, activation='relu'))\n",
    "model.add(Dense(13, activation='relu'))\n",
    "model.add(Dense(13, activation='relu'))\n",
    "# sigmoid is best activation function for output layer in binary classification problems\n",
    "model.add(Dense(1, activation='sigmoid')) \n",
    "# Add a Compile method; which is how we want to train/teach the dataset\n",
    "# binary_crossentropy is best loss function for binary classification\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# print model summary\n",
    "print(model.summary())\n",
    "# Fit the model using training data; use 10% of data for cross validation\n",
    "history = model.fit(X_train, y_train, epochs=5, validation_split=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initially when I ran this cell I obtained a 90% validation accuracy, which was very good, but\n",
    "# I just got lucky because after rerunnning the cell a couple times that validation accuracy decreased to 74%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.style as style\n",
    "style.use('fivethirtyeight')\n",
    "sns.lineplot(x=history.history['accuracy'], y=history.history['loss']);\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('Loss')\n",
    "plt.show();\n",
    "# as loss/error decreases or is minimized, the accuracy of our model increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 202 samples\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 0s 2ms/sample - loss: 0.9002 - accuracy: 0.3366\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 177us/sample - loss: 0.8293 - accuracy: 0.3812\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 166us/sample - loss: 0.7740 - accuracy: 0.4406\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 170us/sample - loss: 0.7258 - accuracy: 0.5198\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 174us/sample - loss: 0.6858 - accuracy: 0.5990\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 170us/sample - loss: 0.6495 - accuracy: 0.6287\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 173us/sample - loss: 0.6172 - accuracy: 0.6634\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 184us/sample - loss: 0.5887 - accuracy: 0.7178\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 149us/sample - loss: 0.5615 - accuracy: 0.7376\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 156us/sample - loss: 0.5364 - accuracy: 0.7723\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 157us/sample - loss: 0.5138 - accuracy: 0.7871\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 154us/sample - loss: 0.4929 - accuracy: 0.7970\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 167us/sample - loss: 0.4743 - accuracy: 0.8069\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 155us/sample - loss: 0.4575 - accuracy: 0.8119\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 172us/sample - loss: 0.4411 - accuracy: 0.8218\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 144us/sample - loss: 0.4267 - accuracy: 0.8317\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 166us/sample - loss: 0.4137 - accuracy: 0.8317\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 165us/sample - loss: 0.4031 - accuracy: 0.8416\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 158us/sample - loss: 0.3932 - accuracy: 0.8416\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 164us/sample - loss: 0.3835 - accuracy: 0.8416\n",
      "101/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 880us/sample - loss: 0.3212 - accuracy: 0.7921\n",
      "Train on 202 samples\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 0s 2ms/sample - loss: 0.7991 - accuracy: 0.5396\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 196us/sample - loss: 0.7112 - accuracy: 0.6485\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 192us/sample - loss: 0.6409 - accuracy: 0.6980\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 180us/sample - loss: 0.5901 - accuracy: 0.7525\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 165us/sample - loss: 0.5501 - accuracy: 0.7426\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 161us/sample - loss: 0.5182 - accuracy: 0.7574\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 185us/sample - loss: 0.4929 - accuracy: 0.7772\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 187us/sample - loss: 0.4714 - accuracy: 0.7921\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 174us/sample - loss: 0.4542 - accuracy: 0.7921\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 184us/sample - loss: 0.4392 - accuracy: 0.7970\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 175us/sample - loss: 0.4273 - accuracy: 0.7970\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 173us/sample - loss: 0.4157 - accuracy: 0.8168\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 186us/sample - loss: 0.4071 - accuracy: 0.8168\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 171us/sample - loss: 0.3990 - accuracy: 0.8218\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 157us/sample - loss: 0.3912 - accuracy: 0.8317\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 181us/sample - loss: 0.3840 - accuracy: 0.8416\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 174us/sample - loss: 0.3780 - accuracy: 0.8416\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 163us/sample - loss: 0.3731 - accuracy: 0.8416\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 165us/sample - loss: 0.3681 - accuracy: 0.8515\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 165us/sample - loss: 0.3640 - accuracy: 0.8515\n",
      "101/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 956us/sample - loss: 0.3346 - accuracy: 0.8119\n",
      "Train on 202 samples\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 0s 2ms/sample - loss: 0.8242 - accuracy: 0.5050\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 161us/sample - loss: 0.7626 - accuracy: 0.5347\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 171us/sample - loss: 0.7117 - accuracy: 0.5644\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 163us/sample - loss: 0.6729 - accuracy: 0.5792\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 167us/sample - loss: 0.6373 - accuracy: 0.6436\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 162us/sample - loss: 0.6076 - accuracy: 0.6980\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 167us/sample - loss: 0.5816 - accuracy: 0.7079\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 173us/sample - loss: 0.5583 - accuracy: 0.7327\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 171us/sample - loss: 0.5381 - accuracy: 0.7624\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 174us/sample - loss: 0.5195 - accuracy: 0.7772\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 164us/sample - loss: 0.5042 - accuracy: 0.7871\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 179us/sample - loss: 0.4895 - accuracy: 0.7871\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 161us/sample - loss: 0.4775 - accuracy: 0.8020\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 174us/sample - loss: 0.4667 - accuracy: 0.8119\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 191us/sample - loss: 0.4564 - accuracy: 0.8020\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 163us/sample - loss: 0.4471 - accuracy: 0.7970\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 159us/sample - loss: 0.4386 - accuracy: 0.7970\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 170us/sample - loss: 0.4314 - accuracy: 0.8020\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 170us/sample - loss: 0.4253 - accuracy: 0.8020\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 164us/sample - loss: 0.4183 - accuracy: 0.8119\n",
      "101/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 923us/sample - loss: 0.3433 - accuracy: 0.8020\n",
      "Train on 202 samples\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 0s 2ms/sample - loss: 0.8305 - accuracy: 0.4059\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 109us/sample - loss: 0.7866 - accuracy: 0.4752\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 120us/sample - loss: 0.7495 - accuracy: 0.5248\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 93us/sample - loss: 0.7169 - accuracy: 0.5842\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 94us/sample - loss: 0.6849 - accuracy: 0.6238\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 112us/sample - loss: 0.6555 - accuracy: 0.6436\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 108us/sample - loss: 0.6285 - accuracy: 0.6782\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 100us/sample - loss: 0.6039 - accuracy: 0.6931\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 89us/sample - loss: 0.5828 - accuracy: 0.7079\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 85us/sample - loss: 0.5639 - accuracy: 0.7129\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 101us/sample - loss: 0.5459 - accuracy: 0.7327\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 95us/sample - loss: 0.5285 - accuracy: 0.7426\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 104us/sample - loss: 0.5131 - accuracy: 0.7376\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 105us/sample - loss: 0.4983 - accuracy: 0.7673\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 99us/sample - loss: 0.4849 - accuracy: 0.7772\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 100us/sample - loss: 0.4722 - accuracy: 0.7871\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 107us/sample - loss: 0.4592 - accuracy: 0.7970\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 90us/sample - loss: 0.4485 - accuracy: 0.8020\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 91us/sample - loss: 0.4376 - accuracy: 0.8020\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 91us/sample - loss: 0.4284 - accuracy: 0.8069\n",
      "101/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 851us/sample - loss: 0.3101 - accuracy: 0.7624\n",
      "Train on 202 samples\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 0s 2ms/sample - loss: 0.8078 - accuracy: 0.4802\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 88us/sample - loss: 0.7560 - accuracy: 0.5347\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 92us/sample - loss: 0.7130 - accuracy: 0.5792\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 94us/sample - loss: 0.6759 - accuracy: 0.6188\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 94us/sample - loss: 0.6400 - accuracy: 0.6733\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 96us/sample - loss: 0.6129 - accuracy: 0.6980\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 104us/sample - loss: 0.5870 - accuracy: 0.7079\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 103us/sample - loss: 0.5640 - accuracy: 0.7228\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 99us/sample - loss: 0.5453 - accuracy: 0.7277\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 116us/sample - loss: 0.5278 - accuracy: 0.7327\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 92us/sample - loss: 0.5136 - accuracy: 0.7723\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 94us/sample - loss: 0.5008 - accuracy: 0.7871\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 111us/sample - loss: 0.4888 - accuracy: 0.7921\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 91us/sample - loss: 0.4780 - accuracy: 0.7871\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 92us/sample - loss: 0.4693 - accuracy: 0.7921\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 88us/sample - loss: 0.4612 - accuracy: 0.7970\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 94us/sample - loss: 0.4518 - accuracy: 0.8020\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 107us/sample - loss: 0.4438 - accuracy: 0.8168\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 107us/sample - loss: 0.4363 - accuracy: 0.8267\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 107us/sample - loss: 0.4291 - accuracy: 0.8267\n",
      "101/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 969us/sample - loss: 0.7311 - accuracy: 0.8119\n",
      "Train on 202 samples\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 0s 2ms/sample - loss: 0.6227 - accuracy: 0.6337\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 124us/sample - loss: 0.6000 - accuracy: 0.6535\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 112us/sample - loss: 0.5820 - accuracy: 0.6832\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 121us/sample - loss: 0.5670 - accuracy: 0.6931\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 108us/sample - loss: 0.5530 - accuracy: 0.7129\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 102us/sample - loss: 0.5405 - accuracy: 0.7129\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 112us/sample - loss: 0.5283 - accuracy: 0.7525\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 101us/sample - loss: 0.5164 - accuracy: 0.7525\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 109us/sample - loss: 0.5062 - accuracy: 0.7525\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 106us/sample - loss: 0.4970 - accuracy: 0.7624\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 96us/sample - loss: 0.4886 - accuracy: 0.7723\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 96us/sample - loss: 0.4807 - accuracy: 0.7723\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 99us/sample - loss: 0.4736 - accuracy: 0.7723\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 105us/sample - loss: 0.4666 - accuracy: 0.7822\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 99us/sample - loss: 0.4600 - accuracy: 0.7871\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 106us/sample - loss: 0.4532 - accuracy: 0.7871\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - ETA: 0s - loss: 0.5258 - accuracy: 0.70 - 0s 93us/sample - loss: 0.4469 - accuracy: 0.7871\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 90us/sample - loss: 0.4408 - accuracy: 0.7871\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 99us/sample - loss: 0.4348 - accuracy: 0.7871\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 89us/sample - loss: 0.4290 - accuracy: 0.7921\n",
      "101/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 859us/sample - loss: 0.3219 - accuracy: 0.8515\n",
      "Train on 202 samples\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 1s 3ms/sample - loss: 0.7944 - accuracy: 0.4257\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 61us/sample - loss: 0.7730 - accuracy: 0.4455\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 58us/sample - loss: 0.7547 - accuracy: 0.4703\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 57us/sample - loss: 0.7386 - accuracy: 0.4851\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 62us/sample - loss: 0.7222 - accuracy: 0.4950\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 61us/sample - loss: 0.7053 - accuracy: 0.5149\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 62us/sample - loss: 0.6904 - accuracy: 0.5248\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 67us/sample - loss: 0.6752 - accuracy: 0.5594\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 59us/sample - loss: 0.6602 - accuracy: 0.5743\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 53us/sample - loss: 0.6475 - accuracy: 0.5990\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 58us/sample - loss: 0.6352 - accuracy: 0.6089\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 57us/sample - loss: 0.6246 - accuracy: 0.6287\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 54us/sample - loss: 0.6140 - accuracy: 0.6485\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 55us/sample - loss: 0.6043 - accuracy: 0.6584\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 58us/sample - loss: 0.5951 - accuracy: 0.6683\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 58us/sample - loss: 0.5852 - accuracy: 0.6980\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 62us/sample - loss: 0.5746 - accuracy: 0.7228\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 66us/sample - loss: 0.5652 - accuracy: 0.7277\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 66us/sample - loss: 0.5569 - accuracy: 0.7327\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 63us/sample - loss: 0.5491 - accuracy: 0.7327\n",
      "101/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 806us/sample - loss: 0.5823 - accuracy: 0.7426\n",
      "Train on 202 samples\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 0s 2ms/sample - loss: 0.5309 - accuracy: 0.7228\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 58us/sample - loss: 0.5188 - accuracy: 0.7327\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 56us/sample - loss: 0.5105 - accuracy: 0.7376\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 60us/sample - loss: 0.5026 - accuracy: 0.7426\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 57us/sample - loss: 0.4947 - accuracy: 0.7475\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 58us/sample - loss: 0.4860 - accuracy: 0.7475\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 57us/sample - loss: 0.4787 - accuracy: 0.7525\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 59us/sample - loss: 0.4723 - accuracy: 0.7624\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 57us/sample - loss: 0.4667 - accuracy: 0.7673\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 58us/sample - loss: 0.4617 - accuracy: 0.7673\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 58us/sample - loss: 0.4562 - accuracy: 0.7772\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 57us/sample - loss: 0.4505 - accuracy: 0.7822\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 59us/sample - loss: 0.4453 - accuracy: 0.7822\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 58us/sample - loss: 0.4407 - accuracy: 0.7970\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 63us/sample - loss: 0.4366 - accuracy: 0.7970\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 59us/sample - loss: 0.4327 - accuracy: 0.8020\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 61us/sample - loss: 0.4292 - accuracy: 0.8020\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 60us/sample - loss: 0.4261 - accuracy: 0.8119\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 57us/sample - loss: 0.4232 - accuracy: 0.8069\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 58us/sample - loss: 0.4208 - accuracy: 0.8069\n",
      "101/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 872us/sample - loss: 0.3543 - accuracy: 0.7822\n",
      "Train on 202 samples\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 1s 3ms/sample - loss: 0.7520 - accuracy: 0.5149\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 57us/sample - loss: 0.7284 - accuracy: 0.5297\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 69us/sample - loss: 0.7071 - accuracy: 0.5446\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 55us/sample - loss: 0.6894 - accuracy: 0.5644\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 57us/sample - loss: 0.6725 - accuracy: 0.5842\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 56us/sample - loss: 0.6564 - accuracy: 0.6238\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 56us/sample - loss: 0.6427 - accuracy: 0.6436\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 55us/sample - loss: 0.6291 - accuracy: 0.6584\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 56us/sample - loss: 0.6173 - accuracy: 0.6782\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 55us/sample - loss: 0.6067 - accuracy: 0.6931\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 56us/sample - loss: 0.5963 - accuracy: 0.7129\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 55us/sample - loss: 0.5860 - accuracy: 0.7079\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 57us/sample - loss: 0.5764 - accuracy: 0.7277\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 57us/sample - loss: 0.5671 - accuracy: 0.7376\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 55us/sample - loss: 0.5580 - accuracy: 0.7525\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 55us/sample - loss: 0.5495 - accuracy: 0.7475\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 60us/sample - loss: 0.5425 - accuracy: 0.7525\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 64us/sample - loss: 0.5363 - accuracy: 0.7525\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 55us/sample - loss: 0.5312 - accuracy: 0.7525\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 63us/sample - loss: 0.5262 - accuracy: 0.7525\n",
      "101/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 915us/sample - loss: 0.5784 - accuracy: 0.7228\n",
      "Train on 202 samples\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 0s 2ms/sample - loss: 0.7233 - accuracy: 0.5545\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 42us/sample - loss: 0.7061 - accuracy: 0.5644\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 40us/sample - loss: 0.6896 - accuracy: 0.5792\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 41us/sample - loss: 0.6741 - accuracy: 0.6089\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 45us/sample - loss: 0.6593 - accuracy: 0.6139\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 44us/sample - loss: 0.6454 - accuracy: 0.6287\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 43us/sample - loss: 0.6323 - accuracy: 0.6436\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 45us/sample - loss: 0.6200 - accuracy: 0.6584\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 42us/sample - loss: 0.6082 - accuracy: 0.6683\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 42us/sample - loss: 0.5975 - accuracy: 0.6782\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 44us/sample - loss: 0.5866 - accuracy: 0.6832\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 44us/sample - loss: 0.5770 - accuracy: 0.6931\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 41us/sample - loss: 0.5678 - accuracy: 0.7079\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 42us/sample - loss: 0.5586 - accuracy: 0.7327\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 43us/sample - loss: 0.5502 - accuracy: 0.7426\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 43us/sample - loss: 0.5422 - accuracy: 0.7426\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 49us/sample - loss: 0.5341 - accuracy: 0.7624\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 56us/sample - loss: 0.5271 - accuracy: 0.7574\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 54us/sample - loss: 0.5200 - accuracy: 0.7624\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 54us/sample - loss: 0.5132 - accuracy: 0.7673\n",
      "101/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 906us/sample - loss: 0.5843 - accuracy: 0.6733\n",
      "Train on 202 samples\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 0s 2ms/sample - loss: 0.6766 - accuracy: 0.6238\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 59us/sample - loss: 0.6623 - accuracy: 0.6535\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 46us/sample - loss: 0.6492 - accuracy: 0.6634\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 43us/sample - loss: 0.6372 - accuracy: 0.6782\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 40us/sample - loss: 0.6254 - accuracy: 0.7030\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 42us/sample - loss: 0.6140 - accuracy: 0.7129\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 38us/sample - loss: 0.6032 - accuracy: 0.7129\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 49us/sample - loss: 0.5931 - accuracy: 0.7178\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 53us/sample - loss: 0.5833 - accuracy: 0.7178\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 44us/sample - loss: 0.5739 - accuracy: 0.7277\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 52us/sample - loss: 0.5656 - accuracy: 0.7376\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 55us/sample - loss: 0.5572 - accuracy: 0.7426\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 53us/sample - loss: 0.5490 - accuracy: 0.7475\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 53us/sample - loss: 0.5416 - accuracy: 0.7475\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 46us/sample - loss: 0.5341 - accuracy: 0.7475\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 41us/sample - loss: 0.5275 - accuracy: 0.7426\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 40us/sample - loss: 0.5208 - accuracy: 0.7574\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 40us/sample - loss: 0.5146 - accuracy: 0.7475\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 39us/sample - loss: 0.5088 - accuracy: 0.7475\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 40us/sample - loss: 0.5029 - accuracy: 0.7525\n",
      "101/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 866us/sample - loss: 0.5289 - accuracy: 0.7129\n",
      "Train on 202 samples\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 0s 2ms/sample - loss: 0.9076 - accuracy: 0.4455\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 45us/sample - loss: 0.8874 - accuracy: 0.4752\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 46us/sample - loss: 0.8689 - accuracy: 0.4851\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 48us/sample - loss: 0.8506 - accuracy: 0.4950\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 47us/sample - loss: 0.8330 - accuracy: 0.4950\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 45us/sample - loss: 0.8165 - accuracy: 0.5149\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 45us/sample - loss: 0.8000 - accuracy: 0.5198\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 45us/sample - loss: 0.7858 - accuracy: 0.5248\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 43us/sample - loss: 0.7708 - accuracy: 0.5446\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 40us/sample - loss: 0.7563 - accuracy: 0.5495\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 50us/sample - loss: 0.7432 - accuracy: 0.5743\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 41us/sample - loss: 0.7302 - accuracy: 0.5792\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 45us/sample - loss: 0.7178 - accuracy: 0.5792\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 39us/sample - loss: 0.7056 - accuracy: 0.5990\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 43us/sample - loss: 0.6945 - accuracy: 0.6139\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 41us/sample - loss: 0.6830 - accuracy: 0.6139\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 40us/sample - loss: 0.6718 - accuracy: 0.6089\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 47us/sample - loss: 0.6616 - accuracy: 0.6238\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 44us/sample - loss: 0.6514 - accuracy: 0.6337\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 40us/sample - loss: 0.6417 - accuracy: 0.6436\n",
      "101/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 837us/sample - loss: 0.6143 - accuracy: 0.7030\n",
      "Train on 202 samples\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 0s 2ms/sample - loss: 0.8182 - accuracy: 0.3762\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 40us/sample - loss: 0.8062 - accuracy: 0.3861\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 48us/sample - loss: 0.7937 - accuracy: 0.3960\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 38us/sample - loss: 0.7821 - accuracy: 0.4109\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 50us/sample - loss: 0.7708 - accuracy: 0.4356\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 50us/sample - loss: 0.7598 - accuracy: 0.4455\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 47us/sample - loss: 0.7493 - accuracy: 0.4703\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 53us/sample - loss: 0.7392 - accuracy: 0.4752\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 58us/sample - loss: 0.7289 - accuracy: 0.4851\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 46us/sample - loss: 0.7194 - accuracy: 0.4950\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 54us/sample - loss: 0.7098 - accuracy: 0.5297\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 51us/sample - loss: 0.7006 - accuracy: 0.5446\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 43us/sample - loss: 0.6922 - accuracy: 0.5644\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 52us/sample - loss: 0.6834 - accuracy: 0.5842\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 51us/sample - loss: 0.6750 - accuracy: 0.5990\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 46us/sample - loss: 0.6670 - accuracy: 0.6139\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 53us/sample - loss: 0.6593 - accuracy: 0.6337\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 44us/sample - loss: 0.6514 - accuracy: 0.6337\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 55us/sample - loss: 0.6441 - accuracy: 0.6584\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 51us/sample - loss: 0.6367 - accuracy: 0.6683\n",
      "101/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 838us/sample - loss: 0.6124 - accuracy: 0.6436\n",
      "Train on 202 samples\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 1s 3ms/sample - loss: 0.6330 - accuracy: 0.6535\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 37us/sample - loss: 0.6254 - accuracy: 0.6683\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 43us/sample - loss: 0.6182 - accuracy: 0.6733\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 34us/sample - loss: 0.6112 - accuracy: 0.6832\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 37us/sample - loss: 0.6042 - accuracy: 0.6881\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 34us/sample - loss: 0.5974 - accuracy: 0.6881\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 34us/sample - loss: 0.5907 - accuracy: 0.6980\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 32us/sample - loss: 0.5842 - accuracy: 0.6881\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 35us/sample - loss: 0.5778 - accuracy: 0.7079\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 40us/sample - loss: 0.5714 - accuracy: 0.7178\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 38us/sample - loss: 0.5651 - accuracy: 0.7228\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 37us/sample - loss: 0.5595 - accuracy: 0.7376\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 34us/sample - loss: 0.5535 - accuracy: 0.7376\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 35us/sample - loss: 0.5477 - accuracy: 0.7376\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 34us/sample - loss: 0.5422 - accuracy: 0.7327\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 34us/sample - loss: 0.5368 - accuracy: 0.7327\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 34us/sample - loss: 0.5313 - accuracy: 0.7426\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 36us/sample - loss: 0.5262 - accuracy: 0.7426\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 36us/sample - loss: 0.5214 - accuracy: 0.7525\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 34us/sample - loss: 0.5162 - accuracy: 0.7624\n",
      "101/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 907us/sample - loss: 0.4871 - accuracy: 0.7525\n",
      "Train on 202 samples\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 0s 2ms/sample - loss: 0.7313 - accuracy: 0.4505\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 50us/sample - loss: 0.7190 - accuracy: 0.4752\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 39us/sample - loss: 0.7063 - accuracy: 0.5000\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 32us/sample - loss: 0.6950 - accuracy: 0.5198\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 34us/sample - loss: 0.6839 - accuracy: 0.5297\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 31us/sample - loss: 0.6732 - accuracy: 0.5693\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 43us/sample - loss: 0.6627 - accuracy: 0.5743\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 36us/sample - loss: 0.6529 - accuracy: 0.5891\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 35us/sample - loss: 0.6431 - accuracy: 0.6089\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 40us/sample - loss: 0.6342 - accuracy: 0.6436\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 34us/sample - loss: 0.6251 - accuracy: 0.6485\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 31us/sample - loss: 0.6166 - accuracy: 0.6634\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 35us/sample - loss: 0.6082 - accuracy: 0.6683\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 35us/sample - loss: 0.6003 - accuracy: 0.6782\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 37us/sample - loss: 0.5922 - accuracy: 0.6881\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 36us/sample - loss: 0.5848 - accuracy: 0.6931\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 40us/sample - loss: 0.5775 - accuracy: 0.6980\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 39us/sample - loss: 0.5705 - accuracy: 0.7030\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 36us/sample - loss: 0.5638 - accuracy: 0.7129\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 36us/sample - loss: 0.5573 - accuracy: 0.7277\n",
      "101/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 889us/sample - loss: 0.5744 - accuracy: 0.7030\n",
      "Train on 202 samples\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 0s 2ms/sample - loss: 0.9118 - accuracy: 0.3812\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 35us/sample - loss: 0.8933 - accuracy: 0.3960\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 36us/sample - loss: 0.8788 - accuracy: 0.4257\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 37us/sample - loss: 0.8653 - accuracy: 0.4257\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 34us/sample - loss: 0.8529 - accuracy: 0.4455\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 35us/sample - loss: 0.8413 - accuracy: 0.4554\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 33us/sample - loss: 0.8300 - accuracy: 0.4554\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 32us/sample - loss: 0.8182 - accuracy: 0.4653\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 33us/sample - loss: 0.8076 - accuracy: 0.4752\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 34us/sample - loss: 0.7975 - accuracy: 0.4802\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 35us/sample - loss: 0.7878 - accuracy: 0.4901\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 33us/sample - loss: 0.7784 - accuracy: 0.5099\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 32us/sample - loss: 0.7692 - accuracy: 0.5198\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 33us/sample - loss: 0.7607 - accuracy: 0.5297\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 33us/sample - loss: 0.7530 - accuracy: 0.5495\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 35us/sample - loss: 0.7457 - accuracy: 0.5545\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 34us/sample - loss: 0.7383 - accuracy: 0.5545\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 34us/sample - loss: 0.7305 - accuracy: 0.5644\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 32us/sample - loss: 0.7233 - accuracy: 0.5743\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 34us/sample - loss: 0.7166 - accuracy: 0.5842\n",
      "101/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 833us/sample - loss: 0.6239 - accuracy: 0.5545\n",
      "Train on 202 samples\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 0s 2ms/sample - loss: 0.9027 - accuracy: 0.4703\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 35us/sample - loss: 0.8870 - accuracy: 0.4752\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 32us/sample - loss: 0.8750 - accuracy: 0.4802\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 32us/sample - loss: 0.8627 - accuracy: 0.5000\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 31us/sample - loss: 0.8512 - accuracy: 0.5099\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 31us/sample - loss: 0.8396 - accuracy: 0.5149\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 36us/sample - loss: 0.8281 - accuracy: 0.5198\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 35us/sample - loss: 0.8169 - accuracy: 0.5347\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 32us/sample - loss: 0.8064 - accuracy: 0.5347\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 34us/sample - loss: 0.7958 - accuracy: 0.5347\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 32us/sample - loss: 0.7861 - accuracy: 0.5396\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 34us/sample - loss: 0.7756 - accuracy: 0.5396\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 34us/sample - loss: 0.7655 - accuracy: 0.5594\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 34us/sample - loss: 0.7553 - accuracy: 0.5743\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 33us/sample - loss: 0.7457 - accuracy: 0.5693\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 32us/sample - loss: 0.7366 - accuracy: 0.5842\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 32us/sample - loss: 0.7273 - accuracy: 0.5891\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 32us/sample - loss: 0.7192 - accuracy: 0.5891\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 35us/sample - loss: 0.7105 - accuracy: 0.6040\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 33us/sample - loss: 0.7017 - accuracy: 0.6089\n",
      "101/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 823us/sample - loss: 0.5356 - accuracy: 0.5347\n",
      "Train on 202 samples\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 0s 2ms/sample - loss: 0.7026 - accuracy: 0.5743\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 46us/sample - loss: 0.6933 - accuracy: 0.5842\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 36us/sample - loss: 0.6863 - accuracy: 0.5792\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 35us/sample - loss: 0.6804 - accuracy: 0.5842\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 38us/sample - loss: 0.6752 - accuracy: 0.5891\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 43us/sample - loss: 0.6697 - accuracy: 0.5891\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 46us/sample - loss: 0.6647 - accuracy: 0.6089\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 46us/sample - loss: 0.6601 - accuracy: 0.6089\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 38us/sample - loss: 0.6555 - accuracy: 0.6089\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 44us/sample - loss: 0.6511 - accuracy: 0.6139\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 44us/sample - loss: 0.6468 - accuracy: 0.6139\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 47us/sample - loss: 0.6427 - accuracy: 0.6238\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 40us/sample - loss: 0.6384 - accuracy: 0.6238\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 42us/sample - loss: 0.6346 - accuracy: 0.6337\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 34us/sample - loss: 0.6307 - accuracy: 0.6386\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 31us/sample - loss: 0.6268 - accuracy: 0.6485\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 30us/sample - loss: 0.6234 - accuracy: 0.6485\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 32us/sample - loss: 0.6201 - accuracy: 0.6634\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 31us/sample - loss: 0.6169 - accuracy: 0.6634\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 30us/sample - loss: 0.6137 - accuracy: 0.6782\n",
      "101/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 1ms/sample - loss: 0.7458 - accuracy: 0.6832\n",
      "Train on 303 samples\n",
      "Epoch 1/20\n",
      "303/303 [==============================] - 0s 1ms/sample - loss: 0.5319 - accuracy: 0.7195\n",
      "Epoch 2/20\n",
      "303/303 [==============================] - 0s 82us/sample - loss: 0.5144 - accuracy: 0.7525\n",
      "Epoch 3/20\n",
      "303/303 [==============================] - 0s 76us/sample - loss: 0.5000 - accuracy: 0.7591\n",
      "Epoch 4/20\n",
      "303/303 [==============================] - 0s 86us/sample - loss: 0.4867 - accuracy: 0.7624\n",
      "Epoch 5/20\n",
      "303/303 [==============================] - 0s 73us/sample - loss: 0.4744 - accuracy: 0.7756\n",
      "Epoch 6/20\n",
      "303/303 [==============================] - 0s 82us/sample - loss: 0.4637 - accuracy: 0.7756\n",
      "Epoch 7/20\n",
      "303/303 [==============================] - 0s 99us/sample - loss: 0.4545 - accuracy: 0.7855\n",
      "Epoch 8/20\n",
      "303/303 [==============================] - 0s 88us/sample - loss: 0.4458 - accuracy: 0.7888\n",
      "Epoch 9/20\n",
      "303/303 [==============================] - 0s 85us/sample - loss: 0.4379 - accuracy: 0.7888\n",
      "Epoch 10/20\n",
      "303/303 [==============================] - 0s 88us/sample - loss: 0.4307 - accuracy: 0.7987\n",
      "Epoch 11/20\n",
      "303/303 [==============================] - 0s 82us/sample - loss: 0.4238 - accuracy: 0.8086\n",
      "Epoch 12/20\n",
      "303/303 [==============================] - 0s 87us/sample - loss: 0.4169 - accuracy: 0.8119\n",
      "Epoch 13/20\n",
      "303/303 [==============================] - 0s 93us/sample - loss: 0.4105 - accuracy: 0.8152\n",
      "Epoch 14/20\n",
      "303/303 [==============================] - 0s 82us/sample - loss: 0.4039 - accuracy: 0.8185\n",
      "Epoch 15/20\n",
      "303/303 [==============================] - 0s 90us/sample - loss: 0.3986 - accuracy: 0.8218\n",
      "Epoch 16/20\n",
      "303/303 [==============================] - 0s 83us/sample - loss: 0.3938 - accuracy: 0.8218\n",
      "Epoch 17/20\n",
      "303/303 [==============================] - 0s 84us/sample - loss: 0.3892 - accuracy: 0.8317\n",
      "Epoch 18/20\n",
      "303/303 [==============================] - 0s 87us/sample - loss: 0.3850 - accuracy: 0.8284\n",
      "Epoch 19/20\n",
      "303/303 [==============================] - 0s 79us/sample - loss: 0.3809 - accuracy: 0.8317\n",
      "Epoch 20/20\n",
      "303/303 [==============================] - 0s 79us/sample - loss: 0.3770 - accuracy: 0.8383\n",
      "Best: 0.8085808555285136 using {'batch_size': 20, 'epochs': 20}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=13, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "\n",
    "# define the grid search parameters\n",
    "# batch_size = [10, 20, 40, 60, 80, 100]\n",
    "# param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "# define the grid search parameters\n",
    "param_grid = {'batch_size': [10, 20, 40, 60, 80, 100],\n",
    "              'epochs': [20]}\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 202 samples\n",
      "Epoch 1/5\n",
      "202/202 [==============================] - 0s 2ms/sample - loss: 0.6608 - accuracy: 0.6238\n",
      "Epoch 2/5\n",
      "202/202 [==============================] - 0s 105us/sample - loss: 0.6330 - accuracy: 0.6535\n",
      "Epoch 3/5\n",
      "202/202 [==============================] - 0s 85us/sample - loss: 0.6118 - accuracy: 0.6782\n",
      "Epoch 4/5\n",
      "202/202 [==============================] - 0s 84us/sample - loss: 0.5934 - accuracy: 0.6832\n",
      "Epoch 5/5\n",
      "202/202 [==============================] - 0s 87us/sample - loss: 0.5738 - accuracy: 0.7030\n",
      "101/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 824us/sample - loss: 0.4884 - accuracy: 0.6634\n",
      "Train on 202 samples\n",
      "Epoch 1/5\n",
      "202/202 [==============================] - 0s 2ms/sample - loss: 1.0527 - accuracy: 0.4703\n",
      "Epoch 2/5\n",
      "202/202 [==============================] - 0s 102us/sample - loss: 0.9699 - accuracy: 0.4703\n",
      "Epoch 3/5\n",
      "202/202 [==============================] - 0s 104us/sample - loss: 0.9048 - accuracy: 0.4901\n",
      "Epoch 4/5\n",
      "202/202 [==============================] - 0s 101us/sample - loss: 0.8445 - accuracy: 0.4950\n",
      "Epoch 5/5\n",
      "202/202 [==============================] - 0s 92us/sample - loss: 0.7944 - accuracy: 0.5099\n",
      "101/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 842us/sample - loss: 0.6167 - accuracy: 0.6139\n",
      "Train on 202 samples\n",
      "Epoch 1/5\n",
      "202/202 [==============================] - 0s 2ms/sample - loss: 0.9610 - accuracy: 0.2871\n",
      "Epoch 2/5\n",
      "202/202 [==============================] - 0s 93us/sample - loss: 0.9149 - accuracy: 0.3020\n",
      "Epoch 3/5\n",
      "202/202 [==============================] - 0s 114us/sample - loss: 0.8751 - accuracy: 0.3119\n",
      "Epoch 4/5\n",
      "202/202 [==============================] - 0s 106us/sample - loss: 0.8371 - accuracy: 0.3663\n",
      "Epoch 5/5\n",
      "202/202 [==============================] - 0s 106us/sample - loss: 0.8029 - accuracy: 0.4109\n",
      "101/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 789us/sample - loss: 0.8134 - accuracy: 0.3267\n",
      "Train on 202 samples\n",
      "Epoch 1/10\n",
      "202/202 [==============================] - 1s 4ms/sample - loss: 0.6167 - accuracy: 0.6584\n",
      "Epoch 2/10\n",
      "202/202 [==============================] - 0s 97us/sample - loss: 0.5861 - accuracy: 0.6980\n",
      "Epoch 3/10\n",
      "202/202 [==============================] - 0s 93us/sample - loss: 0.5624 - accuracy: 0.7178\n",
      "Epoch 4/10\n",
      "202/202 [==============================] - 0s 106us/sample - loss: 0.5403 - accuracy: 0.7327\n",
      "Epoch 5/10\n",
      "202/202 [==============================] - 0s 97us/sample - loss: 0.5192 - accuracy: 0.7624\n",
      "Epoch 6/10\n",
      "202/202 [==============================] - 0s 103us/sample - loss: 0.4994 - accuracy: 0.7970\n",
      "Epoch 7/10\n",
      "202/202 [==============================] - 0s 100us/sample - loss: 0.4833 - accuracy: 0.7921\n",
      "Epoch 8/10\n",
      "202/202 [==============================] - 0s 106us/sample - loss: 0.4677 - accuracy: 0.8020\n",
      "Epoch 9/10\n",
      "202/202 [==============================] - 0s 106us/sample - loss: 0.4542 - accuracy: 0.8020\n",
      "Epoch 10/10\n",
      "202/202 [==============================] - 0s 108us/sample - loss: 0.4419 - accuracy: 0.8020\n",
      "101/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 950us/sample - loss: 0.3550 - accuracy: 0.7228\n",
      "Train on 202 samples\n",
      "Epoch 1/10\n",
      "202/202 [==============================] - 0s 2ms/sample - loss: 0.5919 - accuracy: 0.7277\n",
      "Epoch 2/10\n",
      "202/202 [==============================] - 0s 100us/sample - loss: 0.5701 - accuracy: 0.7525\n",
      "Epoch 3/10\n",
      "202/202 [==============================] - 0s 99us/sample - loss: 0.5526 - accuracy: 0.7475\n",
      "Epoch 4/10\n",
      "202/202 [==============================] - 0s 100us/sample - loss: 0.5366 - accuracy: 0.7376\n",
      "Epoch 5/10\n",
      "202/202 [==============================] - 0s 101us/sample - loss: 0.5212 - accuracy: 0.7673\n",
      "Epoch 6/10\n",
      "202/202 [==============================] - 0s 102us/sample - loss: 0.5094 - accuracy: 0.7822\n",
      "Epoch 7/10\n",
      "202/202 [==============================] - 0s 102us/sample - loss: 0.4980 - accuracy: 0.7871\n",
      "Epoch 8/10\n",
      "202/202 [==============================] - 0s 98us/sample - loss: 0.4869 - accuracy: 0.7921\n",
      "Epoch 9/10\n",
      "202/202 [==============================] - 0s 95us/sample - loss: 0.4776 - accuracy: 0.7970\n",
      "Epoch 10/10\n",
      "202/202 [==============================] - 0s 108us/sample - loss: 0.4688 - accuracy: 0.8020\n",
      "101/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 955us/sample - loss: 0.4085 - accuracy: 0.8020\n",
      "Train on 202 samples\n",
      "Epoch 1/10\n",
      "202/202 [==============================] - 0s 2ms/sample - loss: 0.7851 - accuracy: 0.4505\n",
      "Epoch 2/10\n",
      "202/202 [==============================] - 0s 105us/sample - loss: 0.7470 - accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "202/202 [==============================] - 0s 115us/sample - loss: 0.7148 - accuracy: 0.5495\n",
      "Epoch 4/10\n",
      "202/202 [==============================] - 0s 117us/sample - loss: 0.6879 - accuracy: 0.5792\n",
      "Epoch 5/10\n",
      "202/202 [==============================] - 0s 105us/sample - loss: 0.6624 - accuracy: 0.6089\n",
      "Epoch 6/10\n",
      "202/202 [==============================] - 0s 109us/sample - loss: 0.6382 - accuracy: 0.6535\n",
      "Epoch 7/10\n",
      "202/202 [==============================] - 0s 107us/sample - loss: 0.6174 - accuracy: 0.6584\n",
      "Epoch 8/10\n",
      "202/202 [==============================] - 0s 99us/sample - loss: 0.5961 - accuracy: 0.6782\n",
      "Epoch 9/10\n",
      "202/202 [==============================] - 0s 93us/sample - loss: 0.5771 - accuracy: 0.7079\n",
      "Epoch 10/10\n",
      "202/202 [==============================] - 0s 105us/sample - loss: 0.5606 - accuracy: 0.7327\n",
      "101/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 889us/sample - loss: 0.8402 - accuracy: 0.7723\n",
      "Train on 202 samples\n",
      "Epoch 1/15\n",
      "202/202 [==============================] - 0s 2ms/sample - loss: 0.6490 - accuracy: 0.6139\n",
      "Epoch 2/15\n",
      "202/202 [==============================] - 0s 101us/sample - loss: 0.6049 - accuracy: 0.6832\n",
      "Epoch 3/15\n",
      "202/202 [==============================] - 0s 92us/sample - loss: 0.5678 - accuracy: 0.7178\n",
      "Epoch 4/15\n",
      "202/202 [==============================] - 0s 100us/sample - loss: 0.5372 - accuracy: 0.7475\n",
      "Epoch 5/15\n",
      "202/202 [==============================] - 0s 104us/sample - loss: 0.5101 - accuracy: 0.7624\n",
      "Epoch 6/15\n",
      "202/202 [==============================] - 0s 92us/sample - loss: 0.4878 - accuracy: 0.7772\n",
      "Epoch 7/15\n",
      "202/202 [==============================] - 0s 103us/sample - loss: 0.4674 - accuracy: 0.7772\n",
      "Epoch 8/15\n",
      "202/202 [==============================] - 0s 97us/sample - loss: 0.4503 - accuracy: 0.7822\n",
      "Epoch 9/15\n",
      "202/202 [==============================] - 0s 92us/sample - loss: 0.4338 - accuracy: 0.7921\n",
      "Epoch 10/15\n",
      "202/202 [==============================] - 0s 83us/sample - loss: 0.4188 - accuracy: 0.7970\n",
      "Epoch 11/15\n",
      "202/202 [==============================] - 0s 86us/sample - loss: 0.4066 - accuracy: 0.8119\n",
      "Epoch 12/15\n",
      "202/202 [==============================] - 0s 83us/sample - loss: 0.3941 - accuracy: 0.8267\n",
      "Epoch 13/15\n",
      "202/202 [==============================] - 0s 82us/sample - loss: 0.3847 - accuracy: 0.8317\n",
      "Epoch 14/15\n",
      "202/202 [==============================] - 0s 83us/sample - loss: 0.3744 - accuracy: 0.8366\n",
      "Epoch 15/15\n",
      "202/202 [==============================] - 0s 85us/sample - loss: 0.3661 - accuracy: 0.8416\n",
      "101/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 807us/sample - loss: 0.2941 - accuracy: 0.7822\n",
      "Train on 202 samples\n",
      "Epoch 1/15\n",
      "202/202 [==============================] - 1s 4ms/sample - loss: 0.5942 - accuracy: 0.6980\n",
      "Epoch 2/15\n",
      "202/202 [==============================] - 0s 93us/sample - loss: 0.5749 - accuracy: 0.7178\n",
      "Epoch 3/15\n",
      "202/202 [==============================] - 0s 99us/sample - loss: 0.5577 - accuracy: 0.7426\n",
      "Epoch 4/15\n",
      "202/202 [==============================] - 0s 93us/sample - loss: 0.5421 - accuracy: 0.7673\n",
      "Epoch 5/15\n",
      "202/202 [==============================] - 0s 90us/sample - loss: 0.5277 - accuracy: 0.7772\n",
      "Epoch 6/15\n",
      "202/202 [==============================] - 0s 96us/sample - loss: 0.5125 - accuracy: 0.7871\n",
      "Epoch 7/15\n",
      "202/202 [==============================] - 0s 88us/sample - loss: 0.4997 - accuracy: 0.8020\n",
      "Epoch 8/15\n",
      "202/202 [==============================] - 0s 85us/sample - loss: 0.4876 - accuracy: 0.8069\n",
      "Epoch 9/15\n",
      "202/202 [==============================] - 0s 80us/sample - loss: 0.4745 - accuracy: 0.8069\n",
      "Epoch 10/15\n",
      "202/202 [==============================] - 0s 95us/sample - loss: 0.4630 - accuracy: 0.8267\n",
      "Epoch 11/15\n",
      "202/202 [==============================] - 0s 111us/sample - loss: 0.4541 - accuracy: 0.8416\n",
      "Epoch 12/15\n",
      "202/202 [==============================] - 0s 103us/sample - loss: 0.4444 - accuracy: 0.8515\n",
      "Epoch 13/15\n",
      "202/202 [==============================] - 0s 110us/sample - loss: 0.4360 - accuracy: 0.8515\n",
      "Epoch 14/15\n",
      "202/202 [==============================] - 0s 114us/sample - loss: 0.4275 - accuracy: 0.8564\n",
      "Epoch 15/15\n",
      "202/202 [==============================] - 0s 120us/sample - loss: 0.4200 - accuracy: 0.8663\n",
      "101/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 876us/sample - loss: 0.5429 - accuracy: 0.7624\n",
      "Train on 202 samples\n",
      "Epoch 1/15\n",
      "202/202 [==============================] - 0s 2ms/sample - loss: 0.6788 - accuracy: 0.5693\n",
      "Epoch 2/15\n",
      "202/202 [==============================] - 0s 97us/sample - loss: 0.6404 - accuracy: 0.6089\n",
      "Epoch 3/15\n",
      "202/202 [==============================] - 0s 104us/sample - loss: 0.6094 - accuracy: 0.6337\n",
      "Epoch 4/15\n",
      "202/202 [==============================] - 0s 102us/sample - loss: 0.5813 - accuracy: 0.6683\n",
      "Epoch 5/15\n",
      "202/202 [==============================] - 0s 95us/sample - loss: 0.5567 - accuracy: 0.7178\n",
      "Epoch 6/15\n",
      "202/202 [==============================] - 0s 104us/sample - loss: 0.5330 - accuracy: 0.7475\n",
      "Epoch 7/15\n",
      "202/202 [==============================] - 0s 103us/sample - loss: 0.5132 - accuracy: 0.7673\n",
      "Epoch 8/15\n",
      "202/202 [==============================] - 0s 109us/sample - loss: 0.4967 - accuracy: 0.7871\n",
      "Epoch 9/15\n",
      "202/202 [==============================] - 0s 113us/sample - loss: 0.4822 - accuracy: 0.8069\n",
      "Epoch 10/15\n",
      "202/202 [==============================] - 0s 113us/sample - loss: 0.4691 - accuracy: 0.8069\n",
      "Epoch 11/15\n",
      "202/202 [==============================] - 0s 109us/sample - loss: 0.4568 - accuracy: 0.8218\n",
      "Epoch 12/15\n",
      "202/202 [==============================] - 0s 101us/sample - loss: 0.4469 - accuracy: 0.8218\n",
      "Epoch 13/15\n",
      "202/202 [==============================] - 0s 110us/sample - loss: 0.4378 - accuracy: 0.8218\n",
      "Epoch 14/15\n",
      "202/202 [==============================] - 0s 97us/sample - loss: 0.4292 - accuracy: 0.8218\n",
      "Epoch 15/15\n",
      "202/202 [==============================] - 0s 89us/sample - loss: 0.4218 - accuracy: 0.8168\n",
      "101/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 917us/sample - loss: 0.6154 - accuracy: 0.8614\n",
      "Train on 202 samples\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 0s 2ms/sample - loss: 0.8738 - accuracy: 0.4455\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 113us/sample - loss: 0.8246 - accuracy: 0.4703\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 114us/sample - loss: 0.7841 - accuracy: 0.5198\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 112us/sample - loss: 0.7471 - accuracy: 0.5396\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 121us/sample - loss: 0.7124 - accuracy: 0.5842\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 112us/sample - loss: 0.6814 - accuracy: 0.6188\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 113us/sample - loss: 0.6537 - accuracy: 0.6485\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 120us/sample - loss: 0.6293 - accuracy: 0.6683\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 121us/sample - loss: 0.6064 - accuracy: 0.6782\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 124us/sample - loss: 0.5860 - accuracy: 0.7030\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 120us/sample - loss: 0.5674 - accuracy: 0.7228\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 120us/sample - loss: 0.5492 - accuracy: 0.7475\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - ETA: 0s - loss: 0.4204 - accuracy: 0.90 - 0s 122us/sample - loss: 0.5329 - accuracy: 0.7624\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 126us/sample - loss: 0.5179 - accuracy: 0.7723\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 108us/sample - loss: 0.5042 - accuracy: 0.7822\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 110us/sample - loss: 0.4923 - accuracy: 0.7871\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 126us/sample - loss: 0.4807 - accuracy: 0.8020\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 113us/sample - loss: 0.4700 - accuracy: 0.8020\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 120us/sample - loss: 0.4597 - accuracy: 0.8020\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 121us/sample - loss: 0.4500 - accuracy: 0.8119\n",
      "101/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 884us/sample - loss: 0.3111 - accuracy: 0.7228\n",
      "Train on 202 samples\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 0s 2ms/sample - loss: 0.8847 - accuracy: 0.5396\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 93us/sample - loss: 0.8406 - accuracy: 0.5594\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 92us/sample - loss: 0.8000 - accuracy: 0.5792\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 93us/sample - loss: 0.7668 - accuracy: 0.5990\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 96us/sample - loss: 0.7365 - accuracy: 0.6040\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 97us/sample - loss: 0.7086 - accuracy: 0.6188\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 93us/sample - loss: 0.6821 - accuracy: 0.6238\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 100us/sample - loss: 0.6603 - accuracy: 0.6238\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 92us/sample - loss: 0.6379 - accuracy: 0.6485\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 96us/sample - loss: 0.6205 - accuracy: 0.6683\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 93us/sample - loss: 0.6037 - accuracy: 0.6782\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 110us/sample - loss: 0.5881 - accuracy: 0.6931\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 101us/sample - loss: 0.5727 - accuracy: 0.7129\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 106us/sample - loss: 0.5586 - accuracy: 0.7327\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 97us/sample - loss: 0.5449 - accuracy: 0.7376\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 104us/sample - loss: 0.5320 - accuracy: 0.7277\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 90us/sample - loss: 0.5209 - accuracy: 0.7327\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 105us/sample - loss: 0.5107 - accuracy: 0.7426\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 96us/sample - loss: 0.5019 - accuracy: 0.7475\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 100us/sample - loss: 0.4927 - accuracy: 0.7673\n",
      "101/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 937us/sample - loss: 0.4759 - accuracy: 0.7822\n",
      "Train on 202 samples\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 1s 4ms/sample - loss: 0.6956 - accuracy: 0.5891\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 98us/sample - loss: 0.6663 - accuracy: 0.6337\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 95us/sample - loss: 0.6420 - accuracy: 0.6535\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 93us/sample - loss: 0.6197 - accuracy: 0.6584\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 95us/sample - loss: 0.5999 - accuracy: 0.6733\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 99us/sample - loss: 0.5823 - accuracy: 0.7079\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 108us/sample - loss: 0.5653 - accuracy: 0.7376\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 99us/sample - loss: 0.5517 - accuracy: 0.7426\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 92us/sample - loss: 0.5383 - accuracy: 0.7475\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 101us/sample - loss: 0.5261 - accuracy: 0.7574\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 99us/sample - loss: 0.5157 - accuracy: 0.7723\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 102us/sample - loss: 0.5048 - accuracy: 0.7822\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 98us/sample - loss: 0.4965 - accuracy: 0.7822\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 102us/sample - loss: 0.4874 - accuracy: 0.7822\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 94us/sample - loss: 0.4789 - accuracy: 0.7772\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 98us/sample - loss: 0.4704 - accuracy: 0.7772\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 103us/sample - loss: 0.4626 - accuracy: 0.7871\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 98us/sample - loss: 0.4557 - accuracy: 0.7921\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 102us/sample - loss: 0.4494 - accuracy: 0.7921\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 100us/sample - loss: 0.4441 - accuracy: 0.7921\n",
      "101/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 932us/sample - loss: 0.3497 - accuracy: 0.8020\n",
      "Train on 202 samples\n",
      "Epoch 1/30\n",
      "202/202 [==============================] - 0s 2ms/sample - loss: 0.7031 - accuracy: 0.5248\n",
      "Epoch 2/30\n",
      "202/202 [==============================] - 0s 111us/sample - loss: 0.6519 - accuracy: 0.5792\n",
      "Epoch 3/30\n",
      "202/202 [==============================] - 0s 129us/sample - loss: 0.6113 - accuracy: 0.6485\n",
      "Epoch 4/30\n",
      "202/202 [==============================] - 0s 101us/sample - loss: 0.5772 - accuracy: 0.6782\n",
      "Epoch 5/30\n",
      "202/202 [==============================] - 0s 102us/sample - loss: 0.5494 - accuracy: 0.7030\n",
      "Epoch 6/30\n",
      "202/202 [==============================] - 0s 110us/sample - loss: 0.5254 - accuracy: 0.7673\n",
      "Epoch 7/30\n",
      "202/202 [==============================] - 0s 112us/sample - loss: 0.5025 - accuracy: 0.8020\n",
      "Epoch 8/30\n",
      "202/202 [==============================] - 0s 102us/sample - loss: 0.4824 - accuracy: 0.8168\n",
      "Epoch 9/30\n",
      "202/202 [==============================] - 0s 105us/sample - loss: 0.4639 - accuracy: 0.8119\n",
      "Epoch 10/30\n",
      "202/202 [==============================] - 0s 103us/sample - loss: 0.4473 - accuracy: 0.8119\n",
      "Epoch 11/30\n",
      "202/202 [==============================] - 0s 107us/sample - loss: 0.4326 - accuracy: 0.8168\n",
      "Epoch 12/30\n",
      "202/202 [==============================] - 0s 102us/sample - loss: 0.4192 - accuracy: 0.8267\n",
      "Epoch 13/30\n",
      "202/202 [==============================] - 0s 121us/sample - loss: 0.4078 - accuracy: 0.8218\n",
      "Epoch 14/30\n",
      "202/202 [==============================] - 0s 106us/sample - loss: 0.3968 - accuracy: 0.8168\n",
      "Epoch 15/30\n",
      "202/202 [==============================] - 0s 103us/sample - loss: 0.3877 - accuracy: 0.8317\n",
      "Epoch 16/30\n",
      "202/202 [==============================] - 0s 103us/sample - loss: 0.3792 - accuracy: 0.8366\n",
      "Epoch 17/30\n",
      "202/202 [==============================] - 0s 113us/sample - loss: 0.3719 - accuracy: 0.8515\n",
      "Epoch 18/30\n",
      "202/202 [==============================] - 0s 112us/sample - loss: 0.3650 - accuracy: 0.8614\n",
      "Epoch 19/30\n",
      "202/202 [==============================] - 0s 116us/sample - loss: 0.3581 - accuracy: 0.8564\n",
      "Epoch 20/30\n",
      "202/202 [==============================] - 0s 110us/sample - loss: 0.3518 - accuracy: 0.8564\n",
      "Epoch 21/30\n",
      "202/202 [==============================] - 0s 103us/sample - loss: 0.3463 - accuracy: 0.8564\n",
      "Epoch 22/30\n",
      "202/202 [==============================] - 0s 109us/sample - loss: 0.3410 - accuracy: 0.8564\n",
      "Epoch 23/30\n",
      "202/202 [==============================] - 0s 98us/sample - loss: 0.3354 - accuracy: 0.8614\n",
      "Epoch 24/30\n",
      "202/202 [==============================] - 0s 94us/sample - loss: 0.3308 - accuracy: 0.8663\n",
      "Epoch 25/30\n",
      "202/202 [==============================] - 0s 114us/sample - loss: 0.3261 - accuracy: 0.8614\n",
      "Epoch 26/30\n",
      "202/202 [==============================] - 0s 104us/sample - loss: 0.3224 - accuracy: 0.8614\n",
      "Epoch 27/30\n",
      "202/202 [==============================] - 0s 100us/sample - loss: 0.3185 - accuracy: 0.8614\n",
      "Epoch 28/30\n",
      "202/202 [==============================] - 0s 102us/sample - loss: 0.3154 - accuracy: 0.8663\n",
      "Epoch 29/30\n",
      "202/202 [==============================] - 0s 114us/sample - loss: 0.3119 - accuracy: 0.8713\n",
      "Epoch 30/30\n",
      "202/202 [==============================] - 0s 102us/sample - loss: 0.3091 - accuracy: 0.8713\n",
      "101/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 874us/sample - loss: 0.3168 - accuracy: 0.7624\n",
      "Train on 202 samples\n",
      "Epoch 1/30\n",
      "202/202 [==============================] - 0s 2ms/sample - loss: 0.8015 - accuracy: 0.4604\n",
      "Epoch 2/30\n",
      "202/202 [==============================] - 0s 102us/sample - loss: 0.7596 - accuracy: 0.4752\n",
      "Epoch 3/30\n",
      "202/202 [==============================] - 0s 103us/sample - loss: 0.7251 - accuracy: 0.5297\n",
      "Epoch 4/30\n",
      "202/202 [==============================] - 0s 102us/sample - loss: 0.6952 - accuracy: 0.5446\n",
      "Epoch 5/30\n",
      "202/202 [==============================] - 0s 105us/sample - loss: 0.6653 - accuracy: 0.6089\n",
      "Epoch 6/30\n",
      "202/202 [==============================] - 0s 108us/sample - loss: 0.6405 - accuracy: 0.6535\n",
      "Epoch 7/30\n",
      "202/202 [==============================] - 0s 112us/sample - loss: 0.6167 - accuracy: 0.7228\n",
      "Epoch 8/30\n",
      "202/202 [==============================] - 0s 98us/sample - loss: 0.5970 - accuracy: 0.7376\n",
      "Epoch 9/30\n",
      "202/202 [==============================] - 0s 101us/sample - loss: 0.5785 - accuracy: 0.7574\n",
      "Epoch 10/30\n",
      "202/202 [==============================] - 0s 106us/sample - loss: 0.5606 - accuracy: 0.7772\n",
      "Epoch 11/30\n",
      "202/202 [==============================] - 0s 113us/sample - loss: 0.5439 - accuracy: 0.7822\n",
      "Epoch 12/30\n",
      "202/202 [==============================] - 0s 115us/sample - loss: 0.5284 - accuracy: 0.7871\n",
      "Epoch 13/30\n",
      "202/202 [==============================] - 0s 96us/sample - loss: 0.5142 - accuracy: 0.7871\n",
      "Epoch 14/30\n",
      "202/202 [==============================] - 0s 108us/sample - loss: 0.5016 - accuracy: 0.7921\n",
      "Epoch 15/30\n",
      "202/202 [==============================] - 0s 101us/sample - loss: 0.4900 - accuracy: 0.8020\n",
      "Epoch 16/30\n",
      "202/202 [==============================] - 0s 103us/sample - loss: 0.4781 - accuracy: 0.7970\n",
      "Epoch 17/30\n",
      "202/202 [==============================] - 0s 99us/sample - loss: 0.4680 - accuracy: 0.8119\n",
      "Epoch 18/30\n",
      "202/202 [==============================] - 0s 97us/sample - loss: 0.4586 - accuracy: 0.8168\n",
      "Epoch 19/30\n",
      "202/202 [==============================] - 0s 100us/sample - loss: 0.4497 - accuracy: 0.8168\n",
      "Epoch 20/30\n",
      "202/202 [==============================] - 0s 107us/sample - loss: 0.4409 - accuracy: 0.8119\n",
      "Epoch 21/30\n",
      "202/202 [==============================] - 0s 97us/sample - loss: 0.4339 - accuracy: 0.8168\n",
      "Epoch 22/30\n",
      "202/202 [==============================] - 0s 110us/sample - loss: 0.4279 - accuracy: 0.8119\n",
      "Epoch 23/30\n",
      "202/202 [==============================] - 0s 113us/sample - loss: 0.4220 - accuracy: 0.8218\n",
      "Epoch 24/30\n",
      "202/202 [==============================] - 0s 95us/sample - loss: 0.4166 - accuracy: 0.8267\n",
      "Epoch 25/30\n",
      "202/202 [==============================] - 0s 101us/sample - loss: 0.4115 - accuracy: 0.8267\n",
      "Epoch 26/30\n",
      "202/202 [==============================] - 0s 96us/sample - loss: 0.4066 - accuracy: 0.8317\n",
      "Epoch 27/30\n",
      "202/202 [==============================] - 0s 95us/sample - loss: 0.4020 - accuracy: 0.8317\n",
      "Epoch 28/30\n",
      "202/202 [==============================] - 0s 134us/sample - loss: 0.3977 - accuracy: 0.8366\n",
      "Epoch 29/30\n",
      "202/202 [==============================] - 0s 115us/sample - loss: 0.3945 - accuracy: 0.8366\n",
      "Epoch 30/30\n",
      "202/202 [==============================] - 0s 95us/sample - loss: 0.3907 - accuracy: 0.8366\n",
      "101/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 931us/sample - loss: 0.3628 - accuracy: 0.8515\n",
      "Train on 202 samples\n",
      "Epoch 1/30\n",
      "202/202 [==============================] - 0s 2ms/sample - loss: 0.8627 - accuracy: 0.4059\n",
      "Epoch 2/30\n",
      "202/202 [==============================] - 0s 99us/sample - loss: 0.8172 - accuracy: 0.4307\n",
      "Epoch 3/30\n",
      "202/202 [==============================] - 0s 99us/sample - loss: 0.7813 - accuracy: 0.4703\n",
      "Epoch 4/30\n",
      "202/202 [==============================] - 0s 100us/sample - loss: 0.7496 - accuracy: 0.5050\n",
      "Epoch 5/30\n",
      "202/202 [==============================] - 0s 94us/sample - loss: 0.7216 - accuracy: 0.5446\n",
      "Epoch 6/30\n",
      "202/202 [==============================] - 0s 99us/sample - loss: 0.6944 - accuracy: 0.5792\n",
      "Epoch 7/30\n",
      "202/202 [==============================] - 0s 88us/sample - loss: 0.6702 - accuracy: 0.6040\n",
      "Epoch 8/30\n",
      "202/202 [==============================] - 0s 102us/sample - loss: 0.6473 - accuracy: 0.6634\n",
      "Epoch 9/30\n",
      "202/202 [==============================] - 0s 106us/sample - loss: 0.6263 - accuracy: 0.6881\n",
      "Epoch 10/30\n",
      "202/202 [==============================] - 0s 95us/sample - loss: 0.6086 - accuracy: 0.7079\n",
      "Epoch 11/30\n",
      "202/202 [==============================] - 0s 94us/sample - loss: 0.5920 - accuracy: 0.7228\n",
      "Epoch 12/30\n",
      "202/202 [==============================] - 0s 100us/sample - loss: 0.5787 - accuracy: 0.7277\n",
      "Epoch 13/30\n",
      "202/202 [==============================] - 0s 99us/sample - loss: 0.5651 - accuracy: 0.7426\n",
      "Epoch 14/30\n",
      "202/202 [==============================] - 0s 98us/sample - loss: 0.5536 - accuracy: 0.7426\n",
      "Epoch 15/30\n",
      "202/202 [==============================] - 0s 106us/sample - loss: 0.5421 - accuracy: 0.7525\n",
      "Epoch 16/30\n",
      "202/202 [==============================] - 0s 94us/sample - loss: 0.5325 - accuracy: 0.7426\n",
      "Epoch 17/30\n",
      "202/202 [==============================] - 0s 102us/sample - loss: 0.5221 - accuracy: 0.7426\n",
      "Epoch 18/30\n",
      "202/202 [==============================] - 0s 99us/sample - loss: 0.5133 - accuracy: 0.7426\n",
      "Epoch 19/30\n",
      "202/202 [==============================] - 0s 97us/sample - loss: 0.5051 - accuracy: 0.7574\n",
      "Epoch 20/30\n",
      "202/202 [==============================] - 0s 101us/sample - loss: 0.4972 - accuracy: 0.7673\n",
      "Epoch 21/30\n",
      "202/202 [==============================] - 0s 93us/sample - loss: 0.4895 - accuracy: 0.7723\n",
      "Epoch 22/30\n",
      "202/202 [==============================] - 0s 97us/sample - loss: 0.4823 - accuracy: 0.7822\n",
      "Epoch 23/30\n",
      "202/202 [==============================] - 0s 103us/sample - loss: 0.4761 - accuracy: 0.7772\n",
      "Epoch 24/30\n",
      "202/202 [==============================] - 0s 85us/sample - loss: 0.4702 - accuracy: 0.7772\n",
      "Epoch 25/30\n",
      "202/202 [==============================] - 0s 106us/sample - loss: 0.4644 - accuracy: 0.7822\n",
      "Epoch 26/30\n",
      "202/202 [==============================] - 0s 99us/sample - loss: 0.4588 - accuracy: 0.7871\n",
      "Epoch 27/30\n",
      "202/202 [==============================] - 0s 100us/sample - loss: 0.4544 - accuracy: 0.8020\n",
      "Epoch 28/30\n",
      "202/202 [==============================] - 0s 96us/sample - loss: 0.4488 - accuracy: 0.7921\n",
      "Epoch 29/30\n",
      "202/202 [==============================] - 0s 99us/sample - loss: 0.4438 - accuracy: 0.7921\n",
      "Epoch 30/30\n",
      "202/202 [==============================] - 0s 102us/sample - loss: 0.4397 - accuracy: 0.7921\n",
      "101/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 853us/sample - loss: 0.4046 - accuracy: 0.8614\n",
      "Train on 202 samples\n",
      "Epoch 1/40\n",
      "202/202 [==============================] - 0s 2ms/sample - loss: 0.8229 - accuracy: 0.4554\n",
      "Epoch 2/40\n",
      "202/202 [==============================] - 0s 113us/sample - loss: 0.7770 - accuracy: 0.4851\n",
      "Epoch 3/40\n",
      "202/202 [==============================] - 0s 93us/sample - loss: 0.7316 - accuracy: 0.5050\n",
      "Epoch 4/40\n",
      "202/202 [==============================] - 0s 101us/sample - loss: 0.6926 - accuracy: 0.5099\n",
      "Epoch 5/40\n",
      "202/202 [==============================] - 0s 107us/sample - loss: 0.6581 - accuracy: 0.5644\n",
      "Epoch 6/40\n",
      "202/202 [==============================] - 0s 100us/sample - loss: 0.6257 - accuracy: 0.6040\n",
      "Epoch 7/40\n",
      "202/202 [==============================] - 0s 100us/sample - loss: 0.5985 - accuracy: 0.6634\n",
      "Epoch 8/40\n",
      "202/202 [==============================] - 0s 104us/sample - loss: 0.5721 - accuracy: 0.6832\n",
      "Epoch 9/40\n",
      "202/202 [==============================] - 0s 105us/sample - loss: 0.5490 - accuracy: 0.7129\n",
      "Epoch 10/40\n",
      "202/202 [==============================] - 0s 111us/sample - loss: 0.5280 - accuracy: 0.7475\n",
      "Epoch 11/40\n",
      "202/202 [==============================] - 0s 105us/sample - loss: 0.5086 - accuracy: 0.7574\n",
      "Epoch 12/40\n",
      "202/202 [==============================] - 0s 115us/sample - loss: 0.4902 - accuracy: 0.7921\n",
      "Epoch 13/40\n",
      "202/202 [==============================] - 0s 93us/sample - loss: 0.4732 - accuracy: 0.8119\n",
      "Epoch 14/40\n",
      "202/202 [==============================] - 0s 105us/sample - loss: 0.4585 - accuracy: 0.8218\n",
      "Epoch 15/40\n",
      "202/202 [==============================] - 0s 93us/sample - loss: 0.4438 - accuracy: 0.8317\n",
      "Epoch 16/40\n",
      "202/202 [==============================] - ETA: 0s - loss: 0.3994 - accuracy: 0.90 - 0s 106us/sample - loss: 0.4297 - accuracy: 0.8317\n",
      "Epoch 17/40\n",
      "202/202 [==============================] - 0s 94us/sample - loss: 0.4172 - accuracy: 0.8218\n",
      "Epoch 18/40\n",
      "202/202 [==============================] - 0s 93us/sample - loss: 0.4057 - accuracy: 0.8317\n",
      "Epoch 19/40\n",
      "202/202 [==============================] - 0s 92us/sample - loss: 0.3939 - accuracy: 0.8317\n",
      "Epoch 20/40\n",
      "202/202 [==============================] - 0s 96us/sample - loss: 0.3841 - accuracy: 0.8366\n",
      "Epoch 21/40\n",
      "202/202 [==============================] - 0s 111us/sample - loss: 0.3752 - accuracy: 0.8465\n",
      "Epoch 22/40\n",
      "202/202 [==============================] - 0s 97us/sample - loss: 0.3670 - accuracy: 0.8564\n",
      "Epoch 23/40\n",
      "202/202 [==============================] - 0s 101us/sample - loss: 0.3597 - accuracy: 0.8564\n",
      "Epoch 24/40\n",
      "202/202 [==============================] - 0s 91us/sample - loss: 0.3526 - accuracy: 0.8564\n",
      "Epoch 25/40\n",
      "202/202 [==============================] - 0s 116us/sample - loss: 0.3467 - accuracy: 0.8564\n",
      "Epoch 26/40\n",
      "202/202 [==============================] - 0s 94us/sample - loss: 0.3403 - accuracy: 0.8515\n",
      "Epoch 27/40\n",
      "202/202 [==============================] - 0s 89us/sample - loss: 0.3346 - accuracy: 0.8515\n",
      "Epoch 28/40\n",
      "202/202 [==============================] - 0s 94us/sample - loss: 0.3297 - accuracy: 0.8515\n",
      "Epoch 29/40\n",
      "202/202 [==============================] - 0s 93us/sample - loss: 0.3252 - accuracy: 0.8515\n",
      "Epoch 30/40\n",
      "202/202 [==============================] - 0s 117us/sample - loss: 0.3210 - accuracy: 0.8515\n",
      "Epoch 31/40\n",
      "202/202 [==============================] - 0s 113us/sample - loss: 0.3169 - accuracy: 0.8515\n",
      "Epoch 32/40\n",
      "202/202 [==============================] - 0s 83us/sample - loss: 0.3129 - accuracy: 0.8515\n",
      "Epoch 33/40\n",
      "202/202 [==============================] - 0s 88us/sample - loss: 0.3091 - accuracy: 0.8614\n",
      "Epoch 34/40\n",
      "202/202 [==============================] - 0s 84us/sample - loss: 0.3053 - accuracy: 0.8614\n",
      "Epoch 35/40\n",
      "202/202 [==============================] - 0s 91us/sample - loss: 0.3020 - accuracy: 0.8614\n",
      "Epoch 36/40\n",
      "202/202 [==============================] - 0s 81us/sample - loss: 0.2994 - accuracy: 0.8663\n",
      "Epoch 37/40\n",
      "202/202 [==============================] - 0s 82us/sample - loss: 0.2973 - accuracy: 0.8713\n",
      "Epoch 38/40\n",
      "202/202 [==============================] - 0s 92us/sample - loss: 0.2946 - accuracy: 0.8713\n",
      "Epoch 39/40\n",
      "202/202 [==============================] - 0s 108us/sample - loss: 0.2923 - accuracy: 0.8762\n",
      "Epoch 40/40\n",
      "202/202 [==============================] - 0s 107us/sample - loss: 0.2901 - accuracy: 0.8762\n",
      "101/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 5ms/sample - loss: 0.2621 - accuracy: 0.8119\n",
      "Train on 202 samples\n",
      "Epoch 1/40\n",
      "202/202 [==============================] - 0s 2ms/sample - loss: 0.9591 - accuracy: 0.4307\n",
      "Epoch 2/40\n",
      "202/202 [==============================] - 0s 100us/sample - loss: 0.8971 - accuracy: 0.4604\n",
      "Epoch 3/40\n",
      "202/202 [==============================] - 0s 102us/sample - loss: 0.8461 - accuracy: 0.5149\n",
      "Epoch 4/40\n",
      "202/202 [==============================] - 0s 108us/sample - loss: 0.7983 - accuracy: 0.5495\n",
      "Epoch 5/40\n",
      "202/202 [==============================] - 0s 104us/sample - loss: 0.7598 - accuracy: 0.5594\n",
      "Epoch 6/40\n",
      "202/202 [==============================] - 0s 107us/sample - loss: 0.7261 - accuracy: 0.5792\n",
      "Epoch 7/40\n",
      "202/202 [==============================] - 0s 114us/sample - loss: 0.6961 - accuracy: 0.6238\n",
      "Epoch 8/40\n",
      "202/202 [==============================] - 0s 107us/sample - loss: 0.6703 - accuracy: 0.6485\n",
      "Epoch 9/40\n",
      "202/202 [==============================] - 0s 106us/sample - loss: 0.6462 - accuracy: 0.6584\n",
      "Epoch 10/40\n",
      "202/202 [==============================] - 0s 118us/sample - loss: 0.6252 - accuracy: 0.6634\n",
      "Epoch 11/40\n",
      "202/202 [==============================] - 0s 119us/sample - loss: 0.6066 - accuracy: 0.6782\n",
      "Epoch 12/40\n",
      "202/202 [==============================] - 0s 106us/sample - loss: 0.5885 - accuracy: 0.6931\n",
      "Epoch 13/40\n",
      "202/202 [==============================] - 0s 103us/sample - loss: 0.5736 - accuracy: 0.7079\n",
      "Epoch 14/40\n",
      "202/202 [==============================] - 0s 90us/sample - loss: 0.5591 - accuracy: 0.7277\n",
      "Epoch 15/40\n",
      "202/202 [==============================] - 0s 87us/sample - loss: 0.5456 - accuracy: 0.7376\n",
      "Epoch 16/40\n",
      "202/202 [==============================] - 0s 87us/sample - loss: 0.5326 - accuracy: 0.7426\n",
      "Epoch 17/40\n",
      "202/202 [==============================] - 0s 85us/sample - loss: 0.5218 - accuracy: 0.7525\n",
      "Epoch 18/40\n",
      "202/202 [==============================] - 0s 86us/sample - loss: 0.5117 - accuracy: 0.7624\n",
      "Epoch 19/40\n",
      "202/202 [==============================] - 0s 87us/sample - loss: 0.5017 - accuracy: 0.7624\n",
      "Epoch 20/40\n",
      "202/202 [==============================] - 0s 86us/sample - loss: 0.4922 - accuracy: 0.7673\n",
      "Epoch 21/40\n",
      "202/202 [==============================] - 0s 87us/sample - loss: 0.4826 - accuracy: 0.7624\n",
      "Epoch 22/40\n",
      "202/202 [==============================] - 0s 87us/sample - loss: 0.4731 - accuracy: 0.7624\n",
      "Epoch 23/40\n",
      "202/202 [==============================] - 0s 88us/sample - loss: 0.4656 - accuracy: 0.7673\n",
      "Epoch 24/40\n",
      "202/202 [==============================] - 0s 88us/sample - loss: 0.4583 - accuracy: 0.7723\n",
      "Epoch 25/40\n",
      "202/202 [==============================] - 0s 87us/sample - loss: 0.4510 - accuracy: 0.7822\n",
      "Epoch 26/40\n",
      "202/202 [==============================] - 0s 99us/sample - loss: 0.4436 - accuracy: 0.7921\n",
      "Epoch 27/40\n",
      "202/202 [==============================] - 0s 95us/sample - loss: 0.4362 - accuracy: 0.8020\n",
      "Epoch 28/40\n",
      "202/202 [==============================] - 0s 89us/sample - loss: 0.4300 - accuracy: 0.8119\n",
      "Epoch 29/40\n",
      "202/202 [==============================] - 0s 88us/sample - loss: 0.4247 - accuracy: 0.8069\n",
      "Epoch 30/40\n",
      "202/202 [==============================] - 0s 91us/sample - loss: 0.4193 - accuracy: 0.8069\n",
      "Epoch 31/40\n",
      "202/202 [==============================] - 0s 87us/sample - loss: 0.4137 - accuracy: 0.8020\n",
      "Epoch 32/40\n",
      "202/202 [==============================] - 0s 107us/sample - loss: 0.4093 - accuracy: 0.8020\n",
      "Epoch 33/40\n",
      "202/202 [==============================] - 0s 102us/sample - loss: 0.4046 - accuracy: 0.8069\n",
      "Epoch 34/40\n",
      "202/202 [==============================] - 0s 102us/sample - loss: 0.3999 - accuracy: 0.8119\n",
      "Epoch 35/40\n",
      "202/202 [==============================] - 0s 103us/sample - loss: 0.3959 - accuracy: 0.8119\n",
      "Epoch 36/40\n",
      "202/202 [==============================] - 0s 99us/sample - loss: 0.3920 - accuracy: 0.8168\n",
      "Epoch 37/40\n",
      "202/202 [==============================] - 0s 92us/sample - loss: 0.3884 - accuracy: 0.8218\n",
      "Epoch 38/40\n",
      "202/202 [==============================] - 0s 105us/sample - loss: 0.3850 - accuracy: 0.8267\n",
      "Epoch 39/40\n",
      "202/202 [==============================] - 0s 104us/sample - loss: 0.3815 - accuracy: 0.8317\n",
      "Epoch 40/40\n",
      "202/202 [==============================] - 0s 105us/sample - loss: 0.3778 - accuracy: 0.8366\n",
      "101/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 917us/sample - loss: 0.4356 - accuracy: 0.8317\n",
      "Train on 202 samples\n",
      "Epoch 1/40\n",
      "202/202 [==============================] - 0s 2ms/sample - loss: 0.8546 - accuracy: 0.4851\n",
      "Epoch 2/40\n",
      "202/202 [==============================] - 0s 98us/sample - loss: 0.8170 - accuracy: 0.5050\n",
      "Epoch 3/40\n",
      "202/202 [==============================] - 0s 93us/sample - loss: 0.7847 - accuracy: 0.5248\n",
      "Epoch 4/40\n",
      "202/202 [==============================] - 0s 89us/sample - loss: 0.7575 - accuracy: 0.5545\n",
      "Epoch 5/40\n",
      "202/202 [==============================] - 0s 103us/sample - loss: 0.7319 - accuracy: 0.5792\n",
      "Epoch 6/40\n",
      "202/202 [==============================] - 0s 99us/sample - loss: 0.7086 - accuracy: 0.6089\n",
      "Epoch 7/40\n",
      "202/202 [==============================] - 0s 94us/sample - loss: 0.6890 - accuracy: 0.6238\n",
      "Epoch 8/40\n",
      "202/202 [==============================] - 0s 87us/sample - loss: 0.6706 - accuracy: 0.6386\n",
      "Epoch 9/40\n",
      "202/202 [==============================] - 0s 86us/sample - loss: 0.6525 - accuracy: 0.6634\n",
      "Epoch 10/40\n",
      "202/202 [==============================] - 0s 85us/sample - loss: 0.6364 - accuracy: 0.6733\n",
      "Epoch 11/40\n",
      "202/202 [==============================] - 0s 91us/sample - loss: 0.6196 - accuracy: 0.6931\n",
      "Epoch 12/40\n",
      "202/202 [==============================] - 0s 93us/sample - loss: 0.6059 - accuracy: 0.7079\n",
      "Epoch 13/40\n",
      "202/202 [==============================] - 0s 92us/sample - loss: 0.5932 - accuracy: 0.7178\n",
      "Epoch 14/40\n",
      "202/202 [==============================] - 0s 95us/sample - loss: 0.5816 - accuracy: 0.7327\n",
      "Epoch 15/40\n",
      "202/202 [==============================] - 0s 89us/sample - loss: 0.5699 - accuracy: 0.7327\n",
      "Epoch 16/40\n",
      "202/202 [==============================] - 0s 91us/sample - loss: 0.5584 - accuracy: 0.7475\n",
      "Epoch 17/40\n",
      "202/202 [==============================] - 0s 85us/sample - loss: 0.5494 - accuracy: 0.7574\n",
      "Epoch 18/40\n",
      "202/202 [==============================] - 0s 80us/sample - loss: 0.5398 - accuracy: 0.7673\n",
      "Epoch 19/40\n",
      "202/202 [==============================] - 0s 85us/sample - loss: 0.5315 - accuracy: 0.7673\n",
      "Epoch 20/40\n",
      "202/202 [==============================] - 0s 86us/sample - loss: 0.5217 - accuracy: 0.7772\n",
      "Epoch 21/40\n",
      "202/202 [==============================] - 0s 85us/sample - loss: 0.5130 - accuracy: 0.7822\n",
      "Epoch 22/40\n",
      "202/202 [==============================] - 0s 85us/sample - loss: 0.5059 - accuracy: 0.7871\n",
      "Epoch 23/40\n",
      "202/202 [==============================] - 0s 82us/sample - loss: 0.4984 - accuracy: 0.7871\n",
      "Epoch 24/40\n",
      "202/202 [==============================] - 0s 90us/sample - loss: 0.4912 - accuracy: 0.7871\n",
      "Epoch 25/40\n",
      "202/202 [==============================] - 0s 88us/sample - loss: 0.4849 - accuracy: 0.7772\n",
      "Epoch 26/40\n",
      "202/202 [==============================] - 0s 92us/sample - loss: 0.4786 - accuracy: 0.7772\n",
      "Epoch 27/40\n",
      "202/202 [==============================] - 0s 93us/sample - loss: 0.4726 - accuracy: 0.7822\n",
      "Epoch 28/40\n",
      "202/202 [==============================] - 0s 89us/sample - loss: 0.4670 - accuracy: 0.7871\n",
      "Epoch 29/40\n",
      "202/202 [==============================] - 0s 89us/sample - loss: 0.4617 - accuracy: 0.8020\n",
      "Epoch 30/40\n",
      "202/202 [==============================] - 0s 84us/sample - loss: 0.4572 - accuracy: 0.8020\n",
      "Epoch 31/40\n",
      "202/202 [==============================] - 0s 82us/sample - loss: 0.4523 - accuracy: 0.7970\n",
      "Epoch 32/40\n",
      "202/202 [==============================] - 0s 88us/sample - loss: 0.4474 - accuracy: 0.7970\n",
      "Epoch 33/40\n",
      "202/202 [==============================] - 0s 93us/sample - loss: 0.4444 - accuracy: 0.7921\n",
      "Epoch 34/40\n",
      "202/202 [==============================] - 0s 89us/sample - loss: 0.4390 - accuracy: 0.7970\n",
      "Epoch 35/40\n",
      "202/202 [==============================] - 0s 87us/sample - loss: 0.4349 - accuracy: 0.8020\n",
      "Epoch 36/40\n",
      "202/202 [==============================] - 0s 87us/sample - loss: 0.4308 - accuracy: 0.8020\n",
      "Epoch 37/40\n",
      "202/202 [==============================] - 0s 96us/sample - loss: 0.4270 - accuracy: 0.8069\n",
      "Epoch 38/40\n",
      "202/202 [==============================] - 0s 106us/sample - loss: 0.4227 - accuracy: 0.8119\n",
      "Epoch 39/40\n",
      "202/202 [==============================] - 0s 118us/sample - loss: 0.4190 - accuracy: 0.8119\n",
      "Epoch 40/40\n",
      "202/202 [==============================] - 0s 102us/sample - loss: 0.4161 - accuracy: 0.8119\n",
      "101/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 859us/sample - loss: 0.3626 - accuracy: 0.8614\n",
      "Train on 303 samples\n",
      "Epoch 1/40\n",
      "303/303 [==============================] - 0s 1ms/sample - loss: 0.8983 - accuracy: 0.3960\n",
      "Epoch 2/40\n",
      "303/303 [==============================] - 0s 83us/sample - loss: 0.8333 - accuracy: 0.4356\n",
      "Epoch 3/40\n",
      "303/303 [==============================] - 0s 84us/sample - loss: 0.7802 - accuracy: 0.4851\n",
      "Epoch 4/40\n",
      "303/303 [==============================] - 0s 79us/sample - loss: 0.7364 - accuracy: 0.5347\n",
      "Epoch 5/40\n",
      "303/303 [==============================] - 0s 97us/sample - loss: 0.6996 - accuracy: 0.5809\n",
      "Epoch 6/40\n",
      "303/303 [==============================] - 0s 93us/sample - loss: 0.6672 - accuracy: 0.6172\n",
      "Epoch 7/40\n",
      "303/303 [==============================] - 0s 79us/sample - loss: 0.6399 - accuracy: 0.6370\n",
      "Epoch 8/40\n",
      "303/303 [==============================] - 0s 87us/sample - loss: 0.6160 - accuracy: 0.6601\n",
      "Epoch 9/40\n",
      "303/303 [==============================] - 0s 84us/sample - loss: 0.5942 - accuracy: 0.6865\n",
      "Epoch 10/40\n",
      "303/303 [==============================] - 0s 79us/sample - loss: 0.5730 - accuracy: 0.7096\n",
      "Epoch 11/40\n",
      "303/303 [==============================] - 0s 75us/sample - loss: 0.5551 - accuracy: 0.7195\n",
      "Epoch 12/40\n",
      "303/303 [==============================] - 0s 75us/sample - loss: 0.5373 - accuracy: 0.7327\n",
      "Epoch 13/40\n",
      "303/303 [==============================] - 0s 79us/sample - loss: 0.5214 - accuracy: 0.7426\n",
      "Epoch 14/40\n",
      "303/303 [==============================] - 0s 78us/sample - loss: 0.5070 - accuracy: 0.7624\n",
      "Epoch 15/40\n",
      "303/303 [==============================] - 0s 81us/sample - loss: 0.4930 - accuracy: 0.7624\n",
      "Epoch 16/40\n",
      "303/303 [==============================] - 0s 75us/sample - loss: 0.4811 - accuracy: 0.7690\n",
      "Epoch 17/40\n",
      "303/303 [==============================] - 0s 81us/sample - loss: 0.4695 - accuracy: 0.7789\n",
      "Epoch 18/40\n",
      "303/303 [==============================] - 0s 83us/sample - loss: 0.4588 - accuracy: 0.7888\n",
      "Epoch 19/40\n",
      "303/303 [==============================] - 0s 91us/sample - loss: 0.4487 - accuracy: 0.7888\n",
      "Epoch 20/40\n",
      "303/303 [==============================] - 0s 74us/sample - loss: 0.4397 - accuracy: 0.7888\n",
      "Epoch 21/40\n",
      "303/303 [==============================] - 0s 72us/sample - loss: 0.4314 - accuracy: 0.7987\n",
      "Epoch 22/40\n",
      "303/303 [==============================] - 0s 71us/sample - loss: 0.4239 - accuracy: 0.8053\n",
      "Epoch 23/40\n",
      "303/303 [==============================] - 0s 79us/sample - loss: 0.4161 - accuracy: 0.8119\n",
      "Epoch 24/40\n",
      "303/303 [==============================] - 0s 85us/sample - loss: 0.4094 - accuracy: 0.8119\n",
      "Epoch 25/40\n",
      "303/303 [==============================] - 0s 87us/sample - loss: 0.4031 - accuracy: 0.8218\n",
      "Epoch 26/40\n",
      "303/303 [==============================] - 0s 75us/sample - loss: 0.3971 - accuracy: 0.8251\n",
      "Epoch 27/40\n",
      "303/303 [==============================] - 0s 74us/sample - loss: 0.3921 - accuracy: 0.8284\n",
      "Epoch 28/40\n",
      "303/303 [==============================] - 0s 80us/sample - loss: 0.3876 - accuracy: 0.8284\n",
      "Epoch 29/40\n",
      "303/303 [==============================] - 0s 81us/sample - loss: 0.3834 - accuracy: 0.8317\n",
      "Epoch 30/40\n",
      "303/303 [==============================] - 0s 75us/sample - loss: 0.3788 - accuracy: 0.8350\n",
      "Epoch 31/40\n",
      "303/303 [==============================] - 0s 72us/sample - loss: 0.3747 - accuracy: 0.8350\n",
      "Epoch 32/40\n",
      "303/303 [==============================] - 0s 78us/sample - loss: 0.3712 - accuracy: 0.8350\n",
      "Epoch 33/40\n",
      "303/303 [==============================] - 0s 84us/sample - loss: 0.3678 - accuracy: 0.8449\n",
      "Epoch 34/40\n",
      "303/303 [==============================] - 0s 86us/sample - loss: 0.3643 - accuracy: 0.8482\n",
      "Epoch 35/40\n",
      "303/303 [==============================] - 0s 78us/sample - loss: 0.3616 - accuracy: 0.8515\n",
      "Epoch 36/40\n",
      "303/303 [==============================] - 0s 78us/sample - loss: 0.3588 - accuracy: 0.8515\n",
      "Epoch 37/40\n",
      "303/303 [==============================] - 0s 86us/sample - loss: 0.3558 - accuracy: 0.8482\n",
      "Epoch 38/40\n",
      "303/303 [==============================] - 0s 81us/sample - loss: 0.3533 - accuracy: 0.8416\n",
      "Epoch 39/40\n",
      "303/303 [==============================] - 0s 80us/sample - loss: 0.3506 - accuracy: 0.8416\n",
      "Epoch 40/40\n",
      "303/303 [==============================] - 0s 79us/sample - loss: 0.3481 - accuracy: 0.8449\n",
      "Best: 0.8349834879239401 using {'batch_size': 20, 'epochs': 40}\n"
     ]
    }
   ],
   "source": [
    "# first I tuned batch_size\n",
    "# batch size 20 was the best parameter with 80.9 accuracy - \n",
    "# this is already an improvement from our baseline model (74%)\n",
    "# now i will tune other hyperparameters\n",
    "\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=13, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "\n",
    "# define the grid search parameters\n",
    "# batch_size = [10, 20, 40, 60, 80, 100]\n",
    "# param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "# define the grid search parameters\n",
    "param_grid = {'batch_size': [20],\n",
    "              'epochs': [5, 10, 15, 20, 30, 40]}\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 40 epochs gave us the best accuracy score of 83%!!"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
